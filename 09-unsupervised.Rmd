# Unsupervised Learning {#unsupervised}

This chapter deals with machine learning problems which are unsupervised.
This means the machine has access to a set of inputs, $x$, but the desired outcome, $y$ is not available. 
Clearly, learning a relation between inputs and outcomes is impossible, but there are still a lot of problems of interest. 
In particular, we may want to find a compact representation of the inputs, be it for visualization of further processing. This is the problem of _dimensionality reduction_.
For the same reasons we may want to group similar inputs. This is the problem of _clustering_.

In the statistical terminology, and with some exceptions, this chapter can be thought of as multivariate __exploratory__ statistics.
For multivariate __inference__, see Chapter \@ref(multivariate).



## Dimensionality Reduction {#dim-reduce}

```{example, label='bmi'}
Consider the heights and weights of a sample of individuals. 
The data may seemingly reside in $2$ dimensions but given the height, we have a pretty good guess of a persons weight, and vice versa. 
We can thus state that heights and weights are not really two dimensional, but roughly lay on a $1$ dimensional subspace of $\mathbb{R}^2$. 
```


```{example, label='iq'}
Consider the correctness of the answers to a questionnaire with $p$ questions. 
The data may seemingly reside in a $p$ dimensional space, but assuming there is a thing as ``skill'', then given the correctness of a person's reply to a subset of questions, we have a good idea how he scores on the rest. 
Put differently, we don't really need a $200$ question questionnaire-- $100$ is more than enough.
If skill is indeed a one dimensional quality, then the questionnaire data should organize around a single line in the $p$ dimensional cube. 
```


```{example, label='blind-signal'}
Consider $n$ microphones recording an individual. 
The digitized recording consists of $p$ samples. 
Are the recordings really a shapeless cloud of $n$ points in $\mathbb{R}^p$?
Since they all record the same sound, one would expect the $n$ $p$-dimensional points to arrange around the source sound bit: a single point in $\mathbb{R}^p$.
If microphones have different distances to the source, volumes may differ. 
We would thus expect the $n$ points to arrange about a __line__ in $\mathbb{R}^p$. 	
```

### Principal Component Analysis {#pca}

_Principal Component Analysis_ (PCA) is such a basic technique, it has been rediscovered and renamed independently in many fields. 
It can be found under the names of 
Discrete Karhunen–Loève Transform; Hotteling Transform; Proper Orthogonal Decomposition; Eckart–Young  Theorem; Schmidt–Mirsky Theorem; Empirical Orthogonal Functions; Empirical Eigenfunction Decomposition; Empirical Component Analysis; Quasi-Harmonic Modes; Spectral Decomposition; Empirical Modal Analysis, and possibly more^[http://en.wikipedia.org/wiki/Principal_component_analysis].
The many names are quite interesting as they offer an insight into the different problems that led to PCA's (re)discovery.


Return to the BMI problem in Example \@ref(exm:bmi).
Assume you wish to give each individual a "size score", that is a __linear__ combination of height and weight: PCA does just that. 
It returns the linear combination that has the largest variability, i.e., the combination which best distinguishes between individuals. 

The variance maximizing motivation above was the one that guided @hotelling1933analysis.
But $30$ years before him, @pearson1901liii derived the same procedure with a different motivation in mind. 
Pearson was also trying to give each individual a score. 
He did not care about variance maximization, however. 
He simply wanted a small set of coordinates in some (linear) space that approximates the original data well. 

Before we proceed, we give an example to fix ideas.
Consider the crime rate data in `USArrests`, which encodes reported murder events, assaults, rapes, and the urban population of each american state. 

```{r}
head(USArrests)
```

Following Hotelling's motivation, we may want to give each state a "crimilality score". 
We first remove the `UrbanPop` variable, which does not encode crime levels. 
We then z-score each variable with `scale`, and call PCA for a sequence of $1,\dots,3$ criminality scores that best separate between states. 

```{r, cache=TRUE}
USArrests.1 <- USArrests[,-3] %>% scale 
pca.1 <- prcomp(USArrests.1, scale = TRUE)
pca.1
```

Things to note:

- Distinguishing between states, i.e., finding the variance maximizing scores, should be indifferent to the __average__ of each variable. 
We also don't want the score to be sensitive to the measurement __scale__. 
We thus perform PCA in the z-score scale of each variable, obtained with the `scale` function.

- PCA is performed with the `prcomp` function. 
It returns the contribution (weight) of the original variables, to the new crimeness score.  
These weights are called the _loadings_ (or `Rotations` in the `prcomp` output, which is rather confusing as we will later see).

- The number of possible scores, is the same as the number of original variables in the data. 

- The new scores are called the _principal components_, labeled `PC1`,...,`PC3` in our output.

- The loadings on PC1 tell us that the best separation between states is along the average crime rate. 
Why is this? 
Because all the $3$ crime variables have a similar loading on PC1.

- The other PCs are slightly harder to interpret, but it is an interesting exercise.

__If we now represent each state, not with its original $4$ variables, but only with the first $2$ PCs (for example), we have reduced the dimensionality of the data.__




### Dimensionality Reduction Preliminaries

Before presenting methods other than PCA, we need some terminology.

- __Variable__: 
A.k.a. _dimension_, or _feature_, or _column_.
- __Data__: 
A.k.a. _sample_, _observations_. 
Will typically consist of $n$, vectors of dimension $p$.
We typically denote the data as a $n\times p$ matrix $X$. 
- __Manifold__: 
A generalization of a linear space, which is regular enough so that, __locally__, it has all the properties of a linear space. 
We will denote an arbitrary manifold by $\mathcal{M}$, and by $\mathcal{M}_q$ a $q$ dimensional^[You are probably used to thinking of the __dimension__ of linear spaces. We will not rigorously define what is the dimension of a manifold, but you may think of it as the number of free coordinates needed to navigate along the manifold.] manifold.
- __Embedding__: 
Informally speaking: a "shape preserving" mapping of a space into another (see figure below).
- __Linear Embedding__: 
An embedding done via a linear operation. 
- __Generative Model__: 
Known to statisticians as the __sampling distribution__. 
The assumed stochastic process that generated the observed $X$. 

![Various embedding algorithms. Source: http://scikit-learn.org/stable/auto_examples/manifold/plot_manifold_sphere.html#sphx-glr-auto-examples-manifold-plot-manifold-sphere-py](art/sphx_glr_plot_manifold_sphere_001.png)


There are many motivations for dimensionality reduction:

1. __Scoring__: 
Give each observation an interpretable, simple score (Hotelling's motivation).
1. __Latent structure__: 
Recover unobservable information from indirect measurements. 
E.g: Blind signal reconstruction, CT scan, cryo-electron microscopy, etc. 
1. __Signal to Noise__: 
Denoise measurements before further processing like clustering, supervised learning, etc. 
1. __Compression__: 
Save on RAM ,CPU, and communication when operating on a lower dimensional representation of the data. 
	


### Latent Variable Generative Approaches
All generative approaches to dimensionality reduction will include a set of latent/unobservable variables, which we can try to recover from the observables $X$. 
The unobservable variables will typically have a lower dimension than the observables, thus, dimension is reduced. 
We start with the simplest case of linear Factor Analysis. 

#### Factor Analysis (FA)

FA originates from the psychometric literature. 
We thus revisit the IQ (actually g-factor^[https://en.wikipedia.org/wiki/G_factor_(psychometrics)]) Example \@ref(exm:iq):

```{example}
Assume $n$ respondents answer $p$ quantitative questions: $x_i \in \mathbb{R}^p, i=1,\dots,n$. 
Also assume, their responses are some linear function of a single personality attribute, $s_i$. 
We can think of $s_i$ as the subject's ``intelligence''.
We thus have 
\begin{align}
	x_i = A s_i + \varepsilon_i
\end{align}
And in matrix notation:
\begin{align}
	X = S A+\varepsilon,
	(\#eq:factor)
\end{align}
where $A$ is the $q \times p$ matrix of factor loadings, and $S$ the $n \times q$ matrix of latent personality traits. 
In our particular example where $q=1$, the problem is to recover the unobservable intelligence scores, $s_1,\dots,s_n$, from the observed answers $X$.	
```



We may try to estimate $S A$ by assuming some distribution on $S$ and $\varepsilon$ and apply maximum likelihood.
Under standard assumptions on the distribution of $S$ and $\varepsilon$, recovering  $S$ from $\widehat{S A }$ is still impossible as there are infinitely many such solutions.
In the statistical parlance we say the problem is _non identifiable_, and in the applied mathematics parlance we say the problem is _ill posed_.
To see this, consider an orthogonal _rotation_ matrix $R$ ($R' R=I$). 
For each such $R$: $S A = S R' R A = S^* A^*$.
While both solve Eq.\@ref(eq:factor), $A$ and $A^*$ may have very different interpretations. 
This is why many researchers find FA an unsatisfactory inference tool.

```{remark}
The non-uniqueness (non-identifiability) of the FA solution under variable rotation is never mentioned in the PCA context. 
Why is this?
This is because the methods solve different problems. 
The reason the solution to PCA is well defined is that PCA does not seek a single $S$ but rather a __sequence__ of $S_q$ with dimensions growing from $q=1$ to $q=p$. 
```

```{remark}
In classical FA in Eq.\@ref(eq:factor) is clearly an embedding to a linear space:
the one spanned by $S$. 
Under the classical probabilistic assumptions on $S$ and $\varepsilon$ the embedding itself is also linear, and is sometimes solved with PCA. 
Being a generative model, there is no restriction for the embedding to be linear, and there certainly exists sets of assumptions for which the FA returns a non linear embedding into a linear space. 
```

The FA terminology is slightly different than PCA:

- __Factors__: 
The unobserved attributes $S$. 
Akin to the _principal components_ in PCA.
- __Loading__:
The $A$ matrix; the contribution of each factor to the observed $X$.
- __Rotation__:	
An arbitrary orthogonal re-combination of the factors, $S$, and loadings, $A$, which changes the interpretation of the result.


The FA literature offers several heuristics to "fix" the identifiability problem of FA. 
These are known as _rotations_, and go under the names of _Varimax_, _Quartimax_, _Equimax_, _Oblimin_, _Promax_, and possibly others. 


#### Independent Component Analysis (ICA)
Like FA, _independent compoent analysis_ (ICA) is a family of latent space models, thus, a _meta-method_.
It assumes data is generated as some function of the latent variables $S$. 
In many cases this function is assumed to be linear in $S$ so that ICA is compared, if not confused, with PCA and even more so with FA. 

The fundamental idea of ICA is that $S$ has a joint distribution of __non-Gaussian__, __independent__ variables. 
This independence assumption, solves the the non-uniqueness of $S$ in FA.

Being a generative model, estimation of $S$ can then be done using maximum likelihood, or other estimation principles. 

ICA is a popular technique in signal processing, where $A$ is actually the signal, such as sound in Example \@ref(exm:blind-signal).
Recovering $A$ is thus recovering the original signals mixing in the recorded $X$. 





### Purely Algorithmic Approaches

We now discuss dimensionality reduction approaches that are not stated via their generative model, but rather, directly as an algorithm.
This does not mean that they cannot be cast via their generative model, but rather they were not motivated as such.


#### Multidimensional Scaling (MDS)
MDS can be thought of as a variation on PCA, that begins with the $n \times n$ graph^[The term Graph is typically used in this context instead of Network. 
But a graph allows only yes/no relations, while a network, which is a weighted graph, allows a continuous measure of similarity (or dissimilarity). _Network_ is thus more appropriate than _graph_.] of distances between data points, and not the original $n \times p$ data. 

MDS aims at embedding a graph of distances, while preserving the original distances.
Basic results in graph/network theory [@graham1988isometric] suggest that the geometry of a graph cannot be preserved when embedding it into lower dimensions. 
The different types of MDSs, such as _Classical MDS_, and _Sammon Mappings_, differ in the _stress function_ penalizing for geometric distortion.


#### Local Multidimensional Scaling (Local MDS)

```{example, label='non-euclidean'}
Consider data of coordinates on the globe. 
At short distances, constructing a dissimilarity graph with Euclidean distances will capture the true distance between points. 
At long distances, however, the Euclidean distances as grossly inappropriate. 
A more extreme example is coordinates on the brain's cerebral cortex.
Being a highly folded surface, the Euclidean distance between points is far from the true geodesic distances along the cortex's surface^[Then again, it is possible that the true distances are the white matter fibers connecting going within the cortex, in which case, Euclidean distances are more appropriate than geodesic distances. We put that aside for now.].
```

Local MDS is aimed at solving the case where we don't know how to properly measure distances. 
It is an algorithm that compounds both the construction of the dissimilarity graph, and the embedding. 
The solution of local MDS, as the name suggests, rests on the computation of _local_ distances, where the Euclidean assumption may still be plausible, and then aggregate many such local distances, before calling upon regular MDS for the embedding.

Because local MDS ends with a regular MDS, it can be seen as a non-linear embedding into a linear $\mathcal{M}$. 

Local MDS is not popular.
Why is this? 
Because it makes no sense: 
If we believe the points reside in a non-Euclidean space, thus motivating the use of geodesic distances, why would we want to wrap up with regular MDS, which embeds in a linear space?!
It does offer, however, some intuition to the following, more popular, algorithms.


#### Isometric Feature Mapping (IsoMap) {#isomap}
Like localMDS, only that the embedding, and not only the computation of the distances, is local.


#### Local Linear Embedding (LLE)
Very similar to IsoMap \@ref(isomap).


#### Kernel PCA
TODO

#### Simplified Component Technique LASSO (SCoTLASS)
TODO

#### Sparse Principal Component Analysis (sPCA)
TODO

#### Sparse kernel principal component analysis (skPCA)
TODO




### Dimensionality Reduction in R

#### PCA {#pca-in-r}

We already saw the basics of PCA in \@ref(pca).
The fitting is done with the `prcomp` function.
The _bi-plot_ is a useful way to visualize the output of PCA.

```{r}
library(devtools)
# install_github("vqv/ggbiplot")
ggbiplot::ggbiplot(pca.1) 
```

Things to note:


- We used the `ggbiplot` function from the __ggbiplot__ (available from github, but not from CRAN), because it has a nicer output than `stats::biplot`. 
- The bi-plot also plots the loadings as arrows. The coordinates of the arrows belong to the weight of each of the original variables in each PC. 
For example, the x-value of each arrow is the loadings on the first PC (on the x-axis).
Since the weights of Murder, Assault, and Rape are almost the same, we conclude that PC1 captures the average crime rate in each state.
- The bi-plot plots each data point along its PCs.

The _scree plot_ depicts the quality of the approximation of $X$ as $q$ grows. 
This is depicted using the proportion of variability in $X$ that is removed by each added PC.
It is customary to choose $q$ as the first PC that has a relative low contribution to the approximation of $X$.

```{r}
ggbiplot::ggscreeplot(pca.1)
```

See how the first PC captures the variability in the Assault levels and Murder levels, with a single score.

```{r, echo=FALSE}
USArrests.1 <- USArrests[,-3] %>% scale
load <- pca.1$rotation
slope <- load[2, ]/load[1, ]
mn <- apply(USArrests.1, 2, mean)
intcpt <- mn[2] - (slope * mn[1])

# scatter plot with the two new axes added
USArrests.2 <- USArrests[,1:2] %>%  scale
xlim <- range(USArrests.2)  # overall min, max
plot(USArrests.2, xlim = xlim, ylim = xlim, pch = 16, col = "purple")  # both axes same length
abline(intcpt[1], slope[1], lwd = 2)  # first component solid line
abline(intcpt[2], slope[2], lwd = 2, lty = 2)  # second component dashed
legend("right", legend = c("PC 1", "PC 2"), lty = c(1, 2), lwd = 2, cex = 1)

# projections of points onto PCA 1
y1 <- intcpt[1] + slope[1] * USArrests.2[, 1]
x1 <- (USArrests.1[, 2] - intcpt[1])/slope[1]
y2 <- (y1 + USArrests.1[, 2])/2
x2 <- (x1 + USArrests.1[, 1])/2
segments(USArrests.1[, 1], USArrests.1[, 2], x2, y2, lwd = 2, col = "purple")
```

More implementations of PCA:
```{r many PCA implementations, eval=FALSE}
# FAST solutions:
gmodels::fast.prcomp()

# More detail in output:
FactoMineR::PCA()

# For flexibility in algorithms and visualization:
ade4::dudi.pca()

# Another one...
amap::acp()
```


#### FA


```{r FA, cache=TRUE}
fa.1 <- psych::principal(USArrests.1, nfactors = 2, rotate = "none")
fa.1
biplot(fa.1, labels =  rownames(USArrests.1)) 

# Numeric comparison with PCA:
fa.1$loadings
pca.1$rotation
```

Things to note:

- We perform FA with the `psych::principal` function. The `Principal Component Analysis` title is due to the fact that FA without rotations, is equivalent to PCA.
- The first factor (`fa.1$loadings`) has different weights than the first PC (`pca.1$rotation`) because of normalization. They are the same, however, in that the first PC, and the first factor, capture average crime levels.

Graphical model fans will like the following plot, where the contribution of each variable to each factor is encoded in the width of the arrow. 

```{r}
qgraph::qgraph(fa.1)
```

Let's add a rotation (Varimax), and note that the rotation has indeed changed the loadings of the variables, thus the interpretation of the factors. 

```{r varimax, cache=TRUE}
fa.2 <- psych::principal(USArrests.1, nfactors = 2, rotate = "varimax")

fa.2$loadings
```

Things to note:

- FA with a rotation is no longer equivalent to PCA.
- The rotated factors are now called _rotated componentes_, and reported in `RC1` and `RC2`.


#### ICA

```{r ICA, cache=TRUE}
ica.1 <- fastICA::fastICA(USArrests.1, n.com=2) # Also performs projection pursuit

plot(ica.1$S)
abline(h=0, v=0, lty=2)
text(ica.1$S, pos = 4, labels = rownames(USArrests.1))

# Compare with PCA (first two PCs):
arrows(x0 = ica.1$S[,1], y0 = ica.1$S[,2], x1 = pca.1$x[,2], y1 = pca.1$x[,1], col='red', pch=19, cex=0.5)
```

Things to note:

- ICA is fitted with `fastICA::fastICA`.
- The ICA components, like any other rotated components, are different than the PCA components. 



#### MDS

Classical MDS, also compared with PCA.
```{r MDS, results='hold', cache=TRUE}
# We first need a dissimarity matrix/graph:
state.disimilarity <- dist(USArrests.1)

mds.1 <- cmdscale(state.disimilarity)

plot(mds.1, pch = 19)
abline(h=0, v=0, lty=2)
USArrests.2 <- USArrests[,1:2] %>%  scale
text(mds.1, pos = 4, labels = rownames(USArrests.2), col = 'tomato')

# Compare with PCA (first two PCs):
points(pca.1$x[,1:2], col='red', pch=19, cex=0.5)
```

Things to note:

- We first compute a dissimilarity graph with `dist`. See the `cluster::daisy` function for more dissimilarity measures.
- We learn the MDS embedding with `cmdscale`.
- The embedding of PCA is the same as classical MDS with Euclidean distances. 



Let's try other strain functions for MDS, like Sammon's strain, and compare it with the PCs.
```{r SammonMDS, results='hold'}
mds.2 <- MASS::sammon(state.disimilarity, trace = FALSE)
plot(mds.2$points, pch = 19)
abline(h=0, v=0, lty=2)
text(mds.2$points, pos = 4, labels = rownames(USArrests.2))

# Compare with PCA (first two PCs):
arrows(
  x0 = mds.2$points[,1], y0 = mds.2$points[,2], 
  x1 = pca.1$x[,1], y1 = pca.1$x[,2], 
  col='red', pch=19, cex=0.5)
```

Things to note:

- `MASS::sammon` does the embedding.
- Sammon strain is different than PCA.






#### Sparse PCA

```{r sPCA}
# Compute similarity graph
state.similarity <- MASS::cov.rob(USArrests.1)$cov

spca1 <- elasticnet::spca(state.similarity, K=2, type="Gram", sparse="penalty", trace=FALSE, para=c(0.06,0.16))
spca1$loadings
```



#### Kernel PCA
```{r kPCA, eval=FALSE}
kernlab::kpca()
```




## Clustering {#cluster}


```{example, label="photos"}
Consider the tagging of your friends' pictures on Facebook. 
If you tagged some pictures, Facebook may try to use a supervised approach to automatically label photos. 
If you never tagged pictures, a supervised approach is impossible. 
It is still possible, however, to group simiar pictures together.
```


```{example, label="spam"}
Consider the problem of spam detection. 
It would be nice if each user could label several thousands emails, to apply a supervised learning approach to spam detection.
This is an unrealistic demand, so a pre-clustering stage is useful: the user only needs to tag a couple dozens of homogenous clusters, before solving the supervised learning problem. 
```


In clustering problems, we seek to group observations that are similar. 

There are many motivations for clustering:

1. __Understanding__:
The most common use of clustering is probably as a an exploratory step, to identify homogeneous groups in the data.
1. __Dimensionality reduction__:
Clustering may be seen as a method for dimensionality reduction. 
Unlike the approaches in the Dimensionality Reduction Section \@ref(dim-reduce), it does not compress __variables__ but rather __observations__. 
Each group of homogeneous observations may then be represented as a single prototypical observation of the group.
1. __Pre-Labelling__:
Clustering may be performed as a pre-processing step for supervised learning, when labeling all the samples is impossible due to "budget" constraints, like in Example \@ref(exm:spam). This is sometimes known as _pre-clustering_.

Clustering, like dimensionality reduction, may rely on some latent variable generative model, or on purely algorithmic approaches.



### Latent Variable Generative Approaches


#### Finite Mixture {#finite-mixture}

```{example, label="males-females"}
Consider the distribution of heights.
Heights have a nice bell shaped distribution within each gender.
If genders have not been recorded, heights will be distributed like a _mixture_ of males and females. 
The gender in this example, is a _latent_ variable taking $K=2$ levels: male and female.
```

A _finite mixture_ is the marginal distribution of $K$ distinct classes, when the class variable is _latent_.
This is useful for clustering:
We can assume the number of classes, $K$, and the distribution of each class. 
We then use maximum likelihood to fit the mixture distribution, and finally, cluster by assigning observations to the most probable class.





### Purely Algorithmic Approaches

#### K-Means
The _K-means_ algorithm is possibly the most popular clustering algorithm.
The goal behind K-means clustering is finding a representative point for each of K clusters, and assign each data point to one of these clusters. 
As each cluster has a representative point, this is also a _prototype method_.
The clusters are defined so that they minimize the average Euclidean distance between all points to the center of the cluster.

In K-means, the clusters are first defined, and then similarities computed. 
This is thus a _top-down_ method.

K-means clustering requires the raw features $X$ as inputs, and not only a similarity graph. 
This is evident when examining the algorithm below. 

The k-means algorithm works as follows:

1. Choose the number of clusters $K$.
1. Arbitrarily assign points to clusters.
1. While clusters keep changing:
    1. Compute the cluster centers as the average of their points.
    1. Assign each point to its closest cluster center (in Euclidean distance).
1. Return Cluster assignments and means.


```{remark}
If trained as a statistician, you may wonder- what population quantity is K-means actually estimating?
The estimand of K-means is known as the _K principal points_.
Principal points are points which are _self consistent_, i.e., they are the mean of their neighbourhood. 
```


#### K-Means++
_K-means++_ is a fast version of K-means thanks to a smart initialization. 

#### K-Medoids

If a Euclidean distance is inappropriate for a particular set of variables, or that robustness to corrupt observations is required, or that we wish to constrain the cluster centers to be actual observations, then the _K-Medoids_ algorithm is an adaptation of K-means that allows this.
It is also known under the name _partition around medoids_ (PAM) clustering, suggesting its relation to [graph partitioning](https://en.wikipedia.org/wiki/Graph_partition).

The k-medoids algorithm works as follows.

1. Given a dissimilarity graph.
1. Choose the number of clusters $K$.
1. Arbitrarily assign points to clusters.
1. While clusters keep changing:
    1. Within each cluster, set the center as the data point that minimizes the sum of distances to other points in the cluster.
    1. Assign each point to its closest cluster center.
1. Return Cluster assignments and centers.


```{remark}
If trained as a statistician, you may wonder- what population quantity is K-medoids actually estimating?
The estimand of K-medoids is the median of their neighbourhood. 
A delicate matter is that quantiles are not easy to define for __multivariate__ variables so that the "multivaraitre median", may be a more subtle quantity than you may think. 
See @small1990survey.
```


#### Hirarchial Clustering
Hierarchical clustering algorithms take dissimilarity graphs as inputs.
Hierarchical clustering is a class of greedy _graph-partitioning_ algorithms. 
Being hierarchical by design, they have the attractive property that the evolution of the clustering can be presented with a _dendogram_, i.e., a tree plot.  
A particular advantage of these methods is that they do not require an a-priori choice of the number of cluster ($K$).

Two main sub-classes of algorithms are _agglomerative_, and _divisive_.

_Agglomerative clustering_ algorithms are __bottom-up__ algorithm which build clusters by joining smaller clusters. 
To decide which clusters are joined at each iteration some measure of closeness between clusters is required. 

- __Single Linkage__:
Cluster distance is defined by the distance between the two __closest__ members.
- __Complete Linkage__:
Cluster distance is defined by the distance between the two __farthest__ members.
- __Group Average__:
Cluster distance is defined by the __average__ distance between members.
- __Group Median__: 
Like Group Average, only using the median.

_Divisive clustering_ algorithms are __top-down__ algorithm which build clusters by splitting larger clusters. 



#### Fuzzy Clustering
Can be thought of as a purely algorithmic view of the finite-mixture in Section \@ref(finite-mixture).






### Clustering in R


#### K-Means
The following code is an adaptation from [David Hitchcock](http://people.stat.sc.edu/Hitchcock/chapter6_R_examples.txt).
```{r kmeans, cache=TRUE}
k <- 2
kmeans.1 <- stats::kmeans(USArrests.1, centers = k)
head(kmeans.1$cluster) # cluster asignments

pairs(USArrests.1, panel=function(x,y) text(x,y,kmeans.1$cluster))
```

Things to note:

- The `stats::kmeans` function does the clustering. 
- The cluster assignment is given in the `cluster` element of the `stats::kmeans` output.
- The visual inspection confirms that similar states have been assigned to the same cluster. 


#### K-Means ++

_K-Means++_ is a smart initialization for K-Means.
The following code is taken from the [r-help](https://stat.ethz.ch/pipermail/r-help/2012-January/300051.html) mailing list.
```{r kmeansPP, cache=TRUE}
# Write my own K-means++ function.
kmpp <- function(X, k) {
  
  n <- nrow(X)
  C <- numeric(k)
  C[1] <- sample(1:n, 1)
  
  for (i in 2:k) {
    dm <- pracma::distmat(X, X[C, ])
    pr <- apply(dm, 1, min); pr[C] <- 0
    C[i] <- sample(1:n, 1, prob = pr)
  }
  
  kmeans(X, X[C, ])
}

kmeans.2 <- kmpp(USArrests.1, k)
head(kmeans.2$cluster)
```

#### K-Medoids
Start by growing a distance graph with `dist` and then partition using `pam`.
```{r kmedoids, cache=TRUE}
state.disimilarity <- dist(USArrests.1)
kmed.1 <- cluster::pam(x= state.disimilarity, k=2)
head(kmed.1$clustering)

plot(pca.1$x[,1], pca.1$x[,2], xlab="PC 1", ylab="PC 2", type ='n', lwd=2)
text(pca.1$x[,1], pca.1$x[,2], labels=rownames(USArrests.1), cex=0.7, lwd=2, col=kmed.1$cluster)
```

Things to note:

- K-medoids starts with the computation of a dissimilarity graph, done by the `dist` function.
- The clustering is done by the `cluster::pam` function.
- Inspecting the output confirms that similar states have been assigned to the same cluster. 
- Many other similarity measures can be found in `proxy::dist()`.
- See `cluster::clara()` for a big-data implementation of PAM.


#### Hirarchial Clustering

We start with agglomerative clustering with single-linkage.
```{r HirarchialClustering}
hirar.1 <- hclust(state.disimilarity, method='single')
plot(hirar.1, labels=rownames(USArrests.1), ylab="Distance")
```

Things to note:

- The clustering is done with the `hclust` function.
- We choose the single-linkage distance using the `method='single'` argument.
- We did not need to a-priori specify the number of clusters, $K$, since all the possible $K$'s are included in the output tree. 
- The `plot` function has a particular method for `hclust` class objects, and plots them as dendograms.


We try other types of linkages, to verify that the indeed affect the clustering.
Starting with complete linkage.
```{r complete linkage}
hirar.2 <- hclust(state.disimilarity, method='complete')
plot(hirar.2, labels=rownames(USArrests.1), ylab="Distance")
```

Now with average linkage.
```{r average linkage}
hirar.3 <- hclust(state.disimilarity, method='average')
plot(hirar.3, labels=rownames(USArrests.1), ylab="Distance")
```


If we know how many clusters we want, we can use `cuttree` to get the class assignments. 

```{r}
cut.2.2 <- cutree(hirar.2, k=2)
head(cut.2.2)
```




## Bibliographic Notes
For more on PCA see my [Dimensionality Reduction Class Notes](https://github.com/johnros/dim_reduce/blob/master/dim_reduce.pdf) and references therein.
For more on everything, see @friedman2001elements.
For a softer introduction, see @james2013introduction.

## Practice Yourself
