---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Multivariate Data Analysis {#multivariate}

The term "multivariate data analysis" is so broad and so overloaded, that we start by clarifying what is discussed and what is not discussed in this chapter.
Broadly speaking, we will discuss statistical _inference_, and leave more "exploratory flavored" matters like clustering, and visualization, to the Unsupervised Learning Chapter \@ref(unsupervised).

We start with an example.

```{example, label='icu'}
Consider the problem of a patient monitored in the intensive care unit. 
At every minute the monitor takes $p$ physiological measurements: blood pressure, body temperature, etc.
The total number of minutes in our data is $n$, so that in total, we have $n \times p$ measurements, arranged in a matrix. 
We also know the typical measurements for this patient when healthy: $\mu_0$.
```

Formally, let $y$ be single (random) measurement of a $p$-variate random vector.
Denote $\mu:=E[y]$.
Here is the set of problems we will discuss, in order of their statistical difficulty. 

- __Signal detection__: 
a.k.a. _multivariate hypothesis testing_, i.e., testing if $\mu$ equals $\mu_0$ and for $\mu_0=0$ in particular. 
In our example: "are the current measurement different than a typical one?"

- __Signal counting__: 
Counting the number of elements in $\mu$ that differ from $\mu_0$, and for $\mu_0=0$ in particular.
In our example: "how many measurements differ than their typical values?"

- __Signal identification__:
a.k.a. _multiple testing_, i.e., testing which of the elements in $\mu$ differ from $\mu_0$ and for $\mu_0=0$ in particular. 
In the ANOVA literature, this is known as a __post-hoc__ analysis.
In our example: "which measurements differ than their typical values?"

- __Signal estimation__: 
Estimating the magnitudes of the departure of $\mu$ from $\mu_0$, and for $\mu_0=0$ in particular. 
If estimation follows a _signal detection_ or _signal identification_ stage, this is known as a _selective estimation_ problem.
In our example: "what is the value of the measurements that differ than their typical values?"

- __Multivariate Regression__:
a.k.a. _MANOVA_ in statistical literature, and _structured learning_ in the machine learning literature. 
In our example: "what factors affect the physiological measurements?"


```{example, label='brain-imaging'}
Consider the problem of detecting regions of cognitive function in the brain using fMRI.
Each measurement is the activation level at each location in a brain's region.
If the region has a cognitive function, the mean activation differs than $\mu_0=0$ when the region is evoked.
```

```{example, label='genetics'}
Consider the problem of detecting cancer encoding regions in the genome.
Each measurement is the vector of the genetic configuration of an individual.
A cancer encoding region will have a different (multivariate) distribution between sick and healthy.
In particular, $\mu$ of sick will differ from $\mu$ of healthy.
```


```{example, label='regression'}
Consider the problem of the simplest multiple regression. 
The estimated coefficient, $\hat \beta$ are a random vector.
Regression theory tells us that its covariance is $(X'X)^{-1}\sigma^2$, and null mean of $\beta$.
We thus see that inference on the vector of regression coefficients, is nothing more than a multivaraite inference problem.
```


```{remark}
In the above, "signal" is defined in terms of $\mu$. 
It is possible that the signal is not in the location, $\mu$, but rather in the covariance, $\Sigma$. 
We do not discuss these problems here, and refer the reader to @nadler2008finite.
```

Another possible question is: does a multivariate analysis gives us something we cannot get from a mass-univariate analysis (i.e., a multivariate analysis on each variable separately).
In Example \@ref(exm:icu) we could have just performed multiple univariate tests, and sign an alarm when any of the univariate detectors was triggered. 
The reason we want a multivariate detector, and not multiple univariate detectors is that it is possible that each measurement alone is borderline, but together, the signal accumulates. 
In our ICU example is may mean that the pulse is borderline, the body temperature is borderline, etc. Analyzed simultaneously, it is clear that the patient is in distress.

The next figure^[My thanks to Efrat Vilneski for the figure.] illustrates the idea that some bi-variate measurements may seem ordinary univariately, while very anomalous when examined bi-variately. 


```{remark}
The following figure may also be used to demonstrate the difference between Euclidean Distance and Mahalanobis Distance. 
```


```{r, echo=FALSE}
library(ellipse)
library(ggplot2)
library(MASS)

samples<-1000 
r<-0.8
ds <- mvrnorm(samples, mu = c(0,0), Sigma = matrix(c(1,r,r,1), ncol = 2), empirical = TRUE)
ds <- as.data.frame(ds)
colnames(ds) <- c("x","y")


ell_2s <- ellipse(cor(ds[,1], ds[,2]), scale=c(sd(ds[,1]),sd(ds[,2])), centre=c(mean(ds[,1]),mean(ds[,2])))
ell_3s <- ellipse(cor(ds[,1], ds[,2]), scale=c(sd(ds[,1]),sd(ds[,2])), centre=c(mean(ds[,1]),mean(ds[,2])), level = 0.998)
ell_2s  <- as.data.frame(ell_2s)
ell_3s  <- as.data.frame(ell_3s)

p <- ggplot(data=ds, aes(x=x, y=y)) + geom_point(size=1.5, alpha=.6) +
  geom_path(data=ell_2s, aes(x=x, y=y), size=1, linetype=2)+
  geom_path(data=ell_3s, aes(x=x, y=y), size=1, linetype=2)
p
```



## Signal Detection
Signal detection deals with the detection of the departure of $\mu$ from some $\mu_0$, and especially, $\mu_0=0$.
This problem can be thought of as the multivariate counterpart of the univariate hypothesis t-test. 

### Hotelling's T2 Test
The most fundamental approach to signal detection is a mere generalization of the t-test, known as _Hotelling's $T^2$ test_. 

Recall the univariate t-statistic of a data vector $x$ of length $n$:
\begin{align}
  t^2(x):= \frac{(\bar{x}-\mu_0)^2}{Var[\bar{x}]}= (\bar{x}-\mu_0)Var[\bar{x}]^{-1}(\bar{x}-\mu_0),
  (\#eq:t-test)
\end{align}
where $Var[\bar{x}]=S^2(x)/n$, and $S^2(x)$ is the unbiased variance estimator $S^2(x):=(n-1)^{-1}\sum (x_i-\bar x)^2$.

Generalizing Eq\@ref(eq:t-test) to the multivariate case: 
$\mu_0$ is a $p$-vector, $\bar x$ is a $p$-vector, and $Var[\bar x]$ is a $p \times p$ matrix of the covariance between the $p$ coordinated of $\bar x$. 
When operating with vectors, the squaring becomes a quadratic form, and the division becomes a matrix inverse. 
We thus have
\begin{align}
  T^2(x):= (\bar{x}-\mu_0)' Var[\bar{x}]^{-1} (\bar{x}-\mu_0),
  (\#eq:hotelling-test)
\end{align}
which is the definition of Hotelling's $T^2$ test statistic. 
We typically denote the covariance between coordinates in $x$ with $\hat \Sigma(x)$, so that 
$\widehat \Sigma_{k,l}:=\widehat {Cov}[x_k,x_l]=(n-1)^{-1} \sum (x_{k,i}-\bar x_k)(x_{l,i}-\bar x_l)$.
Using the $\Sigma$ notation, Eq.\@ref(eq:hotelling-test) becomes 
\begin{align}
  T^2(x):= n (\bar{x}-\mu_0)' \hat \Sigma(x)^{-1} (\bar{x}-\mu_0),
\end{align}
which is the standard notation of Hotelling's test statistic.

For inference, we need the null distribution of Hotelling's test statistic. For this we introduce some vocabulary^[This vocabulary is not standard in the literature, so when you read a text, you need to verify yourself what the author means.]:

1. __Low Dimension__:
We call a problem _low dimensional_ if $n \gg p$, i.e. $p/n \approx 0$.
This means there are many observations per estimated parameter.
1. __High Dimension__: 
We call a problem _high dimensional_ if $p/n \to c$, where $c\in (0,1)$.
This means there are more observations than parameters, but not many.
1. __Very High Dimension__: 
We call a problem _very high dimensional_ if $p/n \to c$, where $1<c<\infty$.
This means there are less observations than parameter.

Hotelling's $T^2$ test can only be used in the low dimensional regime. 
For some intuition on this statement, think of taking $n=20$ measurements of $p=100$ physiological variables. 
We seemingly have $20$ observations, but there are $100$ unknown quantities in $\mu$. 
Would you trust your conclusion that $\bar x$ is different than $\mu_0$ based on merely $20$ observations. 

The above criticism is formalized in @bai1996effect. 
For modern applications, Hotelling's $T^2$ is not recommended, since many modern alternatives have been made available. See @rosenblatt2016better and references for a review.


### Various Types of Signal to Detect
In the previous, we assumed that the signal is a departure of $\mu$ from some $\mu_0$. 
For vactor-valued data $y$, that is distributed $F$, we may define "signal" as any departure from some $F_0$. 
This is the multivaraite counterpart of goodness-of-fit (GOF) tests. 

Even when restricting "signal" to departures of $\mu$ from $\mu_0$, we may try to detect various types of signal:

1. __Dense Signal__: when the departure is in all coordinates of $\mu$. 
1. __Sparse Signal__: when the departure is in a subset of coordinates of $\mu$.

A manufactoring motivation is consistent with a dense signal: if a manufacturing process has failed, we expect a change in many measurements (i.e. coordinates of $\mu$). 
A brain-imaging motivation is consistent with a dense signal: if a region encodes cognitive function, we expect a change in many brain locations (i.e. coordinates of $\mu$.)
A genetic motivation is consistent with a sparse signal: if susceptibility of disease is genetic, only a small subset of locations in the genome will encode it. 

Hotelling's $T^2$ statistic is designed for dense signal. 
The following is a simple statistic designed for sparse signal.


### Simes' Test
Hotelling's $T^2$ statistic has currently two limitations: It is designed for dense signals, and it requires estimating the covariance, which is a very difficult problem.

An algorithm, that is sensitive to sparse signal and allows statistically valid detection under a wide range of covariances (even if we don't know the covariance) is known as _Simes' Test_.
The statistic is defined vie the following algorithm:

1. Compute $p$ variable-wise p-values: $p_1,\dots,p_j$.
1. Denote $p_{(1)},\dots,p_{(j)}$ the sorted p-values.
1. Simes' statistic is $p_{Simes}:=min_j\{p_{(j)} \times p/j\}$.
1. Reject the "no signal" null hypothesis at significance $\alpha$ if $p_{Simes}<\alpha$. 



### Signal Detection with R

Let's generate some data with no signal.

```{r}
library(mvtnorm)
n <- 100 # observations
p <- 18 # parameter dimension
mu <- rep(0,p) # no signal
x <- rmvnorm(n = n, mean = mu)
dim(x)
lattice::levelplot(x)
```

Now make our own Hotelling function.

```{r}
hotellingOneSample <- function(x, mu0=rep(0,ncol(x))){
  n <- nrow(x)
  p <- ncol(x)
  stopifnot(n > 5* p)
  bar.x <- colMeans(x)
  Sigma <- var(x)
  Sigma.inv <- solve(Sigma)
  T2 <- n * (bar.x-mu0) %*% Sigma.inv %*% (bar.x-mu0)
  p.value <- pchisq(q = T2, df = p, lower.tail = FALSE)
  return(list(statistic=T2, pvalue=p.value))
}
hotellingOneSample(x)
```

Things to note:

- `stopifnot(n > 5 * p)` is a little verification to check that the problem is indeed low dimensional. Otherwise, the $\chi^2$ approximation cannot be trusted.
- `solve` returns a matrix inverse. 
- `%*%` is the matrix product operator (see also `crossprod()`).
- A function may return only a single object, so we wrap the statistic and its p-value in a `list` object.

Just for verification, we compare our home made Hotelling's test, to the implementation in the __rrcov__ package. 
The statistic is clearly OK, but our $\chi^2$ approximation of the distribution leaves room to desire. 
Personally, I would never trust a Hotelling test if $n$ is not much greater than $p$, in which case I would use a high-dimensional adaptation (see Bibliography).

```{r}
rrcov::T2.test(x)
```

Let's do the same with Simes':
```{r}
Simes <- function(x){
  p.vals <- apply(x, 2, function(z) t.test(z)$p.value) # Compute variable-wise pvalues
  p <- ncol(x)
  p.Simes <- p * min(sort(p.vals)/seq_along(p.vals)) # Compute the Simes statistic
  return(c(pvalue=p.Simes))
}
Simes(x)
```

And now we verify that both tests can indeed detect signal when present. Are p-values small enough to reject the "no signal" null hypothesis?
```{r}
mu <- rep(x = 10/p,times=p) # inject signal
x <- rmvnorm(n = n, mean = mu)
hotellingOneSample(x)
Simes(x)
```

... yes. All p-values are very small, so that all statistics can detect the non-null distribution.

## Signal Counting 
There are many ways to approach the _signal counting_ problem. 
For the purposes of this book, however, we will not discuss them directly, and solve the signal counting problem as a signal identification problem: if we know __where__ $\mu$ departs from $\mu_0$, we only need to count coordinates to solve the signal counting problem. 

```{remark}
In the sparsity or multiple-testing literature, what we call "signal counting" is known as "adapting to sparsit", or "adaptivity".
```

## Signal Identification {#identification}
The problem of _signal identification_ is also known as _selective testing_, or more commonly as _multiple testing_.

In the ANOVA literature, an identification stage will typically follow a detection stage.
These are known as the _omnibus F test_, and _post-hoc_ tests, respectively.
In the multiple testing literature there will typically be no preliminary detection stage. It is typically assumed that signal is present, and the only question is "where?"


The first question when approaching a multiple testing problem is "what is an error"?
Is an error declaring a coordinate in $\mu$ to be different than $\mu_0$ when it is actually not?
Is an error an overly high proportion of falsely identified coordinates?
The former is known as the _family wise error rate_ (FWER), and the latter as the _false discovery rate_ (FDR).

```{remark}
These types of errors have many names in many communities.
See the Wikipedia entry on [ROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) for a table of the (endless) possible error measures. 
```



### Signal Identification in R

One (of many) ways to do signal identification involves the `stats::p.adjust` function.
The function takes as inputs a $p$-vector of the variable-wise __p-values__. 
Why do we start with variable-wise p-values, and not the full data set?

a. Because we want to make inference variable-wise, so it is natural to start with variable-wise statistics. 
a. Because we want to avoid dealing with covariances if possible. Computing variable-wise p-values does not require estimating covariances. 
a. So that the identification problem is decoupled from the variable-wise inference problem, and may be applied much more generally than in the setup we presented. 

We start be generating some high-dimensional multivariate data and computing the coordinate-wise (i.e. hypothesis-wise) p-value.

```{r}
library(mvtnorm)
n <- 1e1
p <- 1e2
mu <- rep(0,p)
x <- rmvnorm(n = n, mean = mu)
dim(x)
lattice::levelplot(x)
```

We now compute the pvalues of each coordinate.
We use a coordinate-wise t-test. 
Why a t-test? Because for the purpose of demonstration we want a simple test. In reality, you may use any test that returns valid p-values. 

```{r}
t.pval <- function(y) t.test(y)$p.value
p.values <- apply(X = x, MARGIN = 2, FUN = t.pval) 
plot(p.values, type='h')
```

Things to note:

- `t.pval` is a function that merely returns the p-value of a t.test.
- We used the `apply` function to apply the same function to each column of `x`. 
- `MARGIN=2` tells `apply` to compute over columns and not rows. 
- The output, `p.values`, is a vector of 100 p-values. 

We are now ready to do the identification, i.e., find which coordinate of $\mu$ is different than $\mu_0=0$.
The workflow for identification has the same structure, regardless of the desired error guarantees: 

1. Compute an `adjusted p-value`. 
1. Compare the adjusted p-value to the desired error level.

If we want $FWER \leq 0.05$, meaning that we allow a $5\%$ probability of making any mistake, we will use the `method="holm"` argument of `p.adjust`.

```{r holm}
alpha <- 0.05
p.values.holm <- p.adjust(p.values, method = 'holm' )
which(p.values.holm < alpha)
```

If we want $FDR \leq 0.05$, meaning that we allow the proportion of false discoveries to be no larger than $5\%$, we use the `method="BH"` argument of `p.adjust`.

```{r bh}
alpha <- 0.05
p.values.BH <- p.adjust(p.values, method = 'BH' )
which(p.values.BH < alpha)
```

We now inject some strong signal in $\mu$ just to see that the process works.
We will artificially inject signal in the first 10 coordinates.

```{r}
mu[1:10] <- 2 # inject signal in first 10 variables
x <- rmvnorm(n = n, mean = mu) # generate data
p.values <- apply(X = x, MARGIN = 2, FUN = t.pval) 
p.values.BH <- p.adjust(p.values, method = 'BH' )
which(p.values.BH < alpha)
```

Indeed- we are now able to detect that the first coordinates carry signal, because their respective coordinate-wise null hypotheses have been rejected. 


## Signal Estimation (*)

The estimation of the elements of $\mu$ is a seemingly straightforward task.
This is not the case, however, if we estimate only the elements that were selected because they were significant (or any other data-dependent criterion).
Clearly, estimating only significant entries will introduce a bias in the estimation. 
In the statistical literature, this is known as _selection bias_.
Selection bias also occurs when you perform inference on regression coefficients after some model selection, say, with a lasso, or a forward search^[You might find this shocking, but it does mean that you cannot trust the `summary` table of a model that was selected from a multitude of models.].

Selective inference is a complicated and active research topic so we will not offer any off-the-shelf solution to the matter.
The curious reader is invited to read @rosenblatt2014selective, @javanmard2014confidence, or [Will Fithian's](http://www.stat.berkeley.edu/~wfithian/) PhD thesis [@fithian2015topics] for more on the topic. 









## Bibliographic Notes
For a general introduction to multivariate data analysis see @anderson2004introduction.
For an R oriented introduction, see @everitt2011introduction.
For more on the difficulties with high dimensional problems, see @bai1996effect.
For some cutting edge solutions for testing in high-dimension, see @rosenblatt2016better and references therein.
Simes' test is not very well known. It is introduced in @simes1986improved, and proven to control the type I error of detection under a PRDS type of dependence in @benjamini2001control.
For more on multiple testing, and signal identification, see @efron2012large.
For more on the choice of your error rate see @rosenblatt2013practitioner.
For an excellent review on graphical models see @kalisch2014causal. 
Everything you need on graphical models, Bayesian belief networks, and structure learning in R, is collected in the [Task View](https://cran.r-project.org/web/views/gR.html).


## Practice Yourself

1. Generate multivariate data with:
    ```{r}
set.seed(3)
mean<-rexp(50,6)
multi<-  rmvnorm(n = 100, mean = mean) 
```
    a. Use Hotelling's test to determine if $\mu$ equals $\mu_0=0$. Can you detect the signal? 
    a. Perform t.test on each variable and extract the p-value. Try to identify visually the variables which depart from $\mu_0$.
    a. Use `p.adjust` to identify in which variables there are any departures from $\mu_0=0$. Allow 5% probability of making any false identification.
    a. Use `p.adjust` to identify in which variables there are any departures from $\mu_0=0$. Allow a 5% proportion of errors within identifications.

1. Generate multivariate data from two groups: `rmvnorm(n = 100, mean = rep(0,10))` for the first, and `rmvnorm(n = 100, mean = rep(0.1,10))` for the second.
    a. Do we agree the groups differ?
    b. Implement the two-group Hotelling test described in Wikipedia: (https://en.wikipedia.org/wiki/Hotelling%27s_T-squared_distribution#Two-sample_statistic).
    c. Verify that you are able to detect that the groups differ. 
    d. Perform a two-group t-test on each coordinate. On which coordinates can you detect signal while controlling the FWER? On which while controlling the FDR? Use `p.adjust`.

1. Return to the previous problem, but set `n=9`. Verify that you cannot compute your Hotelling statistic. 
