--- 
title: "R (BGU course)"
author: "Jonathan D. Rosenblatt"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [bib.bib]
biblio-style: apalike
link-citations: yes
description: "Class notes for the R course at the BGU's IE&M dept."
---

```{r knitr defaults, echo=FALSE}
knitr::opts_chunk$set(out.width='50%')
set.seed(1)
```




# Preface

This book accompanies BGU's "R" course, at the department of Industrial Engineering and Management. 
It has several purposes:

- Help me organize and document the course material.
- Help students during class so that they may focus on listening and not writing.
- Help students after class, so that they may self-study.

At its current state it is experimental. 
It can thus be expected to change from time to time, and include mistakes.
I will be enormously grateful to whoever decides to share with me any mistakes found.

I am enormously grateful to Yihui Xie, who's *bookdown* R package made it possible to easily write a book which has many mathematical formulae, and R output. 

I hope the reader will find this text interesting and useful. 

For reproducing my results you will want to run `set.seed(1)`.





## Notation Conventions

In this text we use the following conventions:
Lower case $x$ may be a vector or a scalar, random of fixed, as implied by the context. 
Upper case $A$ will stand for matrices. 
Equality $=$ is an equality, and $:=$ is a definition.
Norm functions are denoted with $\Vert x \Vert$ for vector norms, and $\Vert A \Vert$ for matrix norms. 
The type of norm is indicated in the subscript; e.g. $\Vert x \Vert_2$ for the Euclidean ($l_2$) norm.
Tag, $x'$ is a transpose. 
The distribution of a random vector is $\sim$.



## Acknowledgements

I have consulted many people during the writing of this text.
I would like to thank [Yoav Kessler](https://kesslerlab.wordpress.com/), [Lena Novack](http://fohs.bgu.ac.il/research/profileBrief.aspx?id=VeeMVried), Efrat Vilensk, Ron Sarfian, and Liad Shekel in particular, for their valuable inputs. 

<!--chapter:end:index.Rmd-->

# Introduction {#intro}



## What is R? {#what-r}

R was not designed to be a bona-fide programming language. 
It is an evolution of the S language, developed at Bell labs (later Lucent) as a wrapper for the endless collection of statistical libraries they wrote in Fortran.

As of 2011, half of R's libraries are [actually written in C](https://wrathematics.github.io/2011/08/27/how-much-of-r-is-written-in-r/). 



For more on the history of R see [AT&T's site](http://www.research.att.com/articles/featured_stories/2013_09/201309_SandR.html?fbid=Yxy4qyQzmMa), John Chamber's talk at [UserR! 2014](https://www.youtube.com/watch?v=_hcpuRB5nGs) or the Introduction to the excellent @venables2013modern.

## The R Ecosystem {#ecosystem}

A large part of R's success is due to the ease in which a user, or a firm, can augment it. 
This led to a large community of users, developers, and protagonists. 
Some of the most important parts of R's ecosystem include:

- [CRAN](https://cran.r-project.org/): 
a repository for R packages, mirrored worldwide.

- [R-help](https://www.r-project.org/mail.html): 
an immensely active mailing list. 
Noways being replaced by StackExchange meta-site. 
Look for the R tags in the [StackOverflow](http://stackoverflow.com/) and [CrossValidated](http://stats.stackexchange.com/) sites.

- [TakViews](https://cran.r-project.org/web/views/): 
part of CRAN that collects packages per topic. 

- [Bioconductor](https://www.bioconductor.org/): 
A CRAN-like repository dedicated to the life sciences. 

- [Neuroconductor](https://www.neuroconductor.org/):
A CRAN-like repository dedicated to neuroscience, and neuroimaging. 

- [Books](https://www.r-project.org/doc/bib/R-books.html): 
An insane amount of books written on the language. Some are free, some are not. 

- [The Israeli-R-user-group](https://groups.google.com/forum/#!forum/israel-r-user-group): 
just like the name suggests.

- Commercial R: being open source and lacking support may seem like a problem that would prohibit R from being adopted for commercial applications. 
This void is filled by several very successful commercial versions such as [Microsoft R](https://mran.microsoft.com/open/), with its accompanying CRAN equivalent called [MRAN](https://mran.microsoft.com/),  [Tibco's Spotfire](http://spotfire.tibco.com/discover-spotfire/what-does-spotfire-do/predictive-analytics/tibco-enterprise-runtime-for-r-terr), and [others](https://en.wikipedia.org/wiki/R_(programming_language)#Commercial_support_for_R).

- [RStudio](https://www.rstudio.com/products/rstudio/download-server/): since its earliest days R came equipped with a minimal text editor. 
It later received plugins for major integrated development environments (IDEs) such as Eclipse, WinEdit and even [VisualStudio](https://www.visualstudio.com/vs/rtvs/). 
None of these, however, had the impact of the RStudio IDE. 
Written completely in JavaScript, the RStudio IDE allows the seamless integration of cutting edge web-design technologies, remote access, and other killer features, making it today's most popular IDE for R.



## Bibliographic Notes

## Practice Yourself

<!--chapter:end:01-intro.Rmd-->

# R Basics {#basics}

We now start with the basics of R. 
If you have any experience at all with R, you can probably skip this section.

First, make sure you work with the RStudio IDE.
Some useful pointers for this IDE include:

- Ctrl+Return to run lines from editor.
- Alt+Shift+k for RStudio keyboard shortcuts.
- Alt+Shift+j to navigate between sections
- tab for auto-completion
- Ctrl+1 to skip to editor. 
- Ctrl+2 to skip to console.
- Ctrl+8 to skip to the environment list.
- Code Folding:
    - Alt+l collapse chunk.
    - Alt+Shift+l unfold chunk.
    - Alt+o collapse all.
    - Alt+Shift+o unfold all.


## Simple calculator
R can be used as a simple calculator.
```{r}
10+5
70*81
2**4
2^4
log(10)       					
log(16, 2)    					
log(1000, 10)   				
```


## Probability calculator 
R can be used as a probability calculator. 
You probably wish you knew this when you did your Intro To Probability.

The binomial distribution function:
```{r}
dbinom(x=3, size=10, prob=0.5) 	# Compute P(X=3) for X~B(n=10, p=0.5) 
```
Notice that arguments do not need to be named explicitly
```{r}
dbinom(3, 10, 0.5)
```
The binomial cumulative distribution function (CDF):
```{r}
pbinom(q=3, size=10, prob=0.5) # Compute P(X<=3) for X~B(n=10, p=0.5)  	
```
The binomial quantile function:
```{r}
qbinom(p=0.1718, size=10, prob=0.5) # For X~B(n=10, p=0.5) returns k such that P(X<=k)=0.1718
```
Generate random variables:
```{r}
rbinom(n=10, size=10, prob=0.5)
```

R has many built-in distributions. 
Their names may change, but the prefixes do not:

- __d__ prefix for the _distribution_ function.
- __p__ prefix for the _cummulative distribution_ function (CDF).
- __q__ prefix for the _quantile_ function (i.e., the inverse CDF).
- __r__ prefix to generate random samples. 

Demonstrating this idea, using the CDF of several popular distributions:

- `pbinom()` for the binomial CDF.
- `ppois()` for the Poisson CDF.
- `pnorm()` for the Gaussian CDF.
- `pexp()` for the exponential CDF.

For more information see `?distributions`.



## Getting Help
One of the most important parts of working with a language, is to know where to find help. 
R has several in-line facilities, besides the various help resources in the R [ecosystem](#ecosystem).


Get help for a particular function.
```{r, eval=FALSE}
?dbinom 
help(dbinom)
```
If you don't know the name of the function you are looking for, search local help files for a particular string:
```{r, eval=FALSE}
??binomial
help.search('dbinom') 
```
Or load a menu where you can navigate local help in a web-based fashion:
```{r, eval=FALSE}
help.start() 
```


## Variable Asignment
Assignment of some output into an object named "x":
```{r}
x = rbinom(n=10, size=10, prob=0.5) # Works. Bad style.
x <- rbinom(n=10, size=10, prob=0.5) 
```
If you are familiar with other programming languages you may prefer the `=` assignment rather than the `<-` assignment. 
We recommend you make the effort to change your preferences. 
This is because thinking with `<-` helps to read your code, distinguishes between assignments and function arguments: think of `function(argument=value)` versus `function(argument<-value)`. 
It also helps understand special assignment operators such as `<<-` and `->`. 


```{remark}
__Style__:
We do not discuss style guidelines in this text, but merely remind the reader that good style is extremely important. When you write code, think of other readers, but also think of future self.
See [Hadley's style guide](http://adv-r.had.co.nz/Style.html) for more. 
```

To print the contents of an object just type its name
```{r}
x
```
which is an implicit call to 
```{r}
print(x)  
```
Alternatively, you can assign and print simultaneously using parenthesis.
```{r}
(x <- rbinom(n=10, size=10, prob=0.5))  # Assign and print.
```

Operate on the object
```{r}
mean(x)  # compute mean
var(x)  # compute variance
hist(x) # plot histogram
```

R saves every object you create in RAM^[S and S-Plus used to save objects on disk. Working from RAM has advantages and disadvantages. More on this in Chapter \@ref(memory).]. 
The collection of all such objects is the __workspace__ which you can inspect with 
```{r}
ls()
```
or with Ctrl+8 in RStudio. 

If you lost your object, you can use `ls` with a text pattern to search for it
```{r}
ls(pattern='x')
```



To remove objects from the workspace:
```{r}
rm(x) # remove variable
ls() # verify
```
You may think that if an object is removed then its memory is freed. 
This is almost true, and depends on a negotiation mechanism between R and the operating system. 
R's memory management is discussed in Chapter \@ref(memory). 


## Piping 
Because R originates in Unix and Linux environments, it inherits much of its flavor.
[Piping](http://ryanstutorials.net/linuxtutorial/piping.php) is an idea taken from the Linux shell which allows to use the output of one expression as the input to another.
Piping thus makes code easier to read and write. 

```{remark}
Volleyball fans may be confused with the idea of spiking a ball from the 3-meter line, also called [piping](https://www.youtube.com/watch?v=GWW15Nr1lQM). 
So:
(a) These are very different things.
(b) If you can pipe, [ASA-BGU](http://in.bgu.ac.il/sport/Pages/asa.aspx) is looking for you!
```

Prerequisites:
```{r}
library(magrittr) # load the piping functions
x <- rbinom(n=1000, size=10, prob=0.5) # generate some toy data
```

Examples
```{r, eval=FALSE}
x %>% var() # Instead of var(x)
x %>% hist()  # Instead of hist(x)
x %>% mean() %>% round(2) %>% add(10) 
```


The next example^[Taken from  http://cran.r-project.org/web/packages/magrittr/vignettes/magrittr.html] demonstrates the benefits of piping.
The next two chunks of code do the same thing.
Try parsing them in your mind:
```{r, eval=FALSE}
# Functional (onion) style
car_data <- 
  transform(aggregate(. ~ cyl, 
                      data = subset(mtcars, hp > 100), 
                      FUN = function(x) round(mean(x, 2))), 
            kpl = mpg*0.4251)
```

```{r, eval=FALSE}
# Piping (magrittr) style
car_data <- 
  mtcars %>%
  subset(hp > 100) %>%
  aggregate(. ~ cyl, data = ., FUN = . %>% mean %>% round(2)) %>%
  transform(kpl = mpg %>% multiply_by(0.4251)) %>%
  print
```

Tip: RStudio has a keyboard shortcut for the `%>%` operator. Try Ctrl+Shift+m.

## Vector Creation and Manipulation 
The most basic building block in R is the __vector__.
We will now see how to create them, and access their elements (i.e. subsetting).
Here are three ways to create the same arbitrary vector:
```{r, eval=FALSE}
c(10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21) # manually
10:21 # the `:` operator 							
seq(from=10, to=21, by=1) # the seq() function
```

Let's assign it to the object named "x":
```{r}
x <- c(10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21) 	
```

```{remark}
In line with the linux look and feel, variables starting with a dot (.) are saved but are hidden.
To show them see `?ls`.
```


Operations usually work element-wise:
```{r}
x+2
x*2    
x^2    
sqrt(x)  
log(x)   
```


## Search Paths and Packages
R can be easily extended with packages, which are merely a set of documented functions, which can be loaded or unloaded conveniently.
Let's look at the function `read.csv`. 
We can see its contents by calling it without arguments:
```{r}
read.csv
```
Never mind what the function does. 
Note the `environment: namespace:utils` line at the end. 
It tells us that this function is part of the __utils__ package.
We did not need to know this because it is loaded by default.
Here are the packages that are currently loaded:
```{r}
head(search())
```
Other packages can be loaded via the `library` function, or downloaded from the internet using the `install.packages` function before loading with `library`. 
R's package import mechanism is quite powerful, and is one of the reasons for R's success. 



## Simple Plotting 
R has many plotting facilities as we will further detail in the Plotting Chapter \@ref(plotting).
We start with the simplest facilities, namely, the `plot` function from the __graphics__ package, which is loaded by default. 
```{r}
x<- 1:100
y<- 3+sin(x) 
plot(x = x, y = y) # x,y syntax  						
```

Given an `x` argument and a `y` argument, `plot` tries to present a scatter plot.
We call this the `x,y` syntax.
R has another unique syntax to state functional relations.
We call `y~x` the "tilde" syntax, which originates in works of @wilkinson1973symbolic and was adopted in the early days of S.
```{r}
plot(y ~ x) # y~x syntax 
```

The syntax `y~x` is read as "y is a function of x".
We will prefer the `y~x` syntax over the `x,y` syntax since it is easier to read, and will be very useful when we discuss more complicated models. 




Here are some arguments that control the plot's appearance.
We use `type` to control the plot type, `main` to control the main title.
```{r}
plot(y~x, type='l', main='Plotting a connected line') 
```

We use `xlab` for the x-axis label, `ylab` for the y-axis.
```{r}
plot(y~x, type='h', main='Sticks plot', xlab='Insert x axis label', ylab='Insert y axis label') 
```

We use `pch` to control the point type. 
```{r}
plot(y~x, pch=5) # Point type with pcf
```

We use `col` to control the color,  `cex` for the point size, and `abline` to add a straight line.
```{r, results='hold'}
plot(y~x, pch=10, type='p', col='blue', cex=4) 
abline(3, 0.002) 
```


For more plotting options run these
```{r, eval=FALSE}
example(plot)
example(points)
?plot
help(package='graphics')
```

When your plotting gets serious, go to Chapter \@ref(plotting).


## Object Types
We already saw that the basic building block of R objects is the vector.
Vectors can be of the following types:

- __character__ Where each element is a string, i.e., a sequence of alphanumeric symbols.
- __numeric__ Where each element is a [real number](https://en.wikipedia.org/wiki/Real_number) in [double precision](https://en.wikipedia.org/wiki/Double-precision_floating-point_format) floating point format.
- __integer__ Where each element is an [integer](https://en.wikipedia.org/wiki/Integer). 
- __logical__ Where each element is either TRUE, FALSE, or NA^[R uses a [__three__ valued logic](https://en.wikipedia.org/wiki/Three-valued_logic) where a missing value (NA) is neither TRUE, nor FALSE.]
- __complex__ Where each element is a complex number. 
- __list__ Where each element is an arbitrary R object.
- __factor__ Factors are not actually vector objects, but they feel like such. 
They are used to encode any finite set of values. 
This will be very useful when fitting linear model, but may be confusing if you think you are dealing with a character vector when in fact you are dealing with a factor. 
Be alert!

Vectors can be combined into larger objects. 
A `matrix` can be thought of as the binding of several vectors of the same type. 
In reality, a matrix is merely a vector with a dimension attribute, that tells R to read it as a matrix and not a vector.

If vectors of different types (but same length) are binded, we get a `data.frame` which is the most fundamental object in R for data analysis. 



## Data Frames 

Creating a simple data frame:
```{r}
x<- 1:10
y<- 3 + sin(x) 
frame1 <- data.frame(x=x, sin=y)	
```

Let's inspect our data frame:
```{r}
head(frame1)
```

Now using the RStudio Excel-like viewer:
```{r, eval=FALSE}
frame1 %>% View() 
```

We highly advise against editing the data this way since there will be no documentation of the changes you made.

Verifying this is a data frame:
```{r}
class(frame1) # the object is of type data.frame
```

Check the dimension of the data
```{r}
dim(frame1)  							
```

Note that checking the dimension of a vector is different than checking the dimension of a data frame.
```{r}
length(x)
```

The length of a `data.frame` is merely the number of columns.
```{r}
length(frame1) 
```


## Exctraction
R provides many ways to subset and extract elements from vectors and other objects. 
The basics are fairly simple, but not paying attention to the "personality" of each extraction mechanism may cause you a lot of headache. 

For starters, extraction is done with the `[` operator. 
The operator can take vectors of many types. 

Extracting element with by integer index:
```{r}
frame1[1, 2]  # exctract the element in the 1st row and 2nd column.
```

Extract __column__ by index:
```{r}
frame1[,1]      						
```

Extract column by name:
```{r}
frame1[, 'sin']
```

As a general rule, extraction with `[` will conserve the class of the parent object. 
There are, however, exceptions.
Notice the extraction mechanism and the class of the output in the following examples. 

```{r}
class(frame1[, 'sin'])  # extracts a column vector
class(frame1['sin'])  # extracts a data frame
class(frame1[,1:2])  # extracts a data frame
class(frame1[2])  # extracts a data frame
class(frame1[2, ])  # extract a data frame
class(frame1$sin)  # extracts a column vector
```

The `subset()` function does the same
```{r, eval=FALSE}
subset(frame1, select=sin) 
subset(frame1, select=2)
subset(frame1, select= c(2,0))
```

If you want to force the stripping of the class attribute when extracting, try the `[[` mechanism instead of `[`.
```{r}
a <- frame1[1] # [ extraction
b <- frame1[[1]] # [[ extraction
a==b # objects are element-wise identical 
class(a)==class(b)
```

The different types of output classes cause different behaviors. Compare the behavior of `[` on seemingly identical objects. 
```{r}
frame1[1][1]
frame1[[1]][1]
```

If you want to learn more about subsetting see [Hadley's guide](http://adv-r.had.co.nz/Subsetting.html).


## Data Import and Export 

For any practical purpose, you will not be generating your data manually. 
R comes with many importing and exporting mechanisms which we now present.
If, however, you do a lot of data "munging", make sure to see Hadley-verse Chapter \@ref(hadley).
If you work with MASSIVE data sets, read the Memory Efficiency Chapter \@ref(memory).


### Import from WEB 
The `read.table` function is the main importing workhorse.
It can import directly from the web.
```{r, eval=FALSE}
URL <- 'http://statweb.stanford.edu/~tibs/ElemStatLearn/datasets/bone.data'
tirgul1 <- read.table(URL)
```

```{r, echo=FALSE}
tirgul1 <- read.table('bone.data')
```


Always look at the imported result!
```{r}
head(tirgul1)
```

Ohh dear. The header row was not recognized. Fix with `header=TRUE`:
```{r, eval=FALSE}
tirgul1 <- read.table(URL, header = TRUE) 
head(tirgul1)
```

```{r, echo=FALSE}
tirgul1 <- read.table('bone.data', header = TRUE)
```



### Export as CSV 
Let's write a simple file so that we have something to import
```{r}
head(airquality) #  examine the data to export
temp.file.name <- tempfile() # get some arbitrary file name
write.csv(x = airquality, file = temp.file.name) # export
```
Now let's import the exported file. Being a .csv file, I can use `read.csv` instead of `read.table`. 
```{r}
my.data<- read.csv(file=temp.file.name) # import
head(my.data) # verify import
```


```{remark}
Windows users may need to use "\\" instead of "/".
```



### Reading From Text Files 

Some general notes on importing text files via the `read.table` function.
But first, we need to know what is the active directory.
Here is how to get and set R's active directory:
```{r, eval=FALSE}
getwd() #What is the working directory?
setwd() #Setting the working directory in Linux
```

We can now call the `read.table` function to import text files. 
If you care about your sanity, see `?read.table` before starting imports.
Some notable properties of the function:

- `read.table` will try to guess column separators (tab, comma, etc.)
- `read.table` will try to guess if a header row is present.
- `read.table` will convert character vectors to factors unless told not to.
- The output of `read.table` needs to be explicitly assigned to an object for it to be saved. 



### Writing Data to Text Files

The function `write.table` is the exporting counterpart of `read.table`.


### .XLS(X) files 
Strongly recommended to convert to .csv in Excel, and then import as csv. 
If you still insist see the __xlsx__ package. 


### Massive files 

The above importing and exporting mechanisms were not designed for massive files. 
See the section on Sparse Representation (\@ref(sparse)) and Out-of-Ram Algorithms (\@ref(memory)) for more on working with massive data files.  


### Databases 
R does not need to read from text files; it can read directly from a database.
This is very useful since it allows the filtering, selecting and joining operations to rely on the database's optimized algorithms.
See https://rforanalytics.wordpress.com/useful-links-for-r/odbc-databases-for-r/ for more information.




## Functions
One of the most basic building blocks of programming is the ability of writing your own functions.
A function in R, like everything else, is an object accessible using its name.
We first define a simple function that sums its two arguments
```{r}
my.sum <- function(x,y) {
  return(x+y)
}
my.sum(10,2)
```

From this example you may notice that:

- The function `function` tells R to construct a function object.

- The arguments of the `function`, i.e. `(x,y)`, need to be named but we are not required to specify their type. This makes writing functions very easy, but it is also the source of many bugs, and slowness of R compared to type declaring languages (C, Fortran,Java,...).

- A typical R function does not change objects^[This is a classical _functional programming_ paradigm. If you are used to _object oriented_ programming, you may want to read about [references classes](http://adv-r.had.co.nz/R5.html) which may be required if you are planning to compute with very complicated objects.] but rather creates new ones. 
To save the output of `my.sum` we will need to assign it using the `<-` operator.

Here is a (slightly) more advanced example.

```{r}
my.sum.2 <- function(x, y , absolute=FALSE) {
  if(absolute==TRUE) {
    result <- abs(x+y)
  }
  else{
    result <- x+y
  } 
  result
}
my.sum.2(-10,2, TRUE)
```

Things to note:

- The function will output its last evaluated expression. You don't need to use the `return` function explicitly.
- Using `absolute=FALSE` sets the default value of `absolute` to `FALSE`. This is overriden if `absolute` is stated explicitly in the function call.


An important behavior of R is the _scoping rules_. This refers to the way R seeks for variables used in functions. 
As a rule of thumb, R will first look for variables inside the function and if not found, will search for the variable values in outer environments^[More formally, this is called [Lexical Scoping](https://darrenjw.wordpress.com/2011/11/23/lexical-scope-and-function-closures-in-r/).].
Think of the next example.

```{r}
a <- 1
b <- 2
x <- 3
scoping <- function(a,b){
  a+b+x
}
scoping(10,11)
```



## Looping 
The real power of scripting is when repeated operations are done by iteration.
R supports the usual `for`, `while`, and `repated` loops. 
Here is an embarrassingly simple example
```{r looping}
for (i in 1:5){
    print(i)
    }
```



## Apply 
For applying the same function to a set of elements, there is no need to write an explicit loop.
This is such en elementary operation that every programming language will provide some facility to __apply__, or __map__ the function to all elements of a set.
R provides several facilities to perform this. 
The most basic of which is `lapply` which applies a function over all elements of a list, and return a list of outputs:
```{r lapply}
the.list <- list(1,'a',mean)
lapply(X = the.list, FUN = class)
```

R provides many variations on `lapply` to facilitate programing. 
Here is a partial list:

- `sapply`: The same as `lapply` but tries to arrange output in a vector or matrix, and not an unstructured list.
- `vapply`: A safer version of `sapply`, where the output class is pre-specified.
- `apply`: For applying over the rows or columns of matrices.
- `mapply`: For applying functions with various inputs. 
- `tapply`: For splitting vectors and applying functions on subsets. 
- `rapply`: A recursive version of `lapply`.
- `eapply`: Like `lapply`, only operates on `environments` instead of lists. 
- `Map`+`Reduce`: For a [Common Lisp](https://en.wikipedia.org/wiki/Common_Lisp) look and feel of `lapply`.
- `parallel::parLapply`: A parallel version of `lapply`.
- `parallel::parLBapply`: A parallel version of `lapply`, with load balancing.



## Recursion 
The R compiler is really not designed for recursion, and you will rarely need to do so.  
See the RCpp Chapter \@ref(rcpp) for linking C code, which is better suited for recursion. 
If you really insist to write recursions in R, make sure to use the `Recall` function, which, as the name suggests, recalls the function in which it is place. 
Here is a demonstration with the Fibonacci series. 
```{r recusrion, cache=TRUE}
fib<-function(n) {
    if (n < 2) fn<-1 
    else fn<-Recall(n - 1) + Recall(n - 2) 
    return(fn)
} 
fib(5)
```


## Dates and Times
[TODO]


## Bibliographic Notes

There are endlessly many introductory texts on R.
For a list of free resources see [CrossValidated](http://stats.stackexchange.com/questions/138/free-resources-for-learning-r).
I personally recommend the official introduction @venables2004introduction, [available online](https://cran.r-project.org/doc/manuals/r-release/R-intro.pdf), or anything else Bill Venables writes. 
For advanced R programming see @wickham2014advanced, [available online](http://adv-r.had.co.nz/),  or anything else Hadley Wickham writes. 


## Practice Yourself

<!--chapter:end:02-r-basics.Rmd-->

# Exploratory Data Analysis {#eda}


Exploratory Data Analysis (EDA) is a term cast by [John W. Tukey](https://en.wikipedia.org/wiki/John_Tukey) in his seminal book [@tukey1977exploratory].
It is the practice of inspecting, and exploring your data, before stating hypotheses, fitting predictors, and other more ambitious inferential goals.
It typically includes the computation of simple _summary statistics_ which capture some property of interest in the data, and _visualization_.
EDA can be thought of as an assumption free, purely algorithmic practice.

In this text we present EDA techniques along the following lines:

- How we explore: with summary statistics, or visually?
- How many variables analyzed simultaneously: univariate, bivariate, or multivariate?
- What type of variable: categorical or continuous?


## Summary Statistics

### Categorical Data
Categorical variables do not admit any mathematical operations on them. 
We cannot sum them, or even sort them. 
We can only __count__ them. 
As such, summaries of categorical variables will always start with the counting of the frequency of each category.

#### Summary of Univariate Categorical Data

```{r}
gender <- c(rep('Boy', 10), rep('Girl', 12))
drink <- c(rep('Coke', 5), rep('Sprite', 3), rep('Coffee', 6), rep('Tea', 7), rep('Water', 1))  
age <-  sample(c('Young', 'Old'), size = length(gender), replace = TRUE)

table(gender)
table(drink)
table(age)
```

If instead of the level counts you want the proportions, you can use `prop.table`

```{r}
prop.table(table(gender))
```


#### Summary of Bivariate Categorical Data

```{r}
library(magrittr)
cbind(gender, drink) %>% head # inspect the raw data
table1 <- table(gender, drink) 
table1										
```

#### Summary of Multivariate Categorical Data
You may be wondering how does R handle tables with more than two dimensions.
It is indeed not trivial, and R offers several solutions: `table` is easier to compute with, and `ftable` is human readable.

```{r}
table2.1 <- table(gender, drink, age) # A multilevel table. 
table2.1
table.2.2 <- ftable(gender, drink, age) # A human readable table.
table.2.2
```

If you want proportions instead of counts, you need to specify the denominator, i.e., the margins. 

```{r}
prop.table(table1, margin = 1)
prop.table(table1, margin = 2)
```


### Continous Data
Continuous variables admit many more operations than categorical.
We can thus compute sums, means, quantiles, and more.

#### Summary of Univariate Continuous Data
We distinguish between several types of summaries, each capturing a different property of the data.

#### Summary of Location
Capture the "location" of the data. These include:

```{definition, name="Average"}
The mean, or average, of a sample $x$ of length $n$, denoted $\bar x$ is defined as 
$$ \bar x := n^{-1} \sum x_i. $$
```

The sample mean is __non robust__. 
A single large observation may inflate the mean indefinitely.
For this reason, we define several other summaries of location, which are more robust, i.e., less affected by "contaminations" of the data.

We start by defining the sample quantiles, themselves __not__ a summary of location.

```{definition, name="Quantiles"}
The $\alpha$ quantile of a sample $x$, denoted $x_\alpha$, is (non uniquely) defined as a value above $100 \alpha \%$ of the sample, and below $100 (1-\alpha) \%$.
```

We emphasize that sample quantiles are non-uniquely defined. See `?quantile` for the 9(!) different definitions that R provides. 

We can now define another summary of location, the median.

```{definition, name="Median"}
The median of a sample $x$, denoted $x_{0.5}$ is the $\alpha=0.5$ quantile of the sample.
```

A whole family of summaries of locations is the __alpha trimmed mean__.

```{definition, name="Alpha Trimmed Mean"}
The $\alpha$ trimmed mean of a sample $x$, denoted $\bar x_\alpha$ is the average of the sample after removing the $\alpha$ proportion of largest and $\alpha$ proportion of smallest observations.
```

The simple mean and median are instances of the alpha trimmed mean: $\bar x_0$ and $\bar x_{0.5}$ respectively.

Here are the R implementations:

```{r}
x <- rexp(100)
mean(x) # simple mean
median(x) # median
mean(x, trim = 0.2) # alpha trimmed mean with alpha=0.2
```


#### Summary of Scale
The _scale_ of the data, sometimes known as _spread_, can be thought of its variability. 

```{definition, name="Standard Deviation"}
The standard deviation of a sample $x$, denoted $S(x)$, is defined as 
$$ S(x):=\sqrt{(n-1)^{-1} \sum (x_i-\bar x)^2} . $$
```

For reasons of robustness, we define other, more robust, measures of scale.

```{definition, name="MAD"}
The Median Absolute Deviation from the median, denoted as $MAD(x)$, is defined as
$$MAD(x):= c \: |x-x_{0.5}|_{0.5} . $$
```

where $c$ is some constant, typically set to $c=1.4826$ so that the MAD is a robust estimate of $S(x)$.

```{definition, name="IQR"}
The Inter Quantile Range of a sample $x$, denoted as $IQR(x)$, is defined as 
$$ IQR(x):= x_{0.75}-x_{0.25} .$$
```

Here are the R implementations

```{r}
sd(x) # standard deviation
mad(x) # MAD
IQR(x) # IQR
```



#### Summary of Asymmetry
The symmetry of a univariate sample is easily understood.
Summaries of asymmetry, also known as _skewness_, quantify the departure of the $x$ from a symmetric sample.

```{definition, name="Yule"}
The Yule measure of assymetry, denoted $Yule(x)$ is defined as 
$$Yule(x) := \frac{1/2 \: (x_{0.75}+x_{0.25}) - x_{0.5} }{1/2 \: IQR(x)} .$$
```

Here is an R implementation

```{r yule}
yule <- function(x){
  numerator <- 0.5 * (quantile(x,0.75) + quantile(x,0.25))-median(x) 
  denominator <- 0.5* IQR(x)
  c(numerator/denominator, use.names=FALSE)
}
yule(x)
```


#### Summary of Bivariate Continuous Data
When dealing with bivariate, or multivariate data, we can obviously compute univariate summaries for each variable separately. 
This is not the topic of this section, in which we want to summarize the association __between__ the variables, and not within them.

```{definition, name="Covariance"}
The covariance between two samples, $x$ and $y$, of same length $n$, is defined as 
$$Cov(x,y):= (n-1)^{-1} \sum (x_i-\bar x)(y_i-\bar y)  $$
```

We emphasize this is not the covariance you learned about in probability classes, since it is not the covariance between two _random variables_ but rather, between two _samples_. 
For this reasons, some authors call it the _empirical_ covariance. 

```{definition, name="Pearson's Correlation Coefficient"}
Peasrson's correlation coefficient, a.k.a. Pearson's moment product correlation, or simply, the correlation, denoted `r(x,y)`, is defined as 
$$r(x,y):=\frac{Cov(x,y)}{S(x)S(y)}. $$
```

If you find this definition enigmatic, just think of the correlation as the covariance between $x$ and $y$ after transforming each to the unitless scale of z-scores.

```{definition, name="Z-Score"}
The z-scores of a sample $x$ are defined as the mean-centered, scale normalized observations:
$$z_i(x):= \frac{x_i-\bar x}{S(x)}.$$ 
```

We thus have that $r(x,y)=Cov(z(x),z(y))$. 


#### Summary of Multivariate Continuous Data

The covariance is a simple summary of association between two variables, but it certainly may not capture the whole "story".
Things get more complicated when summarizing the relation between multiple variables. 
The most common summary of relation, is the __covariance matrix__, but we warn that only the simplest multivariate relations are fully summarized by this matrix. 

```{definition, name="Sample Covariance Matrix"}
Given $n$ observations on $p$ variables, denote $x_{i,j}$ the $i$'th observation of the $j$'th variable. 
The _sample covariance matrix_, denoted $\hat \Sigma$ is defined as 
$$\hat \Sigma_{k,l}=(n-1)^{-1} \sum_i [(x_{i,k}-\bar x_k)(x_{i,l}-\bar x_l)].$$
Put differently, the $k,l$'th entry in $\hat \Sigma$ is the sample covariance between variables $k$ and $l$.
```

```{remark}
$\hat \Sigma$ is clearly non robust.
How would you define a robust covariance matrix?
```





## Visualization
Summarizing the information in a variable to a single number clearly conceals much of the story in the sample. 
This is akin to inspecting a person using a caricature, instead of a picture. 
Visualizing the data, when possible, is more informative. 

### Categorical Data
Recalling that with categorical variables we can only count the frequency of each level, the plotting of such variables are typically variations on the _bar plot_.

#### Visualizing Univariate Categorical Data

```{r, barplot}
barplot(table(age))
```



#### Visualizing Bivariate Categorical Data
There are several generalizations of the barplot, aimed to deal with the visualization of bivariate categorical data. 
They are sometimes known as the _clustered bar plot_ and the _stacked bar plot_.
In this text, we advocate the use of the _mosaic plot_ which is also the default in R.
```{r}
plot(table1, main='Bivariate mosaic plot')
```


#### Visualizing Multivariate Categorical Data 
The _mosaic plot_ is not easy to generalize to more than two variables, but it is still possible (at the cost of interpretability).

```{r}
plot(table2.1, main='Trivaraite mosaic plot')
```


### Continuous Data

#### Visualizing Univariate Continuous Data
Unlike categirical variables, there are endlessly many way to visualize continuous variables. 
The simplest way is to look at the raw data via the `stripchart`.

```{r}
sample1 <- rexp(10) 							
stripchart(sample1)
```

Clearly, if there are many observations, the `stripchart` will be a useless line of black dots. 
We thus bin them together, and look at the frequency of each bin; this is the _histogram_.
R's `histogram` function has very good defaults to choose the number of bins.
Here is a histogram showing the counts of each bin.
```{r}
sample1 <- rexp(100) 							
hist(sample1, freq=T, main='Counts')      	
```

The bin counts can be replaced with the proportion of each bin using the `freq` argument.
```{r}
hist(sample1, freq=F, main='Proportion') 	
```


The bins of a histogram are non overlapping.
We can adopt a sliding window approach, instead of binning. 
This is the _density plot_ which is produced with the `density` function, and added to an existing plot with the `lines` function.
The `rug` function adds the original data points as ticks on the axes, and is strongly recommended to detect artifacts introduced by the binning of the histogram, or the smoothing of the density plot. 

```{r, results='hold'}
hist(sample1, freq=F, main='Frequencies') 	
lines(density(sample1))                   	
rug(sample1)
```

```{remark}
Why would it make no sense to make a table, or a barplot, of continuous data?
```

One particularly useful visualization, due to John W. Tukey, is the _boxplot_.
The boxplot is designed to capture the main phenomena in the data, and simultaneously point to outliers. 

```{r}
boxplot(sample1)	
```


#### Visualizing Bivariate Continuous Data
The bivariate counterpart of the `stipchart` is the celebrated scatter plot. 
```{r}
n <- 20
x1 <- rexp(n)
x2 <- 2* x1 + 4 + rexp(n)
plot(x2~x1)
```

Like the univariate `stripchart`, the scatter plot will be an uninformative mess in the presence of a lot of data. 
A nice bivariate counterpart of the univariate histogram is the _hexbin plot_, which tessellates the plane with hexagons, and reports their frequencies.

```{r}
library(hexbin)
n <- 2e5
x1 <- rexp(n)
x2 <- 2* x1 + 4 + rnorm(n)
plot(hexbin(x = x1, y = x2))
```



#### Visualizing Multivariate Continuous Data
Visualizing multivariate data is a tremendous challenge given that we cannot grasp $4$ dimensional spaces, nor can the computer screen present more than $2$ dimensional spaces.
We thus have several options: 
(i) To project the data to 2D. This is discussed in the Dimensionality Reduction Section \@ref(dim-reduce).
(ii) To visualize not the data, but the summaries, like the covariance matrix.

Since the covariance matrix, $\hat \Sigma$ is a matrix, it can be visualized as an image.
Note the use of the `::` operator, which is used to call a function from some package, without loading the whole package. 
We will use the `::` operator when we want to emphasize the package of origin of a function.

```{r}
covariance <- cov(longley) # The covariance of the longley dataset
lattice::levelplot(covariance)
```



## Bibliographic Notes

Like any other topic in this book, you can consult @venables2013modern.
The seminal book on EDA, written long before R was around, is @tukey1977exploratory.
For an excellent text on robust statistics see @wilcox2011introduction.



## Practice Yourself

<!--chapter:end:03-eda.Rmd-->

# Linear Models {#lm}

## Problem Setup

```{example, label='cap-experiment', name="Bottle Cap Production"}
Consider a randomized experiment designed to study the effects of temperature and pressure on the  diameter of manufactured a bottle cap.
```

```{example, label='rental', name="Rental Prices"}
Consider the prediction of rental prices given an appartment's attributes. 
```

Both examples require some statistical model, but they are very different.
The first is a _causal inference_ problem: we want to design an intervention so that we need to recover the causal effect of temperature and pressure.
The second is a _prediction_ problem, a.k.a. a _forecasting_ problem, in which we don't care about the causal effects, we just want good predictions.

In this chapter we discuss the causal problem in Example \@ref(ex:cap-experiment).
This means that when we assume a model, we assume it is the actual _data generating process_, i.e., we assume the _sampling distribution_ is well specified.
The second type of problems is discussed in the Supervised Learning Chapter \@ref(supervised).

Here are some more examples of the types of problems we are discussing.

```{example, name="Plant Growth"}
Consider the treatment of various plants with various fertilizers to study the fertilizer's effect on growth.
```

```{example, name="Return to Education"}
Consider the study of return to education by analyzing the incomes of individuals with different education years. 
```

```{example, name="Drug Effect"}
Consider the study of the effect of a new drug, by analyzing the level of blood coagulation after the administration of various amounts of the new drug. 
```

Let's present the linear model.
We assume that a response^[The "response" is also know as the "dependent" variable in the statistical literature, or the "labels" in the machine learning literature.] variable is the sum of effects of some factors^[The "factors" are also known as the "independent variable", or  "the design", in the statistical literature, and the "features", or "attributes" in the machine learning literature.].
Denoting the dependent by $y$, the factors by $x$, and the effects by $\beta$ the linear model assumption implies that 


\begin{align}
  E[y]=\sum_j x_j \beta_j=x'\beta .
  (\#eq:linear-mean)
\end{align}
Clearly, there may be other factors that affect the the caps' diameters. 
We thus introduce an error term^[The "error term" is also known as the "noise", or the "common causes of variability".], denoted by $\varepsilon$, to capture the effects of all unmodeled factors and measurement error^[You may philosophize if the measurement error is a mere instance of unmodeled factors or not, but this has no real implication for our purposes.]. 
The implied generative process of a sample of $i=1,\dots,n$ observations it thus
\begin{align}
  y_i = \sum_j x_{i,j} \beta_j + \varepsilon_i , i=1,\dots,n .
  (\#eq:linear-observed)
\end{align}
or in matrix notation
\begin{align}
  y = X \beta + \varepsilon .
  (\#eq:linear-matrix)
\end{align}


Let's demonstrate Eq.\@ref(eq:linear-observed).
In our cap example (\@ref(ex:cap-experiment)), assuming that pressure and temperature have two levels each (say, high and low), we would write $x_{i,1}=1$ if the pressure of the $i$'th measurement was set to high, and $x_{i,1}=-1$ if the __pressure__ was set to low. 
Similarly, we would write $x_{i,2}=1$, and $x_{i,2}=-1$, if the __temperature__ was set to high, or low, respectively.
The coding with $\{-1,1\}$ is known as _effect coding_.
If you prefer coding with $\{0,1\}$, this is known as _dummy coding_.

In [Galton's](https://en.wikipedia.org/wiki/Regression_toward_the_mean) classical regression problem, where we try to seek the relation between the heights of sons and fathers then $p=1$, $y_i$ is the height of the $i$'th father, and $x_i$ the height of the $i$'th son.


There are many reasons linear models are very popular:

1. Before the computer age, these were pretty much the only models that could actually be computed^[By "computed" we mean what statisticians call "fitted", or "estimated", and computer scientists call "learned".]. 
The whole Analysis of Variance (ANOVA) literature is an instance of linear models, that relies on sums of squares, which do not require a computer to work with.

1. For purposes of prediction, where the actual data generating process is not of primary importance, they are popular because they simply work. 
Why is that? 
They are simple so that they do not require a lot of data to be computed. 
Put differently, they may be biased, but their variance is small enough to make them more accurate than other models.

1. For categorical or factorial predictors, __any__ functional relation can be cast as a linear model.

1. For the purpose of _screening_, where we only want to show the existence of an effect, and are less interested in the magnitude of that effect, a linear model is enough.

1. If the true generative relation is not linear, but smooth enough, then the linear function is a good approximation via Taylor's theorem.


There are still two matters we have to attend: 
(i) How the estimate $\beta$? 
(ii) How to perform inference?

In the simplest linear models the estimation of $\beta$ is done using the method of least squares. A linear model with least squares estimation is known as Ordinary Least Squares (OLS).
The OLS problem:

\begin{align}
  \hat \beta:= argmin_\beta \{ \sum_i (y_i-x_i'\beta)^2 \},
  (\#eq:ols)
\end{align}
and in matrix notation
\begin{align}
  \hat \beta:= argmin_\beta \{ \Vert y-X\beta \Vert^2_2 \}.
  (\#eq:ols-matrix)
\end{align}

```{remark}
Personally, I prefer the matrix notation because it is suggestive of the geometry of the problem. 
The reader is referred to @friedman2001elements, Section 3.2, for more on the geometry of OLS.
```

Different software suits, and even different R packages, solve Eq.\@ref(eq:ols) in different ways so that we skip the details of how exactly it is solved. 
These are discussed in Chapters \@ref(algebra) and \@ref(convex).

The last matter we need to attend is how to do inference on $\hat \beta$.
For that, we will need some assumptions on $\varepsilon$.
A typical set of assumptions is the following:

1. __Independence__: we assume $\varepsilon_i$ are independent of everything else. 
Think of them as the measurement error of an instrument: it is independent of the measured value and of previous measurements. 
1. __Centered__: we assume that $E[\varepsilon]=0$, meaning there is no systematic error. 
1. __Normality__: we will typically assume that $\varepsilon \sim \mathcal{N}(0,\sigma^2)$, but we will later see that this is not really required. 

We emphasize that these assumptions are only needed for inference on $\hat \beta$ and not for the estimation itself, which is done by the purely algorithmic framework of OLS.

Given the above assumptions, we can apply some probability theory and linear algebra to get the distribution of the estimation error:
\begin{align}
  \hat \beta - \beta \sim \mathcal{N}(0, (X'X)^{-1} \sigma^2).
  (\#eq:ols-distribution)
\end{align}

The reason I am not too strict about the normality assumption above, is that Eq.\@ref(eq:ols-distribution) is approximately correct even if $\varepsilon$ is not normal, provided that there are many more observations than factors ($n \gg p$).

## OLS Estimation in R

We are now ready to estimate some linear models with R.
We will use the `whiteside` data from the __MASS__ package, recording the outside temperature and gas consumption, before and after an appartment's insulation.
```{r, cache=TRUE}
library(MASS)
data(MASS::whiteside)
head(whiteside) # inspect the data
```

We do the OLS estimation on the pre-insulation data with `lm` function, possibly the most important function in R.
```{r}
lm.1 <- lm(Gas~Temp, data=whiteside[whiteside$Insul=='Before',]) # OLS estimation 
```
Things to note:

- We used the tilde syntax `Gas~Temp`, reading "gas as linear function of temperature".
- The `data` argument tells R where to look for the variables Gas and Temp.
We used only observations before the insulation.
- The result is assigned to the object `lm.1`.

Alternative formulations with the same results would be
```{r, eval=FALSE}
lm.1 <- lm(y=Gas, x=Temp, data=whiteside[whiteside$Insul=='Before',]) 
lm.1 <- lm(y=whiteside[whiteside$Insul=='Before',]$Gas, x=whiteside[whiteside$Insul=='Before',]$Temp)  
```

The output is an object of class `lm`.
```{r}
class(lm.1)
```

Objects of class `lm` are very complicated. 
They store a lot of information which may be used for inference, plotting, etc.
The `str` function, short for "structure", shows us the various elements of the object.
```{r}
str(lm.1)
```

At this point, we only want $\hat \beta$ which can be extracted with the `coef` function.
```{r}
coef(lm.1)
```

Things to note:

- R automatically adds an `(Intercept)` term. 
This means we estimate $y=\beta_0 + \beta_1 Gas + \varepsilon$ and not $y=\beta_1 Gas + \varepsilon$. 
This makes sense because we are interested in the contribution of the temperature to the variability of the gas consumption about its __mean__, and not about zero.

- The effect of temperature, i.e., $\hat \beta_1$, is `r round(coef(lm.1)[[2]],2)`. 
The negative sign means that the higher the temperature, the less gas is consumed. 
The magnitude of the coefficient means that for a unit increase in the outside temperature, the gas consumption decreases by `r abs(round(coef(lm.1)[[2]],2))` units. 

We can use the `predict` function to make predictions, but we emphasize that if the purpose of the model is to make predictions, and not interpret coefficients, better skip to the Supervised Learning Chapter \@ref(supervised).
```{r, results='hold'}
plot(predict(lm.1)~whiteside[whiteside$Insul=='Before',]$Gas)
abline(0,1, lty=2)
```

The model seems to fit the data nicely.
A common measure of the goodness of fit is the _coefficient of determination_, more commonly known as the $R^2$.

(ref:R2) $R^2$.

```{definition, name='(ref:R2)'}
The coefficient of determination, denoted $R^2$, is defined as
\begin{align}
  R^2:= 1-\frac{\sum_i (y_i - \hat y_i)^2}{\sum_i (y_i - \bar y)^2},
\end{align}
where $\hat y_i$ is the model's prediction, $\hat y_i = x_i \hat \beta$.
```

It can be easily computed
```{r}
R2 <- function(y, y.hat){
  numerator <- (y-y.hat)^2 %>% sum
  denominator <- (y-mean(y))^2 %>% sum
  1-numerator/denominator
}
R2(y=whiteside[whiteside$Insul=='Before',]$Gas, y.hat=predict(lm.1))
```

This is a nice result implying that about $94\%$ of the variability in gas consumption can be attributed to changes in the outside temperature. 

Obviously, R does provide the means to compute something as basic as $R^2$, but I will let you find it for yourselves. 


## Inference
To perform inference on $\hat \beta$, in order to test hypotheses and construct confidence intervals, we need to quantify the uncertainly in the reported $\hat \beta$.
This is exactly what Eq.\@ref(eq:ols-distribution) gives us.

Luckily, we don't need to manipulate multivariate distributions manually, and everything we need is already implemented. 
The most important function is `summary` which gives us an overview of the model's fit.
We emphasize that that fitting a model with `lm` is an assumption free algorithmic step. 
Inference using `summary` is __not__ assumption free, and requires the set of assumptions leading to Eq.\@ref(eq:ols-distribution).
```{r}
summary(lm.1)
```

Things to note:

- The estimated $\hat \beta$ is reported in the `Coefficients' table, which has point estimates, standard errors, t-statistics, and the p-values of a two-sided hypothesis test for each coefficient $H_{0,j}:\beta_j=0, j=1,\dots,p$.
- The $R^2$ is reported at the bottom. The "Adjusted R-squared" is a variation that compensates for the model's complexity.
- The original call to `lm` is saved in the `Call` section.
- Some summary statistics of the residuals ($y_i-\hat y_i$) in the `Residuals` section.
- The "residuals standard error"^[Sometimes known as the Root Mean Squared Error (RMSE).] is $\sqrt{(n-p)^{-1} \sum_i (y_i-\hat y_i)^2}$. The denominator of this expression is the _degrees of freedom_, $n-p$, which can be thought of as the hardness of the problem. 

As the name suggests, `summary` is merely a summary. The full `summary(lm.1)` object is a monstrous object. 
Its various elements can be queried using `str(sumary(lm.1))`.

Can we check the assumptions required for inference?
Some.
Let's start with the linearity assumption.
If we were wrong, and the data is not arranged about a linear line, the residuals will have some shape. We thus plot the residuals as a function of the predictor to diagnose shape. 
```{r, results='hold'}
plot(residuals(lm.1)~whiteside[whiteside$Insul=='Before',]$Temp)
abline(0,0, lty=2)
```

I can't say I see any shape.
Let's fit a __wrong__ model, just to see what "shape" means.
```{r}
lm.1.1 <- lm(Gas~I(Temp^2), data=whiteside[whiteside$Insul=='Before',])
plot(residuals(lm.1.1)~whiteside[whiteside$Insul=='Before',]$Temp); abline(0,0, lty=2)
```

Things to note:

- We used `I(Temp)^2` to specify the model $Gas=\beta_0 + \beta_1 Temp^2+ \varepsilon$.
- The residuals have a "belly". 
Because they are not a cloud around the linear trend, and we have the wrong model.

To the next assumption.
We assumed $\varepsilon_i$ are independent of everything else.
The residuals, $y_i-\hat y_i$ can be thought of a sample of $\varepsilon_i$. 
When diagnosing the linearity assumption, we already saw their distribution does not vary with the $x$'s, `Temp` in our case. 
They may be correlated with themselves; a positive departure from the model, may be followed by a series of positive departures etc.
Diagnosing these _auto-correlations_ is a real art, which is not part of our course. 

The last assumption we required is normality. 
As previously stated, if $n \gg p$, this assumption can be relaxed. 
If $n \sim p$, i.e., $n$ is in the order of $p$, we need to verify this assumption.
My favorite tool for this task is the _qqplot_.
A qqplot compares the quantiles of the sample with the respective quantiles of the assumed distribution.
If quantiles align along a line, the assumed distribution if OK.
If quantiles depart from a line, then the assumed distribution does not fit the sample.
```{r}
qqnorm(resid((lm.1)))
```

The `qqnorm` function plots a qqplot against a normal distribution.
Judging from the figure, the normality assumption is quite plausible. 
Let's try the same on a non-normal sample, namely a uniformly distributed sample, to see how that would look.
```{r}
qqnorm(runif(100))
```


### Testing a Hypothesis on a Single Coefficient
The first inferential test we consider is a hypothesis test on a single coefficient. 
In our gas example, we may want to test that the temperature has no effect on the gas consumption.
The answer for that is given immediately by `summary(lm.1)`
```{r}
summary.lm1 <- summary(lm.1)
coefs.lm1 <- summary.lm1$coefficients
coefs.lm1
```
We see that the p-value for $H_{0,1}:\hat \beta_1=0$ against a two sided alternative is effectively `r round(coefs.lm1[2,4],2)`, so that $\beta_1$ is unlikely to be $0$.




### Constructing a Confidence Interval on a Single Coefficient
Since the `summary` function gives us the standard errors of $\hat \beta$, we can immediately compute $\hat \beta_j \pm 2 \sqrt{Var[\hat \beta_j]}$ to get ourselves a (roughly) $95\%$ confidence interval.
In our example the interval is
```{r}
coefs.lm1[2,1] + c(-2,2) * coefs.lm1[2,2]
```


### Multiple Regression

```{remark}
_Multiple regression_ is not to be confused with _multivariate regression_ discussed in Chapter \@ref(multivariate).
```


Our next example^[The example is taken from http://rtutorialseries.blogspot.co.il/2011/02/r-tutorial-series-two-way-anova-with.html] contains a hypothetical sample of $60$ participants who are divided into three stress reduction treatment groups (mental, physical, and medical) and two gender groups (male and female). 
The stress reduction values are represented on a scale that ranges from 1 to 5. 
This dataset can be conceptualized as a comparison between three stress treatment programs, one using mental methods, one using physical training, and one using medication across genders. 
The values represent how effective the treatment programs were at reducing participant's stress levels, with larger effects indicating higher effectiveness.

```{r, cache=TRUE}
data <- read.csv('dataset_anova_twoWay_comparisons.csv')
head(data)
```

How many observations per group?
```{r}
table(data$Treatment, data$Age)
```


Since we have two factorial predictors, this multiple regression is nothing but a _two way ANOVA_.
Let's fit the model and inspect it.
```{r}
lm.2 <- lm(StressReduction~.-1,data=data)
summary(lm.2)
```
Things to note:

- The  `StressReduction~.` syntax is read as "Stress reduction as a function of everything else".

- The `StressReduction~.-1` means that I do not want an intercept in the model, so that the baseline response is 0.

- All the (main) effects seem to be significant. 

- The data has 2 factors, but the coefficients table has 4 predictors. This is because `lm` noticed that `Treatment` and `Age` are factors. 
The numerical values of the factors are meaningless.
Instead, R has constructed a dummy variable for each level of each factor. 
The names of the effect are a concatenation of the factor's name, and its level.
You can inspect these dummy variables with the `model.matrix` command. 
```{r}
head(model.matrix(lm.2))
```
If you don't want the default dummy coding, look at `?contrasts`.


If you are more familiar with the ANOVA literature, or that you don't want the effects of each level separately, but rather, the effect of __all__ the levels of each factor, use the `anova` command.
```{r}
anova(lm.2)
```

Things to note:

- The ANOVA table, unlike the `summary` function, tests if __any__ of the levels of a factor has an effect, and not one level at a time. 
- The significance of each factor is computed using an F-test. 
- The degrees of freedom, encoding the number of levels of a factor, is given in the `Df` column.
- The StressReduction seems to vary for different ages and treatments, since both factors are significant.

As in any two-way ANOVA, we may want to ask if different age groups respond differently to different treatments. 
In the statistical parlance, this is called an _interaction_, or more precisely, an _interaction of order 2_.
```{r}
lm.3 <- lm(StressReduction~Treatment+Age+Treatment:Age-1,data=data)
```
The syntax `StressReduction~Treatment+Age+Treatment:Age-1` tells R to include main effects of Treatment, Age, and their interactions.
Here are other ways to specify the same model.
```{r, eval=FALSE}
lm.3 <- lm(StressReduction ~ Treatment * Age - 1,data=data)
lm.3 <- lm(StressReduction~(.)^2 - 1,data=data)
```
The syntax `Treatment * Age` means "mains effects with second order interactions".
The syntax `(.)^2` means "everything with second order interactions"

Let's inspect the model
```{r}
summary(lm.3)
```
Things to note:

- There are still $5$ main effects, but also $4$ interactions. 
This is because when allowing a different average response for every $Treatment*Age$ combination, we are effectively estimating $3*3=9$ cell means, even if they are not parametrized as cell means, but rather as main effect and interactions. 
- The interactions do not seem to be significant.
- The assumptions required for inference are clearly not met in this example, which is there just to demonstrate R's capabilities. 

Asking if all the interactions are significant, is asking if the different age groups have the same response to different treatments. 
Can we answer that based on the various interactions? 
We might, but it is possible that no single interaction is significant, while the combination is. 
To test for all the interactions together, we can simply check if the model without interactions is (significantly) better than a model with interactions. I.e., compare `lm.2` to `lm.3`.
This is done with the `anova` command.
```{r}
anova(lm.2,lm.3, test='F')
```
We see that `lm.3` is __not__  better than `lm.2`, so that we can conclude that there are no interactions: different ages have the same response to different treatments.


### Testing a Hypothesis on a Single Contrast
Returning to the model without interactions, `lm.2`.
```{r}
coef(summary(lm.2))
```
We see that the effect of the various treatments is rather similar. 
It is possible that all treatments actually have the same effect.
Comparing the levels of a factor is called a _contrast_.
Let's test if the medical treatment, has in fact, the same effect as the physical treatment.
```{r}
library(multcomp)
my.contrast <- matrix(c(-1,0,1,0,0), nrow =  1)
lm.4 <- glht(lm.2, linfct=my.contrast)
summary(lm.4)
```

Things to note:

- A contrast is a linear function of the coefficients. In our example $H_0:\beta_1-\beta_3=0$, which justifies the construction of `my.contrast`.
- We used the `glht` function (generalized linear hypothesis test) from the package __multcompt__.
- The contrast is significant, i.e., the effect of a medical treatment, is different than that of a physical treatment.




## Bibliographic Notes
Like any other topic in this book, you can consult @venables2013modern for more on linear models.
For the theory of linear models, I like @greene2003econometric. 


## Practice Yourself

<!--chapter:end:04-lm.Rmd-->

# Generalized Linear Models {#glm}



```{example, label='cigarettes'}
Consider the relation between cigarettes smoked, and the occurance of lung cancer. 
Do we expect the probability of cancer to be linear in the number of cigarettes? Probably not.
Do we expect the variability of events to be constant about the trend? Probably not.
```



## Problem Setup

In the Linear Models Chapter \@ref(lm), we assumed the generative process to be 
\begin{align}
  y|x=x'\beta+\varepsilon
  (\#eq:linear-mean-again)
\end{align}

This does not allow for (assumingly) non-linear relations, nor does it allow for the variability of $\varepsilon$ to change with $x$.
_Generalize linear models_ (GLM), as the name suggests, are a generalization that allow that^[Do not confuse _generalized linear models_ with [_non-linear regression_](https://en.wikipedia.org/wiki/Nonlinear_regression), or [_generalized least squares_](https://en.wikipedia.org/wiki/Generalized_least_squares). These are different things, that we do not discuss.].


To understand GLM, we recall that with the normality of $\varepsilon$, Eq.\@ref(eq:linear-mean-again) implies that 
$$
 y|x \sim \mathcal{N}(x'\beta, \sigma^2)
$$
For Example \@ref(ex:cigarettes), we would like something in the lines of 
$$
 y|x \sim Binom(1,p(x))
$$
More generally, for some distribution $F(\theta)$, with a parameter $\theta$, we would like
\begin{align}
  y|x \sim F(\theta(x))
\end{align}

Possible examples include 
\begin{align}
 y|x &\sim Poisson(\lambda(x)) \\
 y|x &\sim Exp(\lambda(x)) \\
 y|x &\sim \mathcal{N}(\mu(x),\sigma^2(x)) 
\end{align}

GLMs constrain $\theta$ to be some known function, $g$, of a linear combination of the $x$'s. 
Formally, $$\theta(x)=g(x'\beta),$$ where $$x'\beta=\beta_0 + \sum_j x_j \beta_j.$$
The function $g$ is called the _link_ function.


## Logistic Regression

The best known of the GLM class of models is the _logistic regression_ that deals with Binomial, or more precisely, Bernoulli distributed data. 
The link function in the logistic regression is the _logistic function_
\begin{align}
  g(t)=\frac{e^t}{(1+e^t)}
  (\#eq:logistic-link)  
\end{align}
 implying that 
\begin{align}
  y|x \sim Binom \left( 1, p=\frac{e^{x'\beta}}{1+e^{x'\beta}} \right)
  (\#eq:logistic)
\end{align}

Before we fit such a model, we try to justify this construction, in particular, the enigmatic link function in Eq.\@ref(eq:logistic-link).
Let's look at the simplest possible case: the comparison of two groups indexed by $x$: $x=0$ for the first, and $x=1$ for the second.
We start with some definitions.

```{definition, name="Odds"}
The _odds_, of a binary random variable, $y$, is defined as $$\frac{P(y=1)}{P(y=0)}.$$
```

Odds are the same as probabilities, but instead of of telling me there is a $66\%$ of success, they tell me the odds of success are "2 to 1".
If you ever placed a bet, the language of "odds" should not be unfamiliar to you. 

```{definition, name="Odds Ratio"}
The _odds ratio_ between two binary random variables, $y_1$ and $y_2$, is defined as the ratio between their odds. 
Formally:
$$OR(y_1,y_2):=\frac{P(y_1=1)/P(y_1=0)}{P(y_2=1)/P(y_2=0)}.$$
```

Odds ratios (OR) compares between the probabilities of two groups, only that it does not compare  them in probability scale, but rather in odds scale. 

Under the logistic link assumption, the OR between two conditions indexed by $y|x=1$ and $y|x=0$, returns:
\begin{align}
   OR(y|x=1,y|x=0) 
   = \frac{P(y=1|x=1)/P(y=0|x=1)}{P(y=1|x=0)/P(y=0|x=0)} 
   = e^{\beta_1}.  
\end{align}


The last equality demystifies the choice of the link function in the logistic regression: __it allows us to interpret $\beta$ of the logistic regression as a measure of change of binary random variables, namely, as the (log) odds-ratios due to a unit increase in $x$__.

```{remark}
Another popular link function is the normal quantile function, a.k.a., the Gaussian inverse CDF, leading to _probit regression_ instead of logistic regression.
```



### Logistic Regression with R

Let's get us some data. 
The `PlantGrowth` data records the weight of plants under three conditions: control, treatment1, and treatment2.
```{r}
head(PlantGrowth)
```

We will now `attach` the data so that its contents is available in the workspace (don't forget to `detach` afterwards, or you can expect some conflicting object names).
We will also use the `cut` function to create a binary response variable for Light, and Heavy plants (we are doing logistic regression, so we need a two-class response).
As a general rule of thumb, when we discretize continuous variables, we lose information. 
For pedagogical reasons, however, we will proceed with this bad practice. 
```{r}
attach(PlantGrowth)
weight.factor<- cut(weight, 2, labels=c('Light', 'Heavy'))
plot(table(group, weight.factor))
```


Let's fit a logistic regression, and inspect the output.
```{r, label="glm1"}
glm.1<- glm(weight.factor~group, family=binomial)
summary(glm.1)
```

Things to note:

- The `glm` function is our workhorse for all GLM models.
- The `family` argument of `glm` tells R the output is binomial, thus, performing a logistic regression.
- The `summary` function is content aware. It gives a different output for `glm` class objects than for other objects, such as the `lm` we saw in Chapter \@ref(lm). In fact, what `summary` does is merely call `summary.glm`. 
- As usual, we get the coefficients table, but recall that they are to be interpreted as (log) odd-ratios.
- As usual, we get the significance for the test of no-effect, versus a two-sided alternative. 
- The residuals of `glm` are slightly different than the `lm` residuals, and called _Deviance Residuals_. 
- For help see `?glm`, `?family`, and `?summary.glm`.

Like in the linear models, we can use an ANOVA table to check if treatments have any effect, and not one treatment at a time. 
In the case of GLMs, this is called an _analysis of deviance_ table. 
```{r}
anova(glm.1, test='LRT')
```

Things to note:

- The `anova` function, like the `summary` function, are content-aware and produce a different output for the `glm` class than for the `lm` class. All that `anova` does is call `anova.glm`.
- In GLMs there is no canonical test (like the F test for `lm`). 
We thus specify the type of test desired with the `test` argument. 
- The distribution of the weights of the plants does vary with the treatment given, as we may see from the significance of the `group` factor.
- Readers familiar with ANOVA tables, should know that we computed the GLM equivalent of a type I sum- of-squares. 
Run `drop1(glm.1, test='Chisq')` for a GLM equivalent of a type III sum-of-squares. 
- For help see `?anova.glm`.


Let's predict the probability of a heavy plant for each treatment.
```{r}
predict(glm.1, type='response')
```

Things to note:

- Like the `summary` and `anova` functions, the `predict` function is aware that its input is of `glm` class. All that `predict` does is call `predict.glm`. 
- In GLMs there are many types of predictions. The `type` argument controls which type is returned. 
- How do I know we are predicting the probability of a heavy plant, and not a light plant? Just run `contrasts(weight.factor)` to see which of the categories of the factor `weight.factor` is encoded as 1, and which as 0.
- For help see `?predict.glm`.


Let's detach the data so it is no longer in our workspace, and object names do not collide. 
```{r}
detach(PlantGrowth)
```


We gave an example with a factorial (i.e. discrete) predictor.
We can do the same with multiple continuous predictors.
```{r}
data('Pima.te', package='MASS') # Loads data
head(Pima.te)
```


```{r}
glm.2<- step(glm(type~., data=Pima.te, family=binomial))
summary(glm.2)
```

Things to note:

- We used the `~.` syntax to tell R to fit a model with all the available predictors.
- Since we want to focus on significant predictors, we used the `step` function to perform a _step-wise_ regression, i.e. sequentially remove non-significant predictors. 
The function reports each model it has checked, and the variable it has decided to remove at each step.
- The output of `step` is a single model, with the subset of selected predictors. 



## Poisson Regression

Poisson regression means we fit a model assuming $y|x \sim Poisson(\lambda(x))$.
Put differently, we assume that for each treatment, encoded as a combinations of predictors $x$, the response is Poisson distributed with a rate that depends on the predictors. 

The typical link function for Poisson regression is $g(t)=e^t$.
This means that we assume $y|x \sim Poisson(\lambda(x) = e^{x'\beta})$.
Why is this a good choice?
We again resort to the two-group case, encoded by $x=1$ and $x=0$, to understand this model:
$\lambda(x=1)=e^{\beta_0+\beta_1}=e^{\beta_0} \; e^{\beta_1}= \lambda(x=0) \; e^{\beta_1}$.
We thus see that this link function implies that a change in $x$ __multiples__ the rate of events. 
For our example^[Taken from http://www.theanalysisfactor.com/generalized-linear-models-in-r-part-6-poisson-regression-count-variables/] we inspect the number of infected high-school kids, as a function of the days since an  outbreak.
```{r}
cases <-  
structure(list(Days = c(1L, 2L, 3L, 3L, 4L, 4L, 4L, 6L, 7L, 8L, 
8L, 8L, 8L, 12L, 14L, 15L, 17L, 17L, 17L, 18L, 19L, 19L, 20L, 
23L, 23L, 23L, 24L, 24L, 25L, 26L, 27L, 28L, 29L, 34L, 36L, 36L, 
42L, 42L, 43L, 43L, 44L, 44L, 44L, 44L, 45L, 46L, 48L, 48L, 49L, 
49L, 53L, 53L, 53L, 54L, 55L, 56L, 56L, 58L, 60L, 63L, 65L, 67L, 
67L, 68L, 71L, 71L, 72L, 72L, 72L, 73L, 74L, 74L, 74L, 75L, 75L, 
80L, 81L, 81L, 81L, 81L, 88L, 88L, 90L, 93L, 93L, 94L, 95L, 95L, 
95L, 96L, 96L, 97L, 98L, 100L, 101L, 102L, 103L, 104L, 105L, 
106L, 107L, 108L, 109L, 110L, 111L, 112L, 113L, 114L, 115L), 
    Students = c(6L, 8L, 12L, 9L, 3L, 3L, 11L, 5L, 7L, 3L, 8L, 
    4L, 6L, 8L, 3L, 6L, 3L, 2L, 2L, 6L, 3L, 7L, 7L, 2L, 2L, 8L, 
    3L, 6L, 5L, 7L, 6L, 4L, 4L, 3L, 3L, 5L, 3L, 3L, 3L, 5L, 3L, 
    5L, 6L, 3L, 3L, 3L, 3L, 2L, 3L, 1L, 3L, 3L, 5L, 4L, 4L, 3L, 
    5L, 4L, 3L, 5L, 3L, 4L, 2L, 3L, 3L, 1L, 3L, 2L, 5L, 4L, 3L, 
    0L, 3L, 3L, 4L, 0L, 3L, 3L, 4L, 0L, 2L, 2L, 1L, 1L, 2L, 0L, 
    2L, 1L, 1L, 0L, 0L, 1L, 1L, 2L, 2L, 1L, 1L, 1L, 1L, 0L, 0L, 
    0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L)), .Names = c("Days", "Students"
), class = "data.frame", row.names = c(NA, -109L))
attach(cases)
head(cases) 
```

And visually:
```{r}
plot(Days, Students, xlab = "DAYS", ylab = "STUDENTS", pch = 16)
```

We now fit a model to check for the change in the rate of events as a function of the days since the outbreak.
```{r}
glm.3 <- glm(Students ~ Days, family = poisson)
summary(glm.3)
```


Things to note:

- We used `family=poisson` in the `glm` function to tell R that we assume a Poisson distribution.
- The coefficients table is there as usual. 
When interpreting the table, we need to recall that the effect, i.e. the $\hat \beta$, are __multiplicative__ due to the assumed link function.
- Each day __decreases__ the rate of events by a factor of about `r abs(round(summary(glm.3)$coef[2,1],2))`.
- For more information see `?glm` and `?family`.

```{r, echo=FALSE}
detach(cases)
```


## Extensions

As we already implied, GLMs are a very wide class of models.
We do not need to use the default link function,but more importantly, we are not constrained to Binomial, or Poisson distributed response.
For exponential, gamma, and other response distributions, see `?glm` or the references in the Bibliographic Notes section.


## Bibliographic Notes
The ultimate reference on GLMs is @mccullagh1984generalized. 
For a less technical exposition, we refer to the usual @venables2013modern.

## Practice Yourself

1. Try using `lm` for analyzing the plant growth data in `weight.factor` as a function of `group` in the `PlantGrowth` data. 

<!--chapter:end:05-glm.Rmd-->

# Linear Mixed Models {#lme}

```{example, label='fixed-effects', name="Fixed and Random Machine Effect"}
Consider the problem of testing for a change in the distribution of manufactured bottle caps.
Bottle caps are produced by several machines.
We could standardize over machines by removing each machine's average. 
This implies the within-machine variability is the only source of variability we care about. 
Alternatively, we could ignore the machine of origin. 
This second practice implies there are two sources of variability we care about: the within-machine variability, and the between-machine variability.
The former practice is known as a _fixed effects_  model.
The latter as a _random effects_ model.
```


```{example, label='random-effects', name="Fixed and Random Subject Effect"}
Consider a crossover^[If you are unfamiliar with design of experiments, have a look at Chapter 6 of my Quality Engineering [class notes](https://github.com/johnros/qualityEngineering/blob/master/Class_notes/notes.pdf).] experimenal design where each subject is given 2 types of diets, and his health condition is recorded.
We could standardize over subjects by removing the subject-wise average, before comparing diets.
This is what a paired t-test does, and implies the within-subject variability is the only source of variability we care about.
Alternatively, we could ignore the subject of origin. 
This second practice implies there are two sources of variability we care about: the within-subject variability and the betwee-subject variability.
```

The unifying theme of the above two examples, is that the variability we want to infer against has several sources. 
Which are the sources of variability that need to concern us? 
It depends on your purpose...

Mixed models are so fundamental, that they have earned many names:

- __Mixed Effects__: 
Because we may have both _fixed effects_ we want to estimate and remove, and _random effects_ which contribute to the variability to infer against.

- __Variance Components__: 
Because as the examples show, variance has more than a single source (like in the Linear Models of Chapter \@ref(lm)).

- __Hirarchial Models__: 
Because as Example \@ref(ex:random-effects) demonstrates, we can think of the sampling as hierarchical-- first sample a subject, and then sample its response. 

- __Repeated Measures__: 
Because we many have several measurements from each unit, like in \@ref(ex:random-effects).

- __Longitudinal Data__: 
Because we follow units over time, like in Example \@ref(ex:random-effects).

- __Panel Data__:
Is the term typically used in econometric for such longitudinal data. 


We now emphasize: 

1. Mixed effect models are a way to infer against the right level of variability.
Using a naive linear model (which assumes a single source of variability) instead of a mixed effects model, probably means your inference is overly anti-conservative, i.e., the estimation error is higher than you think.
1. A mixed effect models, as we will later see, is typically specified via its fixed and random effects. 
It is possible, however, to specify a mixed effects model by putting all the fixed effects into a linear model, and putting all the random effects into the covariance between $\varepsilon$. This is known as _multivariate regression_, or _multivariate analysis of variance_ (MANOVA).
For more on this view, see Chapter 8 in (the excellent) @weiss2005modeling.
1. Like in previous chapters, by "model" we refer to the assumed generative distribution, i.e., the sampling distribution. 
1. If you are using the model merely for predictions, and not for inference on the fixed effects or variance components, then stating the generative distribution may be be useful, but not necessarily. 
See the Supervised Learning Chapter \@ref(supervised) for more on prediction problems.


## Problem Setup

\begin{align}
  y|x,u = x'\beta + z'u + \varepsilon
  (\#eq:mixed-model)  
\end{align}
where $x$ are the factors with fixed effects, $\beta$, which we may want to study.
The factors $z$, with effects $u$, are the random effects which contribute to variability. 
Put differently, we state $y|x,u$ merely as a convenient way to do inference on $y|x$, instead of directly specifying $Var[y|x]$.

Given a sample of $n$ observations $(y_i,x_i,z_i)$ from model \@ref(eq:mixed-model), we will want to estimate $(\beta,u)$.
Under some assumption on the distribution of $\varepsilon$ and $z$, we can use _maximum likelihood_ (ML). 
In the context of mixed-models, however, ML is typically replaced with _restricted maximum likelihood_ (ReML), because it returns unbiased estimates of $Var[y|x]$ and ML does not.




## Mixed Models with R

We will fit mixed models with the `lmer` function from the __lme4__ package, written by the mixed-models Guru [Douglas Bates](http://www.stat.wisc.edu/~bates/).
We start with a small simulation demonstrating the importance of acknowledging your sources of variability, by fitting a linear model when a mixed model is appropriate. 
We start by creating some synthetic data. 

```{r}
n.groups <- 10
n.repeats <- 2
groups <- gl(n = n.groups, k = n.repeats)
n <- length(groups)
z0 <- rnorm(10,0,10)
z <- z0[as.numeric(groups)] # create the random effect vector.
epsilon <- rnorm(n,0,1) # create the measurement error vector.
beta0 <- 2 # create the global mean
y <- beta0 + z  + epsilon # generate synthetic sample
```

We can now fit the linear and mixed models.
```{r, lme vs lm}
lm.5 <- lm(y~z)  # fit a linear model
library(lme4)
lme.5 <- lmer(y~1|z) # fit a mixed-model
```


The summary of the linear model

```{r, label='lm5'}
summary.lm.5 <- summary(lm.5)
summary.lm.5
```

The summary of the mixed-model

```{r, label='lme5'}
summary.lme.5 <- summary(lme.5)
summary.lme.5
```
Look at the standard error of the global mean, i.e., the intercept:
for `lm` it is `r summary.lm.5$coefficients[1,2]`, and for `lme` it is `r summary.lme.5$coefficients[1,2]`.
Why this difference? 
Because `lm` discounts the group effect^[A.k.a. the _cluster effect_ in the epidemiological literature. ], while it should treat it as another source of variability.
Clearly, inference using `lm` is overly optimistic.

### A Single Random Effect

We will use the `Dyestuff` data from the __lme4__ package, which encodes the yield, in grams, of a coloring solution (dyestuff), produced in 6 batches using 5 different preparations.

```{r}
data(Dyestuff, package='lme4')
attach(Dyestuff)
head(Dyestuff)
```

And visually

```{r}
plot(Yield~Batch)
```

If we want to do inference on the mean yield, we need to account for the two sources of variability: the batch effect, and the measurement error. 
We thus fit a mixed model, with an intercept and random batch effect, which means this is it not a bona-fide mixed-model, but rather, a simple random-effect model.

```{r random intercept}
lme.1<- lmer( Yield ~ 1  | Batch  , Dyestuff )
summary(lme.1)
```

Things to note:

- As usual, `summary` is content aware and has a different behavior for `lme` class objects.
- The syntax `Yield ~ 1  | Batch` tells R to fit a model with a global intercept (`1`) and a random Batch effect (`|Batch`). More on that later. 
- The output distinguishes between random effects, a source of variability, and fixed effect, which's coefficients we want to study.
- Were we not interested in the variance components, and only in the coefficients or predictions, an (almost) equivalent `lm` formulation is `lm(Yield ~ Batch)`.

Some utility functions let us query the `lme` object. 
The function `coef` will work, but will return a cumbersome output. Better use `fixef` to extract the fixed effects, and `ranef` to extract the random effects.
The model matrix (of the fixed effects alone), can be extracted with `model.matrix`, and predictions made with `predict`.
Note, however, that predictions with mixed-effect models are (i) a delicate matter, and (ii) better treated as prediction problems as in the Supervised Learning Chapter \@ref(supervised).


### Multiple Random Effects

Let's make things more interesting by allowing more than one random effect. 
One-way ANOVA can be thought of as the fixed-effects counterpart of the single random effect.
Our next example, involving two random effects, can be though of as the Two-way ANOVA counterpart. 

In the `Penicillin` data, we measured the diameter of spread of an organism, along the plate used (a to x), and penicillin type (A to F). 

```{r}
detach(Dyestuff)
head(Penicillin)
```

One sample per combination:

```{r}
attach(Penicillin)
table(sample, plate)
```

And visually:

```{r}
lattice::dotplot(reorder(plate, diameter) ~ diameter,data=Penicillin,
              groups = sample,
              ylab = "Plate", xlab = "Diameter of growth inhibition zone (mm)",
              type = c("p", "a"), auto.key = list(columns = 6, lines = TRUE))
```

Let's fit a mixed-effects model with a random plate effect, and a random sample effect:

```{r}
lme.2 <- lmer ( diameter ~  1+ (1| plate ) + (1| sample ) , Penicillin )
fixef(lme.2) # Fixed effects
ranef(lme.2) # Random effects
```

Things to note:

- The syntax `1+ (1| plate ) + (1| sample )` fits a global intercept (mean), a random plate effect, and a random sample effect.
- Were we not interested in the variance components, an (almost) equivalent `lm` formulation is `lm(diameter ~ plate + sample)`.



Since we have two random effects, we may compute the variability of the global mean (the only fixed effect) as we did before. 
Perhaps more interestingly, we can compute the variability in the response, for a particular plate or sample type.

```{r}
random.effect.lme2 <- ranef(lme.2, condVar = TRUE) 
qrr2 <- lattice::dotplot(random.effect.lme2, strip = FALSE)
```

Things to note:

- The `condVar` argument of the `ranef` function tells R to compute the variability in response conditional on each random effect at a time. 
- The `dotplot` function, from the __lattice__ package, is only there for the fancy plotting.

Variability in response for each plate, over various sample types:

```{r}
print(qrr2[[1]]) 
```

Variability in response for each sample type, over the various plates:

```{r}
print(qrr2[[2]])  
```

### A Full Mixed-Model

In the `sleepstudy` data, we recorded the reaction times to a series of tests (`Reaction`), after various subject (`Subject`) underwent various amounts of sleep deprivation (`Day`).

```{r}
data(sleepstudy)
lattice::xyplot(Reaction ~ Days | Subject, sleepstudy, aspect = "xy",
             layout = c(9,2), type = c("g", "p", "r"),
             index.cond = function(x,y) coef(lm(y ~ x))[1],
             xlab = "Days of sleep deprivation",
             ylab = "Average reaction time (ms)")
```

We now want to estimate the (fixed) effect of the days of deprivation, while allowing each subject to have his/hers own effect.
Put differently, we want to estimate a _random slope_ for variable `day` for each subject.
The fixed `Days` effect can be thought of as the average slope over subjects.

```{r random slope}
lme.3 <- lmer ( Reaction ~ Days + ( Days | Subject ) , data= sleepstudy )
```

Things to note:

- We used the `Days|Subect` syntax to tell R we want to fit the model `~Days` within each subject.
- Were we fitting the model for purposes of prediction only, an (almost) equivalent `lm` formulation is `lm(Reaction~Days*Subject)`.


The fixed (i.e. average) day effect is:

```{r}
fixef(lme.3)
```

The variability in the average response (intercept) and day effect is

```{r}
ranef(lme.3)
```

Did we really need the whole `lme` machinery to fit a within-subject linear regression and then average over subjects?
The answer is yes.
The assumptions on the distribution of random effect, namely, that they are normally distributed, allows us to pool information from one subject to another. In the words of John Tukey: "we borrow strength over subjects".
Is this a good thing? If the normality assumption is true, it certainly is.
If, on the other hand, you have a lot of samples per subject, and you don't need to "borrow strength" from one subject to another, you can simply fit within-subject linear models without the mixed-models machinery.

To demonstrate the "strength borrowing", here is a comparison of the subject-wise intercepts of the mixed-model, versus a subject-wise linear model. They are not the same.

```{r, echo=FALSE}
library(lattice)
df <- coef(lmList(Reaction ~ Days | Subject, sleepstudy))
fclow <- subset(df, `(Intercept)` < 251)
fchigh <- subset(df, `(Intercept)` > 251)
cc1 <- as.data.frame(coef(lme.3)$Subject)
names(cc1) <- c("A", "B")
df <- cbind(df, cc1)
ff <- fixef(lme.3)
with(df,
     print(xyplot(`(Intercept)` ~ Days, aspect = 1,
                  x1 = B, y1 = A,
                  panel = function(x, y, x1, y1, subscripts, ...) {
                    panel.grid(h = -1, v = -1)
                    x1 <- x1[subscripts]
                    y1 <- y1[subscripts]
                    larrows(x, y, x1, y1, type = "closed", length = 0.1,
                            angle = 15, ...)
                    lpoints(x, y,
                            pch = trellis.par.get("superpose.symbol")$pch[2],
                            col = trellis.par.get("superpose.symbol")$col[2])
                    lpoints(x1, y1,
                            pch = trellis.par.get("superpose.symbol")$pch[1],
                            col = trellis.par.get("superpose.symbol")$col[1])
                    lpoints(ff[2], ff[1], 
                            pch = trellis.par.get("superpose.symbol")$pch[3],
                            col = trellis.par.get("superpose.symbol")$col[3])
                    ltext(fclow[,2], fclow[,1], row.names(fclow),
                          adj = c(0.5, 1.7))
                    ltext(fchigh[,2], fchigh[,1], row.names(fchigh),
                          adj = c(0.5, -0.6))
                  },
                  key = list(space = "top", columns = 3,
                             text = list(c("Mixed model", "Within-group", "Population")),
                             points = list(col = trellis.par.get("superpose.symbol")$col[1:3],
                                           pch = trellis.par.get("superpose.symbol")$pch[1:3]))
     )))

```

Here is a comparison of the random-day effect from `lme` versus a subject-wise linear model. They are not the same.

```{r, echo=FALSE}
print(xyplot(Reaction ~ Days | Subject, sleepstudy, aspect = "xy",
             layout = c(9,2), type = c("g", "p", "r"),
             coef.list = df[,3:4],
             panel = function(..., coef.list) {
               panel.xyplot(...)
               panel.abline(as.numeric(coef.list[packet.number(),]),
                            col.line = trellis.par.get("superpose.line")$col[2],
                            lty = trellis.par.get("superpose.line")$lty[2]
               )
               panel.abline(fixef(lme.3),
                            col.line = trellis.par.get("superpose.line")$col[4],
                            lty = trellis.par.get("superpose.line")$lty[4]
               )
             },
             index.cond = function(x,y) coef(lm(y ~ x))[1],
             xlab = "Days of sleep deprivation",
             ylab = "Average reaction time (ms)",
             key = list(space = "top", columns = 3,
                        text = list(c("Within-subject", "Mixed model", "Population")),
                        lines = list(col = trellis.par.get("superpose.line")$col[c(2:1,4)],
                                     lty = trellis.par.get("superpose.line")$lty[c(2:1,4)]))))
```

```{r}
detach(Penicillin)
```


## The Variance-Components View
<!-- inference on the random effects, not fixed effects.  -->


## Bibliographic Notes
Most of the examples in this chapter are from the documentation of the __lme4__ package [@lme4]. 
For a more theoretical view see @weiss2005modeling or @searle2009variance.
As usual, a hands on view can be found in @venables2013modern.

## Practice Yourself

<!--chapter:end:06-lme.Rmd-->

# Multivariate Data Analysis {#multivariate}

The term "multivariate data analysis" is so broad and so overloaded, that we start by clarifying what is discussed and what is not discussed in this chapter.
Broadly speaking, we will discuss statistical _inference_, and leave more "exploratory flavored" matters like clustering, and visualization, to the Unsupervised Learning Chapter \@ref(unsupervised).

More formally, let $y$ be a $p$ variate random vector, with $E[y]=\mu$.
Here is the set of problems we will discuss, in order of their statistical difficulty. 

- __Signal detection__: 
a.k.a. _multivariate hypothesis testing_, i.e., testing if $\mu$ equals $\mu_0$ and for $\mu_0=0$ in particular.
- __Signal counting__: 
Counting the number of elements in $\mu$ that differ from $\mu_0$, and for $\mu_0=0$ in particular.
- __Signal identification__:
a.k.a. _multiple testing_, i.e., testing which of the elements in $\mu$ differ from $\mu_0$ and for $\mu_0=0$ in particular. 
- __Signal estimation__: 
a.k.a. _selective inference_, i.e., estimating the magnitudes of the departure of $\mu$ from $\mu_0$, and for $\mu_0=0$ in particular. 
- __Multivariate Regression__:
a.k.a. _MANOVA_ in statistical literature, and _structured learning_ in the machine learning literature. 
- __Graphical Models__:
Learning _graphical models_ deals with the fitting/learning the multivariate distribution of $y$. 
In particular, it deals with the identification of independencies between elements of $y$. 


```{example, label='icu'}
Consider the problem of a patient monitored in the intensive care unit. 
At every minute the monitor takes $p$ physiological measurements: blood pressure, body temperature, etc.
The total number of minutes in our data is $n$, so that in total, we have $n \times p$ measurements, arranged in a matrix. 
We also know the typical measurements for this patient when healthy: $\mu_0$.

Signal detection means testing if the patient's measurement depart in any way from his healthy state, $\mu_0$.
Signal counting means measuring _how many_ measurement depart from the healthy state.
Signal identification means pin-pointing which of the physiological measurements depart from his healthy state. 
Signal estimation means estimating the magnitude of the departure from the healthy state. 
Multivaraite regression means finding the factors which many explain the departure from the healthy state. 
Fitting a distribution means finding the joint distribution of the physiological measurements, and in particular, their dependencies and independenceis. 
```




```{remark}
In the above, "signal" is defined in terms of $\mu$. 
It is possible that the signal is not in the location, $\mu$, but rather in the covariance, $\Sigma$. 
We do not discuss these problems here, and refer the reader to @nadler2008finite.
```



## Signal Detection
Signal detection deals with the detection of the departure of $\mu$ from some $\mu_0$, and especially, $\mu_0=0$.
This problem can be thought of as the multivariate counterpart of the univariate hypothesis test. 
Indeed, the most fundamental approach is a mere generalization of the t-test, known as _Hotelling's $T^2$ test_. 

Recall the univariate t-statistic of a data vector $x$ of length $n$:
\begin{align}
  t^2(x):= \frac{(\bar{x}-\mu_0)^2}{Var[\bar{x}]}= (\bar{x}-\mu_0)Var[\bar{x}]^{-1}(\bar{x}-\mu_0),
  (\#eq:t-test)
\end{align}
where $Var[\bar{x}]=S^2(x)/n$, and $S^2(x)$ is the unbiased variance estimator $S^2(x):=(n-1)^{-1}\sum (x_i-\bar x)^2$.

Generalizing Eq\@ref(eq:t-test) to the multivariate case: 
$\mu_0$ is a $p$-vector, $\bar x$ is a $p$-vector, and $Var[\bar x]$ is a $p \times p$ matrix of the covariance between the $p$ coordinated of $\bar x$. 
When operating with vectors, the squaring becomes a quadratic form, and the division becomes a matrix inverse. 
We thus have
\begin{align}
  T^2(x):= (\bar{x}-\mu_0)' Var[\bar{x}]^{-1} (\bar{x}-\mu_0),
  (\#eq:hotelling-test)
\end{align}
which is the definition of Hotelling's $T^2$ test statistic. 
We typically denote the covariance between coordinates in $x$ with $\hat \Sigma(x)$, so that 
$\widehat \Sigma_{k,l}:=\widehat {Cov}[x_k,x_l]=(n-1)^{-1} \sum (x_{k,i}-\bar x_k)(x_{l,i}-\bar x_l)$.
Using the $\Sigma$ notation, Eq.\@ref(eq:hotelling-test) becomes 
\begin{align}
  T^2(x):= n (\bar{x}-\mu_0)' \hat \Sigma(x)^{-1} (\bar{x}-\mu_0),
\end{align}
which is the standard notation of Hotelling's test statistic.

To discuss the distribution of Hotelling's test statistic we need to introduce some vocabulary^[This vocabulary is not standard in the literature, so when you read a text, you need to verify yourself what the author means.]:

1. __Low Dimension__:
We call a problem _low dimensional_ if $n \gg p$, i.e. $p/n \approx 0$.
This means there are many observations per estimated parameter.
1. __High Dimension__: 
We call a problem _high dimensional_ if $p/n \to c$, where $c\in (0,1)$.
This means there are more observations than parameters, but not many.
1. __Very High Dimension__: 
We call a problem _very high dimensional_ if $p/n \to c$, where $1<c<\infty$.
This means there are less observations than parameter.
1. __Extremely high dimensional__: 
We call a problem __extremely high dimensional__ if $p/n \to \infty$.
This means there are many more parameters than observations.

Hotelling's $T^2$ test can only be used in the low dimensional regime. 
For some intuition on this statement, think of taking $n=20$ measurements of $p=100$ physiological variables. 
We seemingly have $20$ observations, but there are $100$ unknown quantities in $\mu$. 
Would you trust your conclusion that $\bar x$ is different than $\mu_0$ based on merely $20$ observations. 

Put formally:
We cannot compute Hotelling's test when $n<p$ because $\hat \Sigma$ is simply not invertible-- this is an algebraic problem.
We cannot compute Hotelling's test when $p/n \to c > 0$ because the signal-to-noise is very low-- this is a statistical problem. 

Only in the low dimensional case can we compute and trust Hotelling's test. 
When $n \gg p$ then $T^2(x)$ is roughly $\chi^2$ distributed with $p$ degrees of freedom. 
The F distribution may also be found in the literature in this context, and will appear if assuming the $n$ $p$-vectors are independent, and $p$-variate Gaussian. 
This F distribution is non-robust the underlying assumptions, so from a practical point of view, I would not trust the Hotelling test unless $n \gg p$.



### Signal Detection with R

Let's generate some data with no signal.

```{r}
library(mvtnorm)
n <- 1e2
p <- 1e1
mu <- rep(0,p) # no signal
x <- rmvnorm(n = n, mean = mu)
dim(x)
image(x)
```

Now make our own Hotelling function.

```{r}
hotellingOneSample <- function(x, mu0=rep(0,ncol(x))){
  n <- nrow(x)
  p <- ncol(x)
  stopifnot(n > 5* p)
  bar.x <- colMeans(x)
  Sigma <- var(x)
  Sigma.inv <- solve(Sigma)
  T2 <- n * (bar.x-mu0) %*% Sigma.inv %*% (bar.x-mu0)
  p.value <- pchisq(q = T2, df = p, lower.tail = FALSE)
  return(list(statistic=T2, pvalue=p.value))
}

hotellingOneSample(x)
```

Things to note:

- `stopifnot(n > 5 * p)` is a little verification to check that the problem is indeed low dimensional. Otherwise, the $\chi^2$ approximation cannot be trusted.  
- `solve` returns a matrix inverse. 
- `%*%` is the matrix product operator (see also `crossprod()`).
- A function may return only a single object, so we wrap the statistic and its p-value in a `list` object.



Just for verification, we compare our home made Hotelling's test, to the implementation in the __rrcov__ package. 
The results look good!

```{r}
rrcov::T2.test(x)
```






## Signal Counting 
There are many ways to approach the _signal counting_ problem. 
For the purposes of this book, however, we will not discuss them directly, and solve the signal counting problem as a signal identification problem: if we know __where__ $\mu$ departs from $\mu_0$, we only need to count coordinates to solve the signal counting problem. 


## Signal Identification {#identification}
The problem of _signal identification_ is also known as _selective testing_, or more commonly as _multiple testing_.

In the ANOVA literature, an identification stage will typically follow a detection stage.
These are known as the _omnibus F test_, and _post-hoc_ tests, respectively.
In the multiple testing literature there will typically be no preliminary detection stage. It is typically assumed that signal is present, and the only question is "where?"


The first question when approaching a multiple testing problem is "what is an error"?
Is an error declaring a coordinate in $\mu$ to be different than $\mu_0$ when it is actually not?
Is an error the proportion of such false declarations.
The former is known as the _family wise error rate_ (FWER), and the latter as the _false discovery rate_ (FDR).


### Signal Identification in R

One (of many) ways to do signal identification involves the `stats::p.adjust` function.
[TODO: clarify why use `p.adjust`?]
The function takes as inputs a $p$-vector of __p-values__. 
This implies that: 
(i) you are assumed to be able to compute the p-value of each the $p$ hypothesis tested; one hypothesis for every coordinate in $\mu$. 
(ii) unlike the Hotelling test, we do not try to estimate the covariance between coordinates. Not because it is not important, but rather, because the methods we will use apply to a wide variety of covariances, so the covariance does not need to be estimated. 

We start be generating some multivariate data and computing the coordinate-wise (i.e. hypothesis-wise) p-value.

```{r}
library(mvtnorm)
n <- 1e1
p <- 1e2
mu <- rep(0,p)
x <- rmvnorm(n = n, mean = mu)
dim(x)
image(x)
```

We now compute the pvalues of each coordinate.
We use a coordinate-wise t-test. 
Why a t-test? Because for the purpose of demonstration we want a simple test. In reality, you may use any test that returns valid p-values. 

```{r}
p.values <- apply(X = x, MARGIN = 2, FUN = function(y) t.test(y)$p.value) 
plot(p.values, type='h')
```

Things to note:

- We used the `apply` function to apply the same function to each column of `x`. 
- The output, `p.values`, is a vector of 100 p-values. 

We are now ready to do the identification, i.e., find which coordinate of $\mu$ is different than $\mu_0=0$.
The workflow is: 
(i) Compute an `adjusted p-value`. 
(ii) Compare the adjusted p-value to the desired error level.

If we want $FWER \leq 0.05$, meaning that we allow a $5\%$ probability of making any mistake, we will use the `method="holm"` argument of `p.adjust`.

```{r holm}
alpha <- 0.05
p.values.holm <- p.adjust(p.values, method = 'holm' )
table(p.values.holm < alpha)
```

If we want $FDR \leq 0.05$, meaning that we allow the proportion of false discoveries to be no larger than $5\%$, we use the `method="BH"` argument of `p.adjust`.

```{r bh}
alpha <- 0.05
p.values.BH <- p.adjust(p.values, method = 'BH' )
table(p.values.BH < alpha)
```

We now inject some signal in $\mu$ just to see that the process works.
We will artificially inject signal in the first 10 coordinates.

```{r}
mu[1:10] <- 2 # inject signal
x <- rmvnorm(n = n, mean = mu) # generate data
p.values <- apply(X = x, MARGIN = 2, FUN = function(y) t.test(y)$p.value) 
p.values.BH <- p.adjust(p.values, method = 'BH' )
which(p.values.BH < alpha)
```

Indeed- we are now able to detect that the first coordinates carry signal, because their respective coordinate-wise null hypotheses have been rejected. 


## Signal Estimation

The estimation of the elements of $\mu$ is a seemingly straightforward task.
This is not the case, however, if we estimate only the elements that were selected because they were significant (or any other data-dependent criterion).
Clearly, estimating only significant entries will introduce a bias in the estimation. 
In the statistical literature, this is known as _selection bias_.
Selection bias also occurs when you perform inference on regression coefficients after some model selection, say, with a lasso, or a forward search^[You might find this shocking, but it does mean that you cannot trust the `summary` table of a model that was selected from a multitude of models.].

Selective inference is a complicated and active research topic so we will not offer any off-the-shelf solution to the matter.
The curious reader is invited to read @rosenblatt2014selective, @javanmard2014confidence, or [Will Fithian's](http://www.stat.berkeley.edu/~wfithian/) PhD thesis [@fithian2015topics] for more on the topic. 




## Multivariate Regression

_Multivaraite regression_, a.k.a. _MANOVA_, similar to [_structured learning_](https://en.wikipedia.org/wiki/Structured_prediction) in machine learning, is simply a regression problem where the outcome, $y$, is not scalar values but vector valued. 
It is not to be confused with _multiple regression_ where the predictor, $x$, is vector valued, but the outcome is scalar. 

If the linear models generalize the two-sample t-test from two, to multiple populations, then multivariate regression generalizes Hotelling's test in the same way.


### Multivariate Regression with R
TODO




## Graphical Models

Fitting a multivariate distribution, i.e. learning a _graphical model_, is a very hard task.
To see why, consider the problem of $p$ continuous variables. 
In the simplest case, where we can assume normality, fitting a distributions means estimating the $p$ parameters in the expectation, $\mu$, and $p(p+1)/2$ parameters in the covariance, $\Sigma$. 
The number of observations required for this task, $n$, may be formidable.

A more humble task, is to identify __independencies__, known as _structure learning_ in the machine learning literature. 
Under the multivariate normality assumption, this means identifying zero entries in $\Sigma$, or more precisely, zero entries in $\Sigma^{-1}$. 
This task can be approached as a __signal identification__ problem (\@ref(identification)).
The same solutions may be applied even if dealing with $\Sigma$ instead of $\mu$. 

If multivariate normality cannot be assumed, then identifying independencies cannot be done via the covariance matrix $\Sigma$ and more elaborate algorithms are required. 


### Graphical Models in R
TODO




## Biblipgraphic Notes
For a general introduction to multivariate data analysis see @anderson2004introduction.
For an R oriented introduction, see @everitt2011introduction.
For more on the difficulties with high dimensional problems, see @bai1996effect.
For more on multiple testing, and signal identification, see @efron2012large.
For more on the choice of your error rate see @rosenblatt2013practitioner.
For an excellent reivew on graphical models see @kalisch2014causal. 
Everything you need on graphical models, Bayesian belief networks, and structure learning in R, is collected in the [Task View](https://cran.r-project.org/web/views/gR.html).

## Practice Yourself

<!--chapter:end:07-multivariate.Rmd-->

# Supervised Learning {#supervised}

Machine learning is very similar to statistics, but it is certainly not the same.
As the name suggests, in machine learning we want machines to learn.
This means that we want to replace hard-coded expert algorithm, with data-driven self-learned algorithm.

There are many learning setups, that depend on what information is available to the machine. 
The most common setup, discussed in this chapter, is _supervised learning_.
The name takes from the fact that by giving the machine data samples with known inputs (a.k.a. features) and desired outputs (a.k.a. labels), the human is effectively supervising the learning. 
If we think of the inputs as predictors, and outcomes as predicted, it is no wonder that supervised learning is very similar to statistical prediction.
When asked "are these the same?" I like to give the example of internet fraud.
If you take a sample of fraud "attacks", a statistical formulation of the problem is highly unlikely. 
This is because fraud events are not randomly drawn from some distribution, but rather, arrive from an adversary learning the defenses and adapting to it. 
This instance of supervised learning is more similar to game theory than statistics. 

Other types of machine learning problems include [@sammut2011encyclopedia]: 

- __Unsupervised learning__: 
See Chapter \@ref(unsupervised).

- __Semi supervised learning__: 
Where only part of the samples are labeled. 
A.k.a. _co-training_, _learning from labeled and unlabeled data_, _transductive learning_. 

- __Active learning__: 
Where the machine is allowed to query the user for labels. Very similar to _adaptive design of experiments_. 

- __Learning on a budget__: 
A version of active learning where querying for labels induces variable costs.

- __Reinforcement learning__:  
Similar to active learning, in that the machine may query for labels. 
Different from active learning, in that the machine does not receive labels, but _rewards_. 

- __Structure learning__: 
When predicting objects with structure such as dependent vectors, graphs, images, tensors, etc.

- __Manifold learning__: An instance of unsupervised learning, where the goal is to reduce the dimension of the data by embedding it into a lower dimensional manifold. 
A.k.a. _support estimation_.

- __Learning to learn__: 
Deals with the carriage of "experience" from one learning problem to another. 
A.k.a. _cummulative learning_, _knowledge transfer_, and _meta learning_.





## Problem Setup

We now present the _empirical risk minimization_ (ERM) approach to supervised learning, a.k.a. _M-estimation_ in the statistical literature. 

```{remark}
We do not discuss purely algorithmic approaches such as K-nearest neighbour and _kernel smoothing_ due to space constraints. 
For a broader review of supervised learning, see the Bibliographic Notes.
```


Given $n$ samples with inputs $x$ from some space $\mathcal{X}$ and desired outcome, $y$, from some space $\mathcal{Y}$.
Samples, $(x,y)$ have some distribution we denote $P$.
We want to learn a function that maps inputs to outputs. 
This function is called a _hypothesis_, or _predictor_, or _classifier_, denoted $f$, that belongs to a hypothesis class $\mathcal{F}$ such that $f:\mathcal{X} \to \mathcal{Y}$.
We also choose some other function that fines us for erroneous prediction.
This function is called the _loss_, and we denote it by $l:\mathcal{Y}\times \mathcal{Y} \to \mathbb{R}^+$.

```{remark}
The _hypothesis_ in machine learning is only vaguely related the _hypothesis_ in statistical testing, which is quite confusing.
```

```{remark}
The _hypothesis_ in machine learning is not a bona-fide _statistical model_ since we don't assume it is the data generating process, but rather some function which we choose for its good predictive performance. 
```

The fundamental task in supervised (statistical) learning is to recover a hypothesis that minimizes the average loss in the sample, and not in the population.
This is know as the _risk minimization problem_.

```{definition, name="Risk Function"}
The _risk function_, a.k.a. _generalization error_, or _test error_, is the population average loss of a predictor $f$:
\begin{align}
  R(f):=E_P[l(f(x),y)].
\end{align}
```

The best predictor, is the risk minimizer:
\begin{align}
  f^* := argmin_f \{R(f)\}.
  (\#eq:risk)  
\end{align}

To make things more explicit, $f$ may be a linear function, and $l$ a squared error loss, in which case problem \@ref(eq:risk) collapses to 
\begin{align}
  f^* := argmin_\beta \{ E_{P(x,y)}[(x'\beta-y)^2] \}.
\end{align}

Another fundamental problem is that we do not know the distribution of all possible inputs and outputs, $P$.
We typically only have a sample of $(x_i,y_i), i=1,\dots,n$.
We thus state the _empirical_ counterpart of \@ref(eq:risk), which consists of minimizing the average loss.
This is known as the _empirical risk miminization_ problem (ERM).

```{definition, name="Empirical Risk"}
The _empirical risk function_, a.k.a. _in-sample error_, or _train error_, is the sample average loss of a predictor $f$:
\begin{align}
  R_n(f):= \sum_i l(f(x_i),y_i).
\end{align}
```

A good candidate predictor, $\hat f$, is thus the _empirical risk minimizer_:
\begin{align}
  \hat f := argmin_f \{ R_n(f) \}.
  (\#eq:erm)  
\end{align}

Making things more explicit again by using a linear hypothesis with squared loss, we see that the empirical risk minimization problem collapses to an ordinary least-squares problem:
\begin{align}
  \hat f := argmin_\beta \{ \sum_i (x_i\beta - y_i)^2 \}.
\end{align}

When data samples are assumingly independent, then maximum likelihood estimation is also an instance of ERM, when using the (negative) log likelihood as the loss function.

If we don't assume any structure on the hypothesis, $f$, then $\hat f$ from \@ref(eq:erm) will interpolate the data, and $\hat f$ will be a very bad predictor. 
We say, it will _overfit_ the observed data, and will have bad performance on new data.

We have several ways to avoid overfitting:

1. Restrict the hypothesis class $\mathcal{F}$ (such as linear functions).
1. Penalize for the complexity of $f$. The penalty denoted by $\Vert f \Vert$.
1. Unbiased risk estimation, where we deal with the overfitted optimism of the empirical risk by debiasing it.


### Common Hypothesis Classes
Some common hypothesis classes, $\mathcal{F}$, with restricted complexity, are:

1. __Linear hypotheses__: such as linear models, GLMs, and (linear) support vector machines (SVM).
1. __Neural networks__: a.k.a. _feed-forward_ neural nets, _artificial_ neural nets, and the celebrated class of _deep_ neural nets.
1. __Tree__: a.k.a. _decision rules_, is a class of hypotheses which can be stated as "if-then" rules. 
1. __Reproducing Kernel Hilbert Space__: a.k.a. RKHS, is a subset of "the space of all functions^[It is even a subset of the Hilbert space, itself a subset of the space of all functions.]" that is both large enough to capture very complicated relations, but small enough so that it is less prone to overfitting, and also surprisingly simple to compute with. 
1. __Ensembles__: a "meta" hypothesis class, which consists of taking multiple hypotheses, possibly from different classes, and combining them. 


### Common Complexity Penalties

The most common complexity penalty applies to classes that have a finite dimensional parametric representation, such as the class of linear predictors, parametrized via its coefficients $\beta$.
In such classes we may penalize for the norm of the parameters.
Common penalties include:

1. __Ridge penalty__: penalizing the $l_2$ norm of the parameter. I.e. $\Vert f \Vert=\Vert \beta \Vert_2^2=\sum_j \beta_j^2$. 
1. __Lasso penalty__: penalizing the $l_1$ norm of the parameter. I.e., $\Vert f \Vert=\Vert \beta \Vert_1=\sum_j |\beta_j|$
1. __Elastic net__: a combination of the lasso and ridge penalty. I.e. ,$\Vert f \Vert= \alpha \Vert \beta \Vert_2^2 + (1-\alpha) \Vert \beta \Vert_1$.

If the hypothesis class $\mathcal{F}$ does not admit a finite dimensional parametric representation, we may penalize it with some functional norm such as $\Vert f \Vert=\sqrt{\int f(t)^2 dt}$. 


### Unbiased Risk Estimation
The fundamental problem of overfitting, is that the empirical risk, $R_n(\hat f)$, is downward biased to the population risk,  $R(\hat f)$.
Formally: $$ R_n(\hat f)<R_n(f^*) $$
Why is that? 
Think of estimating a population's mean with the sample minimum. 
It can be done, but the minimum has to be debiased for it to estimate the population mean.
Unbiased estimation of $R(f)$ broadly fall under: 
(a) purely algorithmic _resampling_ based approaches, and 
(b) theory driven estimators.

1. __Train-Validate-Test__: 
The simplest form of validation is to split the data. 
A _train_ set to train/estimate/learn $\hat f$. 
A _validation_ set to compute the out-of-sample expected loss, $R(\hat f)$, and pick the best performing predictor. 
A _test_ sample to compute the out-of-sample performance of the selected hypothesis.
This is a very simple approach, but it is very "data inefficient", thus motivating the next method.

1. __V-Fold Cross Validation__: 
By far the most popular risk estimation algorithm, in _V-fold CV_ we "fold" the data into $V$ non-overlapping sets. 
For each of the $V$ sets, we learn $\hat f$ with the non-selected fold, and assess $R(\hat f)$) on the selected fold. 
We then aggregate results over the $V$ folds, typically by averaging. 

1. __AIC__:
Akaike's information criterion (AIC) is a theory driven correction of the empirical risk, so that it is unbiased to the true risk.
It is appropriate when using the likelihood loss. 

1. __Cp__: 
Mallow's Cp is an instance of AIC for likelihood loss under normal noise.

Other theory driven unbiased risk estimators include the _Bayesian Information Criterion_ (BIC, aka SBC, aka SBIC), the _Minimum Description Length_ (MDL), _Vapnics Structural Risk Minimization_ (SRM), the _Deviance Information Criterion_ (DIC), and the _Hannan-Quinn Information Criterion_ (HQC).

Other resampling based unbiased risk estimators include resampling __without replacement__ algorithms like _delete-d cross validation_ with its many variations, and __resampling with replacement__, like the _bootstrap_, with its many variations.



### Collecting the Pieces

An ERM problem with regularization will look like
\begin{align}
  \hat f := argmin_{f \in \mathcal{F}} \{ R_n(f)  + \lambda \Vert f \Vert \}.
  (\#eq:erm-regularized)  
\end{align}

Collecting ideas from the above sections, a typical supervised learning pipeline will include: choosing the hypothesis class, choosing the penalty function and level, unbiased risk estimator. 
We emphasize that choosing the penalty function, $\Vert f \Vert$ is not enough, and we need to choose how "hard" to apply it. 
This if known as the _regularization level_, denoted by $\lambda$ in Eq.\@ref(eq:erm-regularized).

Examples of such combos include:

1. Linear regression, no penalty, train-validate test.
1. Linear regression, no penalty, AIC.
1. Linear regression, $l_2$ penalty, V-fold CV. This combo is typically known as _ridge regression_.
1. Linear regression, $l_1$ penalty, V-fold CV. This combo is typically known as _lasso regression_.
1. Linear regression, $l_1$ and $l_2$ penalty, V-fold CV. This combo is typically known as _elastic net regression_.
1. Logistic regression, $l_2$ penalty, V-fold CV.
1. SVM classification, $l_2$ penalty, V-fold CV.
1. Deep network, no penalty, V-fold CV.
1. Etc.


For fans of statistical hypothesis testing we will also emphasize: 
Testing and prediction are related, but are not the same.
__It is indeed possible that we will want to ignore a significant predictor, and add a non-significant one!__ [@foster2004variable]
Some authors will use hypothesis testing as an initial screening of candidate predictors. 
This is a useful heuristic, but that is all it is-- a heuristic.





## Supervised Learning in R
At this point, we have a rich enough language to do supervised learning with R.

In these examples, I will use two data sets from the __ElemStatLearn__ package: 
`spam` for categorical predictions, and `prostate` for continuous predictions.
In `spam` we will try to decide if a mail is spam or not. 
In `prostate` we will try to predict the size of a cancerous tumor.
You can now call `?prostate` and `?spam` to learn more about these data sets.

Some boring pre-processing. 

```{r preprocessing, cache=TRUE}
library(ElemStatLearn) 
data("prostate")
data("spam")

library(magrittr) # for piping

# Preparing prostate data
prostate.train <- prostate[prostate$train, names(prostate)!='train']
prostate.test <- prostate[!prostate$train, names(prostate)!='train'] 
y.train <- prostate.train$lcavol
X.train <- as.matrix(prostate.train[, names(prostate.train)!='lcavol'] )
y.test <- prostate.test$lcavol 
X.test <- as.matrix(prostate.test[, names(prostate.test)!='lcavol'] )

# Preparing spam data:
n <- nrow(spam)

train.prop <- 0.66
train.ind <- c(TRUE,FALSE) %>%  
  sample(size = n, prob = c(train.prop,1-train.prop), replace=TRUE)
spam.train <- spam[train.ind,]
spam.test <- spam[!train.ind,]

y.train.spam <- spam.train$spam
X.train.spam <- as.matrix(spam.train[,names(spam.train)!='spam'] )
y.test.spam <- spam.test$spam
X.test.spam <-  as.matrix(spam.test[,names(spam.test)!='spam']) 

spam.dummy <- spam
spam.dummy$spam <- as.numeric(spam$spam=='spam') 
spam.train.dummy <- spam.dummy[train.ind,]
spam.test.dummy <- spam.dummy[!train.ind,]
```

We also define some utility functions that we will require down the road. 
```{r utility-functions}
l2 <- function(x) x^2 %>% sum %>% sqrt 
l1 <- function(x) abs(x) %>% sum  
MSE <- function(x) x^2 %>% mean 
missclassification <- function(tab) sum(tab[c(2,3)])/sum(tab)
```

### Linear Models with Least Squares Loss {#least-squares}

Starting with OLS regression, and a train-test data approach. 
Notice the better in-sample MSE than the out-of-sample. 
That is overfitting in action.

```{r OLS Regression, cache=TRUE}
ols.1 <- lm(lcavol~. ,data = prostate.train)
# Train error:
MSE( predict(ols.1)- prostate.train$lcavol) 
# Test error:
MSE( predict(ols.1, newdata = prostate.test)- prostate.test$lcavol)
```

We now implement a V-fold CV, instead of our train-test approach.
The assignment of each observation to each fold is encoded in `fold.assignment`. 
The following code is extremely inefficient, but easy to read. 

```{r , cache=TRUE, dependson='preprocessing'}
folds <- 10
fold.assignment <- sample(1:folds, nrow(prostate), replace = TRUE)
errors <- NULL

for (k in 1:folds){
  prostate.cross.train <- prostate[fold.assignment!=k,] # train subset
  prostate.cross.test <-  prostate[fold.assignment==k,] # test subset
  .ols <- lm(lcavol~. ,data = prostate.cross.train) # train
  .predictions <- predict(.ols, newdata=prostate.cross.test)
  .errors <-  .predictions - prostate.cross.test$lcavol # save prediction errors in the fold
  errors <- c(errors, .errors) # aggregate error over folds.
}

# Cross validated prediction error:
MSE(errors)
```


Let's try all possible models, and choose the best performer with respect to the Cp unbiased risk estimator.
This is done with `leaps::regsubsets`.
We see that the best performer has 3 predictors. 

```{r all subset, cache=TRUE, dependson='preprocessing'}
library(leaps)
regfit.full <- prostate.train %>% 
  regsubsets(lcavol~.,data = ., method = 'exhaustive') # best subset selection
plot(regfit.full, scale = "Cp")
```


Instead of the Cp criterion, we now compute the train and test errors for all the possible predictor subsets^[Example taken from https://lagunita.stanford.edu/c4x/HumanitiesScience/StatLearning/asset/ch6.html].
In the resulting plot we can see overfitting in action.

```{r all subsets second, results='hold', cache=TRUE, dependson='all subset'}
model.n <- regfit.full %>% summary %>% length
X.train.named <- model.matrix(lcavol ~ ., data = prostate.train ) 
X.test.named <- model.matrix(lcavol ~ ., data = prostate.test ) 


val.errors <- rep(NA, model.n)
train.errors <- rep(NA, model.n)
for (i in 1:model.n) {
    coefi <- coef(regfit.full, id = i)
    
    pred <-  X.train.named[, names(coefi)] %*% coefi # make in-sample predictions
    train.errors[i] <- MSE(y.train - pred) # train errors

    pred <-  X.test.named[, names(coefi)] %*% coefi # make out-of-sample predictions
    val.errors[i] <- MSE(y.test - pred) # test errors
}
```

Plotting results.
```{r, cache=TRUE, dependson="all subsets second"}
plot(train.errors, ylab = "MSE", pch = 19, type = "o")
points(val.errors, pch = 19, type = "b", col="blue")
legend("topright", 
       legend = c("Training", "Validation"), 
       col = c("black", "blue"), 
       pch = 19)
```



Checking all possible models is computationally very hard.
_Forward selection_ is a greedy approach that adds one variable at a time, using the AIC risk estimator.
If AIC decreases, the variable is added. 

```{r, cache=TRUE, dependson=c('preprocessing','OLS regression')}
ols.0 <- lm(lcavol~1 ,data = prostate.train)
model.scope <- list(upper=ols.1, lower=ols.0)
step(ols.0, scope=model.scope, direction='forward', trace = TRUE)
```


We now learn a linear predictor on the `spam` data using, a least squares loss, and train-test risk estimator. 

```{r, cache=TRUE, dependson='preprocessing'}
# train the predictor
ols.2 <- lm(spam~., data = spam.train.dummy) 

# make in-sample predictions
.predictions.train <- predict(ols.2) > 0.5 
# inspect the confusion matrix
(confusion.train <- table(prediction=.predictions.train, truth=spam.train.dummy$spam)) 
# compute the train (in sample) misclassification
missclassification(confusion.train) 

# make out-of-sample prediction
.predictions.test <- predict(ols.2, newdata = spam.test.dummy) > 0.5 
# inspect the confusion matrix
(confusion.test <- table(prediction=.predictions.test, truth=spam.test.dummy$spam))
# compute the train (in sample) misclassification
missclassification(confusion.test)
```





The `glmnet` package is an excellent package that provides ridge, lasso, and elastic net regularization, for all GLMs, so for linear models in particular.

```{r Ridge-II, cache=TRUE, dependson='preprocessing'}
suppressMessages(library(glmnet))
ridge.2 <- glmnet(x=X.train, y=y.train, family = 'gaussian', alpha = 0)

# Train error:
MSE( predict(ridge.2, newx =X.train)- y.train)

# Test error:
MSE( predict(ridge.2, newx = X.test)- y.test)
```

Things to note:

- The `alpha=0` parameters tells R to do ridge regression. Setting $alpha=1$ will do lasso, and any other value, with return an elastic net with appropriate weights. 
- The `family='gaussian' argument tells R to fit a linear model, with least squares loss.
- The test error is __smaller__ than the train error, which I attribute to the variability of the risk estimators. 

```{remark}
The variability of risk estimator is a very interesting problem, which recieved very little attention in the machine learning literature. 
If this topic interests you, talk to me. 
```


We now use the lasso penalty.
```{r, cache=TRUE}
lasso.1 <- glmnet(x=X.train, y=y.train, , family='gaussian', alpha = 1)

# Train error:
MSE( predict(lasso.1, newx =X.train)- y.train)

# Test error:
MSE( predict(lasso.1, newx = X.test)- y.test)
```

We now use `glmnet` for classification.

```{r, cache=TRUE, dependson='preprocessing'}
logistic.2 <- cv.glmnet(x=X.train.spam, y=y.train.spam, family = "binomial", alpha = 0)
```

Things to note:

- We used `cv.glmnet` to do an automatic search for the optimal level of regularization (the `lambda` argument in `glmnet`) using V-fold CV.
- We set `alpha=0` for ridge regression.

```{r}
# Train confusion matrix:
.predictions.train <- predict(logistic.2, newx = X.train.spam, type = 'class') 
(confusion.train <- table(prediction=.predictions.train, truth=spam.train$spam))
# Train misclassification error
missclassification(confusion.train)

# Test confusion matrix:
.predictions.test <- predict(logistic.2, newx = X.test.spam, type='class') 
(confusion.test <- table(prediction=.predictions.test, truth=y.test.spam))
# Test misclassification error:
missclassification(confusion.test)
```

### SVM

A support vector machine (SVM) is a linear hypothesis class with a particular loss function known as a _hinge loss_.
We learn an SVM with the `svm` function from the __e1071__ package, which is merely a wrapper for the __libsvm__ C library, which is the most popular implementation of SVM today.

```{r svm-classification, cache=TRUE}
library(e1071)
svm.1 <- svm(spam~., data = spam.train)

# Train confusion matrix:
.predictions.train <- predict(svm.1) 
(confusion.train <- table(prediction=.predictions.train, truth=spam.train$spam))
missclassification(confusion.train)

# Test confusion matrix:
.predictions.test <- predict(svm.1, newdata = spam.test) 
(confusion.test <- table(prediction=.predictions.test, truth=spam.test$spam))
missclassification(confusion.test)
```

We can also use SVM for regression.

```{r svm-regression, cache=TRUE}
svm.2 <- svm(lcavol~., data = prostate.train)

# Train error:
MSE( predict(svm.2)- prostate.train$lcavol)
# Test error:
MSE( predict(svm.2, newdata = prostate.test)- prostate.test$lcavol)
```

### Neural Nets

Neural nets (non deep) can be fitted, for example, with the `nnet` function in the __nnet__ package.
We start with a nnet regression.

```{r NNET-regression, cache=TRUE}
library(nnet)
nnet.1 <- nnet(lcavol~., size=20, data=prostate.train, rang = 0.1, decay = 5e-4, maxit = 1000, trace=FALSE)

# Train error:
MSE( predict(nnet.1)- prostate.train$lcavol)
# Test error:
MSE( predict(nnet.1, newdata = prostate.test)- prostate.test$lcavol)
```

And nnet classification.

```{r NNET-Classification, cache=TRUE, dependson='preprocessing'}
nnet.2 <- nnet(spam~., size=5, data=spam.train, rang = 0.1, decay = 5e-4, maxit = 1000, trace=FALSE)

# Train confusion matrix:
.predictions.train <- predict(nnet.2, type='class') 
(confusion.train <- table(prediction=.predictions.train, truth=spam.train$spam))
missclassification(confusion.train)

# Test confusion matrix:
.predictions.test <- predict(nnet.2, newdata = spam.test, type='class') 
(confusion.test <- table(prediction=.predictions.test, truth=spam.test$spam))
missclassification(confusion.test)
```

### Classification and Regression Trees (CART)

A CART, is not a linear model.
It partitions the feature space $\mathcal{X}$, thus creating a set of if-then rules for prediction or classification.
This view clarifies the name of the function `rpart`, which _recursively partitions_ the feature space. 

We start with a regression tree.

```{r Tree-regression, cache=TRUE}
library(rpart)
tree.1 <- rpart(lcavol~., data=prostate.train)

# Train error:
MSE( predict(tree.1)- prostate.train$lcavol)
# Test error:
MSE( predict(tree.1, newdata = prostate.test)- prostate.test$lcavol)
```

[TODO: plot with `rpart.plot`]


Tree are very prone to overfitting.
To avoid this, we reduce a tree's complexity by _pruning_ it. 
This is done with the `prune` function (not demonstrated herein).

We now fit a classification tree.
```{r Tree classification, cache=TRUE}
tree.2 <- rpart(spam~., data=spam.train)

# Train confusion matrix:
.predictions.train <- predict(tree.2, type='class') 
(confusion.train <- table(prediction=.predictions.train, truth=spam.train$spam))
missclassification(confusion.train)

# Test confusion matrix:
.predictions.test <- predict(tree.2, newdata = spam.test, type='class') 
(confusion.test <- table(prediction=.predictions.test, truth=spam.test$spam))
missclassification(confusion.test)
```

### K-nearest neighbour (KNN)

KNN is not an ERM problem.
For completeness, we still show how to fit such a hypothesis class. 

```{r knn-classification, cache=TRUE}
library(class)
knn.1 <- knn(train = X.train.spam, test = X.test.spam, cl =y.train.spam, k = 1)

# Test confusion matrix:
.predictions.test <- knn.1 
(confusion.test <- table(prediction=.predictions.test, truth=spam.test$spam))
missclassification(confusion.test)
```

### Linear Discriminant Analysis (LDA)

LDA is equivalent to least squares classification \@ref(least-squares).
There are, however, some dedicated functions to fit it.

```{r LDA, cache=TRUE}
library(MASS) 
lda.1 <- lda(spam~., spam.train)

# Train confusion matrix:
.predictions.train <- predict(lda.1)$class
(confusion.train <- table(prediction=.predictions.train, truth=spam.train$spam))
missclassification(confusion.train)

# Test confusion matrix:
.predictions.test <- predict(lda.1, newdata = spam.test)$class
(confusion.test <- table(prediction=.predictions.test, truth=spam.test$spam))
missclassification(confusion.test)
```

### Naive Bayes

A Naive-Bayes classifier is also not part of the ERM framework. 
It is, however, very popular, so we present it.

```{r Naive Bayes, cache=TRUE}
library(e1071)
nb.1 <- naiveBayes(spam~., data = spam.train)

# Train confusion matrix:
.predictions.train <- predict(nb.1, newdata = spam.train)
(confusion.train <- table(prediction=.predictions.train, truth=spam.train$spam))
missclassification(confusion.train)

# Test confusion matrix:
.predictions.test <- predict(nb.1, newdata = spam.test)
(confusion.test <- table(prediction=.predictions.test, truth=spam.test$spam))
missclassification(confusion.test)
```






## Bibliographic Notes
The ultimate reference on (statistical) machine learning is @friedman2001elements.
For a softer introduction, see @james2013introduction.
A statistician will also like @ripley2007pattern.
For an R oriented view see @lantz2013machine.
For a very algorithmic view, see the seminal @leskovec2014mining or @conway2012machine.
For a much more theoretical reference, see @mohri2012foundations, @vapnik2013nature, @shalev2014understanding.
Terminology taken from @sammut2011encyclopedia.
For a review of resampling based unbiased risk estimation (i.e. cross validation) see the exceptional review of @arlot2010survey.

## Practice Yourself

<!--chapter:end:08-supervised.Rmd-->

# Unsupervised Learning {#unsupervised}

This chapter deals with machine learning problems which are unsupervised.
This means the machine has access to a set of inputs, $x$, but the desired outcome, $y$ is not available. 
Clearly, learning a relation between inputs and outcomes is impossible, but there are still a lot of problems of interest. 
In particular, we may want to find a compact representation of the inputs, be it for visualization of further processing. This is the problem of _dimensionality reduction_.
For the same reasons we may want to group similar inputs. This is the problem of _clustering_.

In the statistical terminology, and with some exceptions, this chapter can be thought of as multivariate __exploratory__ statistics.
For multivariate __inference__, see Chapter \@ref(multivariate).



## Dimensionality Reduction {#dim-reduce}

```{example, label='bmi'}
Consider the heights and weights of a sample of individuals. 
The data may seemingly reside in $2$ dimensions but given the height, we have a pretty good guess of a persons weight, and vice versa. 
We can thus state that heights and weights are not really two dimensional, but roughly lay on a $1$ dimensional subspace of $\mathbb{R}^2$. 
```

```{example, label='iq'}
Consider the correctness of the answers to a questionnaire with $p$ questions. 
The data may seemingly reside in a $p$ dimensional space, but assuming there is a thing as ``skill'', then given the correctness of a person's reply to a subset of questions, we have a good idea how he scores on the rest. 
Put differently, we don't really need a $200$ question questionnaire-- $100$ is more than enough.
If skill is indeed a one dimensional quality, then the questionnaire data should organize around a single line in the $p$ dimensional cube. 
```


```{example, label='blind-signal'}
Consider $n$ microphones recording an individual. 
The digitized recording consists of $p$ samples. 
Are the recordings really a shapeless cloud of $n$ points in $\mathbb{R}^p$?
Since they all record the same sound, one would expect the $n$ $p$-dimensional points to arrange around the source sound bit: a single point in $\mathbb{R}^p$.
If microphones have different distances to the source, volumes may differ. 
We would thus expect the $n$ points to arrange about a __line__ in $\mathbb{R}^p$. 	
```

### Principal Component Analysis {#pca}

_Principal Component Analysis_ (PCA) is such a basic technique, it has been rediscovered and renamed independently in many fields. 
It can be found under the names of 
Discrete KarhunenLove Transform; Hotteling Transform; Proper Orthogonal Decomposition; EckartYoung  Theorem; SchmidtMirsky Theorem; Empirical Orthogonal Functions; Empirical Eigenfunction Decomposition; Empirical Component Analysis; Quasi-Harmonic Modes; Spectral Decomposition; Empirical Modal Analysis, and possibly more^[http://en.wikipedia.org/wiki/Principal_component_analysis].
The many names are quite interesting as they offer an insight into the different problems that led to PCA's (re)discovery.


Return to the BMI problem in Example \@ref(ex:bmi).
Assume you wish to give each individual a "size score", that is a __linear__ combination of height and weight: PCA does just that. 
It returns the linear combination that has the largest variability, i.e., the combination which best distinguishes between individuals. 

The variance maximizing motivation above was the one that guided @hotelling1933analysis.
But $30$ years before him, @pearson1901liii derived the same procedure with a different motivation in mind. 
Pearson was also trying to give each individual a score. 
He did not care about variance maximization, however. 
He simply wanted a small set of coordinates in some (linear) space that approximates the original data well. 

Before we proceed, we give an example to fix ideas.
Consider the crime rate data in `USArrests`, which encodes reported murder events, assaults, rapes, and the urban population of each american state. 

```{r}
head(USArrests)
```

Following Hotelling's motivation, we may want to give each state a "crimilality score". 
We first remove the `UrbanPop` variable, which does not encode crime levels. 
We then z-score each variable with `scale`, and call PCA for a sequence of $1,\dots,3$ criminality scores that best separate between states. 

```{r, cache=TRUE}
USArrests.1 <- USArrests[,-3] %>% scale 
pca.1 <- prcomp(USArrests.1, scale = TRUE)
pca.1
```

Things to note:

- Distinguishing between states, i.e., finding the variance maximizing scores, should be indifferent to the __average__ of each variable. 
We also don't want the score to be sensitive to the measurement __scale__. 
We thus perform PCA in the z-score scale of each variable, obtained with the `scale` function.
- PCA is performed with the `prcomp` function. 
It returns the contribution (weight) of the original variables, to the new crimeness score.  
These weights are called the _loadings_ (or `Rotations` in the `prcomp` output, which is rather confusing as we will later see).
- The number of possible scores, is the same as the number of original variables in the data. 
- The new scores are called the _principal components_, labeled `PC1`,...,`PC3` in our output.
- The loadings on PC1 tell us that the best separation between states is along the average crime rate. 
Why is this? 
Because all the $3$ crime variables have a similar loading on PC1.
- The other PCs are slightly harder to interpret, but it is an interesting exercise.

__If we now represent each state, not with its original $4$ variables, but only with the first $2$ PCs (for example), we have reduced the dimensionality of the data.__




### Dimensionality Reduction Preliminaries

Before presenting methods other than PCA, we need some terminology.

- __Variable__: 
A.k.a. _dimension_, or _feature_, or _column_.
- __Data__: 
A.k.a. _sample_, _observations_. 
Will typically consist of $n$, vectors of dimension $p$.
We typically denote the data as a $n\times p$ matrix $X$. 
- __Manifold__: 
A generalization of a linear space, which is regular enough so that, __locally__, it has all the properties of a linear space. 
We will denote an arbitrary manifold by $\mathcal{M}$, and by $\mathcal{M}_q$ a $q$ dimensional^[You are probably used to thinking of the __dimension__ of linear spaces. We will not rigorously define what is the dimension of a manifold, but you may think of it as the number of free coordinates needed to navigate along the manifold.] manifold.
- __Embedding__: 
Informally speaking: a ``shape preserving'' mapping of a space into another. 
- __Linear Embedding__: 
An embedding done via a linear operation (thus representable by a matrix). 
- __Generative Model__: 
Known to statisticians as the __sampling distribution__. 
The assumed stochastic process that generated the observed $X$. 

There are many motivations for dimensionality reduction:

1. __Scoring__: 
Give each observation an interpretable, simple score (Hotelling's motivation).
1. __Latent structure__: 
Recover unobservable information from indirect measurements. 
E.g: Blind signal reconstruction, CT scan, cryo-electron microscopy, etc. 
1. __Signal to Noise__: 
Denoise measurements before further processing like clustering, supervised learning, etc. 
1. __Compression__: 
Save on RAM ,CPU, and communication when operating on a lower dimensional representation of the data. 
	


### Latent Variable Generative Approaches
All generative approaches to dimensionality reduction will include a set of latent/unobservable variables, which we can try to recover from the observables $X$. 
The unobservable variables will typically have a lower dimension than the observables, thus, dimension is reduced. 
We start with the simplest case of linear Factor Analysis. 

#### Factor Analysis (FA)

FA originates from the psychometric literature. 
We thus revisit the IQ (actually g-factor^[https://en.wikipedia.org/wiki/G_factor_(psychometrics)]) Example \@ref(ex:iq):

```{example}
Assume $n$ respondents answer $p$ quantitative questions: $x_i \in \mathbb{R}^p, i=1,\dots,n$. 
Also assume, their responses are some linear function of a single personality attribute, $s_i$. 
We can think of $s_i$ as the subject's ``intelligence''.
We thus have 
\begin{align}
	x_i = A s_i + \varepsilon_i
\end{align}
And in matrix notation:
\begin{align}
	X = S A+\varepsilon,
	(\#eq:factor)
\end{align}
where $A$ is the $q \times p$ matrix of factor loadings, and $S$ the $n \times q$ matrix of latent personality traits. 
In our particular example where $q=1$, the problem is to recover the unobservable intelligence scores, $s_1,\dots,s_n$, from the observed answers $X$.	
```



We may try to estimate $S A$ by assuming some distribution on $S$ and $\varepsilon$ and apply maximum likelihood.
Under standard assumptions on the distribution of $S$ and $\varepsilon$, recovering  $S$ from $\widehat{S A }$ is still impossible as there are infinitely many such solutions.
In the statistical parlance we say the problem is _non identifiable_, and in the applied mathematics parlance we say the problem is _ill posed_.
To see this, consider an orthogonal _rotation_ matrix $R$ ($R' R=I$). 
For each such $R$: $S A = S R' R A = S^* A^*$.
While both solve Eq.\@ref(eq:factor), $A$ and $A^*$ may have very different interpretations. 
This is why many researchers find FA an unsatisfactory inference tool.

```{remark}
The non-uniqueness (non-identifiability) of the FA solution under variable rotation is never mentioned in the PCA context. 
Why is this?
This is because the methods solve different problems. 
The reason the solution to PCA is well defined is that PCA does not seek a single $S$ but rather a __sequence__ of $S_q$ with dimensions growing from $q=1$ to $q=p$. 
```

```{remark}
In classical FA in Eq.\@ref(eq:factor) is clearly an embedding to a linear space:
the one spanned by $S$. 
Under the classical probabilistic assumptions on $S$ and $\varepsilon$ the embedding itself is also linear, and is sometimes solved with PCA. 
Being a generative model, there is no restriction for the embedding to be linear, and there certainly exists sets of assumptions for which the FA returns a non linear embedding into a linear space. 
```

The FA terminology is slightly different than PCA:

- __Factors__: 
The unobserved attributes $S$. 
Akin to the _principal components_ in PCA.
- __Loading__:
The $A$ matrix; the contribution of each factor to the observed $X$.
- __Rotation__:	
An arbitrary orthogonal re-combination of the factors, $S$, and loadings, $A$, which changes the interpretation of the result.


The FA literature offers several heuristics to "fix" the identifiability problem of FA. 
These are known as _rotations_, and go under the names of _Varimax_, _Quartimax_, _Equimax_, _Oblimin_, _Promax_, and possibly others. 


#### Independent Component Analysis (ICA)
Like FA, _independent compoent analysis_ (ICA) is a family of latent space models, thus, a _meta-method_.
It assumes data is generated as some function of the latent variables $S$. 
In many cases this function is assumed to be linear in $S$ so that ICA is compared, if not confused, with PCA and even more so with FA. 

The fundamental idea of ICA is that $S$ has a joint distribution of __non-Gaussian__, __independent__ variables. 
This independence assumption, solves the the non-uniqueness of $S$ in FA.

Being a generative model, estimation of $S$ can then be done using maximum likelihood, or other estimation principles. 

ICA is a popular technique in signal processing, where $A$ is actually the signal, such as sound in Example \@ref(ex:blind-signal).
Recovering $A$ is thus recovering the original signals mixing in the recorded $X$. 





### Purely Algorithmic Approaches

We now discuss dimensionality reduction approaches that are not stated via their generative model, but rather, directly as an algorithm.
This does not mean that they cannot be cast via their generative model, but rather they were not motivated as such.


#### Multidimensional Scaling (MDS)
MDS can be thought of as a variation on PCA, that begins with the $n \times n$ graph^[The term Graph is typically used in this context instead of Network. 
But a graph allows only yes/no relations, while a network, which is a weighted graph, allows a continuous measure of similarity (or dissimilarity). _Network_ is thus more appropriate than _graph_.]} of distances between data points, and not the original $n \times p$ data. 

MDS aims at embedding a graph of distances, while preserving the original distances.
Basic results in graph/network theory [@graham1988isometric] suggest that the geometry of a graph cannot be preserved when embedding it into lower dimensions. 
The different types of MDSs, such as _Classical MDS_, and _Sammon Mappings_, differ in the _stress function_ penalizing for geometric distortion.


#### Local Multidimensional Scaling (Local MDS)

```{example, label='non-euclidean'}
Consider data of coordinates on the globe. 
At short distances, constructing a dissimilarity graph with Euclidean distances will capture the true distance between points. 
At long distances, however, the Euclidean distances as grossly inappropriate. 
A more extreme example is coordinates on the brain's cerebral cortex.
Being a highly folded surface, the Euclidean distance between points is far from the true geodesic distances along the cortex's surface^[Then again, it is possible that the true distances are the white matter fibers connecting going within the cortex, in which case, Euclidean distances are more appropriate than geodesic distances. We put that aside for now.].
```

Local MDS is aimed at solving the case where we don't know how to properly measure distances. 
It is an algorithm that compounds both the construction of the dissimilarity graph, and the embedding. 
The solution of local MDS, as the name suggests, rests on the computation of _local_ distances, where the Euclidean assumption may still be plausible, and then aggregate many such local distances, before calling upon regular MDS for the embedding.

Because local MDS ends with a regular MDS, it can be seen as a non-linear embedding into a linear $\mathcal{M}$. 

Local MDS is not popular.
Why is this? 
Because it makes no sense: 
If we believe the points reside in a non-Euclidean space, thus motivating the use of geodesic distances, why would we want to wrap up with regular MDS, which embeds in a linear space?!
It does offer, however, some intuition to the following, more popular, algorithms.


#### Isometric Feature Mapping (IsoMap) {#isomap}
Like localMDS, only that the embedding, and not only the computation of the distances, is local.


#### Local Linear Embedding (LLE)
Very similar to IsoMap \@ref(isomap).


#### Kernel PCA
TODO

#### Simplified Component Technique LASSO (SCoTLASS)
TODO

#### Sparse Principal Component Analysis (sPCA)
TODO

#### Sparse kernel principal component analysis (skPCA)
TODO




### Dimensionality Reduction in R

#### PCA {#pca-in-r}

We already saw the basics of PCA in \@ref(pca).
The fitting is done with the `prcomp` function.
The _bi-plot_ is a useful way to visualize the output of PCA.

```{r}
library(devtools)
# install_github("vqv/ggbiplot")
ggbiplot::ggbiplot(pca.1) 
```

Things to note:


- We used the `ggbiplot` function from the __ggbiplot__ (available from github, but not from CRAN), because it has a nicer output than `stats::biplot`. 
- The bi-plot also plots the loadings as arrows. The coordinates of the arrows belong to the weight of each of the original variables in each PC. 
For example, the x-value of each arrow is the loadings on the first PC (on the x-axis).
Since the weights of Murder, Assault, and Rape are almost the same, we conclude that PC1 captures the average crime rate in each state.
- The bi-plot plots each data point along its PCs.

The _scree plot_ depicts the quality of the approximation of $X$ as $q$ grows. 
This is depicted using the proportion of variability in $X$ that is removed by each added PC.
It is customary to choose $q$ as the first PC that has a relative low contribution to the approximation of $X$.

```{r}
ggbiplot::ggscreeplot(pca.1)
```

See how the first PC captures the variability in the Assault levels and Murder levels, with a single score.

```{r, echo=FALSE}
USArrests.1 <- USArrests[,-3] %>% scale
load <- pca.1$rotation
slope <- load[2, ]/load[1, ]
mn <- apply(USArrests.1, 2, mean)
intcpt <- mn[2] - (slope * mn[1])

# scatter plot with the two new axes added
USArrests.2 <- USArrests[,1:2] %>%  scale
xlim <- range(USArrests.2)  # overall min, max
plot(USArrests.2, xlim = xlim, ylim = xlim, pch = 16, col = "purple")  # both axes same length
abline(intcpt[1], slope[1], lwd = 2)  # first component solid line
abline(intcpt[2], slope[2], lwd = 2, lty = 2)  # second component dashed
legend("right", legend = c("PC 1", "PC 2"), lty = c(1, 2), lwd = 2, cex = 1)

# projections of points onto PCA 1
y1 <- intcpt[1] + slope[1] * USArrests.2[, 1]
x1 <- (USArrests.1[, 2] - intcpt[1])/slope[1]
y2 <- (y1 + USArrests.1[, 2])/2
x2 <- (x1 + USArrests.1[, 1])/2
segments(USArrests.1[, 1], USArrests.1[, 2], x2, y2, lwd = 2, col = "purple")
```

More implementations of PCA:
```{r many PCA implementations, eval=FALSE}
# FAST solutions:
gmodels::fast.prcomp()

# More detail in output:
FactoMineR::PCA()

# For flexibility in algorithms and visualization:
ade4::dudi.pca()

# Another one...
amap::acp()
```


#### FA


```{r FA, cache=TRUE}
fa.1 <- psych::principal(USArrests.1, nfactors = 2, rotate = "none")
fa.1
biplot(fa.1, labels =  rownames(USArrests.1)) 

# Numeric comparison with PCA:
fa.1$loadings
pca.1$rotation
```

Things to note:

- We perform FA with the `psych::principal` function. The `Principal Component Analysis` title is due to the fact that FA without rotations, is equivalent to PCA.
- The first factor (`fa.1$loadings`) has different weights than the first PC (`pca.1$rotation`) because of normalization. They are the same, however, in that the first PC, and the first factor, capture average crime levels.

Graphical model fans will like the following plot, where the contribution of each variable to each factor is encoded in the width of the arrow. 

```{r, cache=TRUE}
qgraph::qgraph(fa.1)
```

Let's add a rotation (Varimax), and note that the rotation has indeed changed the loadings of the variables, thus the interpretation of the factors. 

```{r varimax, cache=TRUE}
fa.2 <- psych::principal(USArrests.1, nfactors = 2, rotate = "varimax")

fa.2$loadings
```

Things to note:

- FA with a rotation is no longer equivalent to PCA.
- The rotated factors are now called _rotated componentes_, and reported in `RC1` and `RC2`.


#### ICA

```{r ICA, cache=TRUE}
ica.1 <- fastICA::fastICA(USArrests.1, n.com=2) # Also performs projection pursuit

plot(ica.1$S)
abline(h=0, v=0, lty=2)
text(ica.1$S, pos = 4, labels = rownames(USArrests.1))

# Compare with PCA (first two PCs):
arrows(x0 = ica.1$S[,1], y0 = ica.1$S[,2], x1 = pca.1$x[,2], y1 = pca.1$x[,1], col='red', pch=19, cex=0.5)
```

Things to note:

- ICA is fitted with `fastICA::fastICA`.
- The ICA components, like any other rotated components, are different than the PCA components. 



#### MDS

Classical MDS, also compared with PCA.
```{r MDS, results='hold', cache=TRUE}
# We first need a dissimarity matrix/graph:
state.disimilarity <- dist(USArrests.1)

mds.1 <- cmdscale(state.disimilarity)

plot(mds.1, pch = 19)
abline(h=0, v=0, lty=2)
USArrests.2 <- USArrests[,1:2] %>%  scale
text(mds.1, pos = 4, labels = rownames(USArrests.2), col = 'tomato')

# Compare with PCA (first two PCs):
points(pca.1$x[,1:2], col='red', pch=19, cex=0.5)
```

Things to note:

- We first compute a dissimilarity graph with `dist`. See the `cluster::daisy` function for more dissimilarity measures.
- We learn the MDS embedding with `cmdscale`.
- The embedding of PCA is the same as classical MDS with Euclidean distances. 



Let's try other strain functions for MDS, like Sammon's strain, and compare it with the PCs.
```{r SammonMDS, results='hold'}
mds.2 <- MASS::sammon(state.disimilarity, trace = FALSE)
plot(mds.2$points, pch = 19)
abline(h=0, v=0, lty=2)
text(mds.2$points, pos = 4, labels = rownames(USArrests.2))

# Compare with PCA (first two PCs):
arrows(
  x0 = mds.2$points[,1], y0 = mds.2$points[,2], 
  x1 = pca.1$x[,1], y1 = pca.1$x[,2], 
  col='red', pch=19, cex=0.5)
```

Things to note:

- `MASS::sammon` does the embedding.
- Sammon strain is different than PCA.






#### Sparse PCA

```{r sPCA}
# Compute similarity graph
state.similarity <- MASS::cov.rob(USArrests.1)$cov

spca1 <- elasticnet::spca(state.similarity, K=2, type="Gram", sparse="penalty", trace=FALSE, para=c(0.06,0.16))
spca1$loadings
```



#### Kernel PCA
```{r kPCA, eval=FALSE}
kernlab::kpca()
```




## Clustering {#cluster}


```{example, label="photos"}
Consider the tagging of your friends' pictures on Facebook. 
If you tagged some pictures, Facebook may try to use a supervised approach to automatically label photos. 
If you never tagged pictures, a supervised approach is impossible. 
It is still possible, however, to group simiar pictures together.
```


```{example, label="spam"}
Consider the problem of spam detection. 
It would be nice if each user could label several thousands emails, to apply a supervised learning approach to spam detection.
This is an unrealistic demand, so a pre-clustering stage is useful: the user only needs to tag a couple dozens of homogenous clusters, before solving the supervised learning problem. 
```


In clustering problems, we seek to group observations that are similar. 

There are many motivations for clustering:

1. __Understanding__:
The most common use of clustering is probably as a an exploratory step, to identify homogeneous groups in the data.
1. __Dimensionality reduction__:
Clustering may be seen as a method for dimensionality reduction. 
Unlike the approaches in the Dimensionality Reduction Section \@ref(dim-reduce), it does not compress __variables__ but rather __observations__. 
Each group of homogeneous observations may then be represented as a single prototypical observation of the group.
1. __Pre-Labelling__:
Clustering may be performed as a pre-processing step for supervised learning, when labeling all the samples is impossible due to "budget" constraints, like in Example \@ref(ex:spam). This is sometimes known as _pre-clustering_.

Clustering, like dimensionality reduction, may rely on some latent variable generative model, or on purely algorithmic approaches.



### Latent Variable Generative Approaches


#### Finite Mixture {#finite-mixture}

```{example, label="males-females"}
Consider the distribution of heights.
Heights have a nice bell shaped distribution within each gender.
If genders have not been recorded, heights will be distributed like a _mixture_ of males and females. 
The gender in this example, is a _latent_ variable taking $K=2$ levels: male and female.
```

A _finite mixture_ is the marginal distribution of $K$ distinct classes, when the class variable is _latent_.
This is useful for clustering:
We can assume the number of classes, $K$, and the distribution of each class. 
We then use maximum likelihood to fit the mixture distribution, and finally, cluster by assigning observations to the most probable class.





### Purely Algorithmic Approaches

#### K-Means
The _K-means_ algorithm is possibly the most popular clustering algorithm.
The goal behind K-means clustering is finding a representative point for each of K clusters, and assign each data point to one of these clusters. 
As each cluster has a representative point, this is also a _prototype method_.
The clusters are defined so that they minimize the average Euclidean distance between all points to the center of the cluster.

In K-means, the clusters are first defined, and then similarities computed. 
This is thus a _top-down_ method.

K-means clustering requires the raw features $X$ as inputs, and not only a similarity graph. 
This is evident when examining the algorithm below. 

The k-means algorithm works as follows:

1. Choose the number of clusters $K$.
1. Arbitrarily assign points to clusters.
1. While clusters keep changing:
    1. Compute the cluster centers as the average of their points.
    1. Assign each point to its closest cluster center (in Euclidean distance).
1. Return Cluster assignments and means.


```{remark}
If trained as a statistician, you may wonder- what population quantity is K-means actually estimating?
The estimand of K-means is known as the _K principal points_.
Principal points are points which are _self consistent_, i.e., they are the mean of their neighbourhood. 
```


#### K-Means++
_K-means++_ is a fast version of K-means thanks to a smart initialization. 

#### K-Medoids

If a Euclidean distance is inappropriate for a particular set of variables, or that robustness to corrupt observations is required, or that we wish to constrain the cluster centers to be actual observations, then the _K-Medoids_ algorithm is an adaptation of K-means that allows this.
It is also known under the name _partition around medoids_ (PAM) clustering, suggesting its relation to [graph partitioning](https://en.wikipedia.org/wiki/Graph_partition).

The k-medoids algorithm works as follows.

1. Given a dissimilarity graph.
1. Choose the number of clusters $K$.
1. Arbitrarily assign points to clusters.
1. While clusters keep changing:
    1. Within each cluster, set the center as the data point that minimizes the sum of distances to other points in the cluster.
    1. Assign each point to its closest cluster center.
1. Return Cluster assignments and centers.


```{remark}
If trained as a statistician, you may wonder- what population quantity is K-medoids actually estimating?
The estimand of K-medoids is the median of their neighbourhood. 
A delicate matter is that quantiles are not easy to define for __multivariate__ variables so that the "multivaraitre median", may be a more subtle quantity than you may think. 
See @small1990survey.
```


#### Hirarchial Clustering
Hierarchical clustering algorithms take dissimilarity graphs as inputs.
Hierarchical clustering is a class of greedy _graph-partitioning_ algorithms. 
Being hierarchical by design, they have the attractive property that the evolution of the clustering can be presented with a _dendogram_, i.e., a tree plot.  
A particular advantage of these methods is that they do not require an a-priori choice of the number of cluster ($K$).

Two main sub-classes of algorithms are _agglomerative_, and _divisive_.

_Agglomerative clustering_ algorithms are __bottom-up__ algorithm which build clusters by joining smaller clusters. 
To decide which clusters are joined at each iteration some measure of closeness between clusters is required. 

- __Single Linkage__:
Cluster distance is defined by the distance between the two __closest__ members.
- __Complete Linkage__:
Cluster distance is defined by the distance between the two __farthest__ members.
- __Group Average__:
Cluster distance is defined by the __average__ distance between members.
- __Group Median__: 
Like Group Average, only using the median.

_Divisive clustering_ algorithms are __top-down__ algorithm which build clusters by splitting larger clusters. 



#### Fuzzy Clustering
Can be thought of as a purely algorithmic view of the finite-mixture in Section \@ref(finite-mixture).






### Clustering in R


#### K-Means
The following code is an adaptation from [David Hitchcock](http://people.stat.sc.edu/Hitchcock/chapter6_R_examples.txt).
```{r kmeans, cache=TRUE}
k <- 2
kmeans.1 <- stats::kmeans(USArrests.1, centers = k)
head(kmeans.1$cluster) # cluster asignments

pairs(USArrests.1, panel=function(x,y) text(x,y,kmeans.1$cluster))
```

Things to note:

- The `stats::kmeans` function does the clustering. 
- The cluster assignment is given in the `cluster` element of the `stats::kmeans` output.
- The visual inspection confirms that similar states have been assigned to the same cluster. 


#### K-Means ++

_K-Means++_ is a smart initialization for K-Means.
The following code is taken from the [r-help](https://stat.ethz.ch/pipermail/r-help/2012-January/300051.html) mailing list.
```{r kmeansPP, cache=TRUE}
# Write my own K-means++ function.
kmpp <- function(X, k) {
  
  n <- nrow(X)
  C <- numeric(k)
  C[1] <- sample(1:n, 1)
  
  for (i in 2:k) {
    dm <- pracma::distmat(X, X[C, ])
    pr <- apply(dm, 1, min); pr[C] <- 0
    C[i] <- sample(1:n, 1, prob = pr)
  }
  
  kmeans(X, X[C, ])
}

kmeans.2 <- kmpp(USArrests.1, k)
head(kmeans.2$cluster)
```

#### K-Medoids
Start by growing a distance graph with `dist` and then partition using `pam`.
```{r kmedoids, cache=TRUE}
state.disimilarity <- dist(USArrests.1)
kmed.1 <- cluster::pam(x= state.disimilarity, k=2)
head(kmed.1$clustering)

plot(pca.1$x[,1], pca.1$x[,2], xlab="PC 1", ylab="PC 2", type ='n', lwd=2)
text(pca.1$x[,1], pca.1$x[,2], labels=rownames(USArrests.1), cex=0.7, lwd=2, col=kmed.1$cluster)
```

Things to note:

- K-medoids starts with the computation of a dissimilarity graph, done by the `dist` function.
- The clustering is done by the `cluster::pam` function.
- Inspecting the output confirms that similar states have been assigned to the same cluster. 
- Many other similarity measures can be found in `proxy::dist()`.
- See `cluster::clara()` for a big-data implementation of PAM.


#### Hirarchial Clustering

We start with agglomerative clustering with single-linkage.
```{r HirarchialClustering}
hirar.1 <- hclust(state.disimilarity, method='single')
plot(hirar.1, labels=rownames(USArrests.1), ylab="Distance")
```

Things to note:

- The clustering is done with the `hclust` function.
- We choose the single-linkage distance using the `method='single'` argument.
- We did not need to a-priori specify the number of clusters, $K$, since all the possible $K$'s are included in the output tree. 
- The `plot` function has a particular method for `hclust` class objects, and plots them as dendograms.


We try other types of linkages, to verify that the indeed affect the clustering.
Starting with complete linkage.
```{r complete linkage}
hirar.2 <- hclust(state.disimilarity, method='complete')
plot(hirar.2, labels=rownames(USArrests.1), ylab="Distance")
```

Now with average linkage.
```{r average linkage}
hirar.3 <- hclust(state.disimilarity, method='average')
plot(hirar.3, labels=rownames(USArrests.1), ylab="Distance")
```


If we know how many clusters we want, we can use `cuttree` to get the class assignments. 

```{r}
cut.2.2 <- cutree(hirar.2, k=2)
head(cut.2.2)
```




## Bibliographic Notes
For more on PCA see my [Dimensionality Reduction Class Notes](https://github.com/johnros/dim_reduce/blob/master/dim_reduce.pdf) and references therein.
For more on everything, see @friedman2001elements.
For a softer introduction, see @james2013introduction.

## Practice Yourself

<!--chapter:end:09-unsupervised.Rmd-->

# Plotting {#plotting}

Whether you are doing EDA, or preparing your results for publication, you need plots.
R has many plotting mechanisms, allowing the user a tremendous amount of flexibility, while abstracting away a lot of the tedious details. 
To be concrete, many of the plots in R are simply impossible to produce with Excel, SPSS, or SAS, and would take a tremendous amount of work to produce with Python, Java and lower level programming languages. 

In this text, we will focus on two plotting packages. 
The basic __graphics__ package, distributed with the base R distribution, and the __ggplot2__ package. 

Before going into the details of the plotting packages, we start with some high-level philosophy.
The __graphics__ package originates from the main-frame days. 
Computers had no graphical interface, and the output of the plot was immediately sent to a printer. 
Once a plot has been produced with the __graphics__ package, just like a printed output, it cannot be queried nor changed, except for further additions.

The philosophy of R is that __everyting is an object__.
The __graphics__ package does not adhere to this philosophy, and indeed it was soon augmented with the __grid__ package [@Rlanguage], that treats plots as objects. 
__grid__ is a low level graphics interface, and users may be more familiar with the __lattice__ package built upon it [@lattice].

__lattice__ is very powerful, but soon enough, it was overtaken in popularity by the __ggplot2__ package [@ggplot2].
__ggplot2__ was the PhD project of [Hadley Wickham](http://hadley.nz/), a name to remember...
Two fundamental ideas underlay __ggplot2__: (i) everything is an object, and (ii), plots can be described by a small set of building blocks. 
The building blocks in __ggplot2__ are the ones stated by @wilkinson2006grammar.
The objects and grammar of __ggplot2__ have later evolved to allow more complicated plotting and in particular, interactive plotting. 

Interactive plotting is a very important feature for EDA, and reporting. 
The major leap in interactive plotting was made possible by the advancement of web technologies, such as JavaScript.
Why is this? 
Because an interactive plot, or report, can be seen as a web-site. 
Building upon the capabilities of JavaScript and your web browser to provide the interactivity, greatly facilitates the development of such plots, as the programmer can rely on the web-browsers capabilities for interactivity. 



## The graphics System

The R code from the Basics Chapter \@ref(basics) is a demonstration of the __graphics__ package and system. 
We make a quick review of the basics.

### Using Existing Plotting Functions

#### Scatter Plot

A simple scatter plot.
```{r}
attach(trees)
plot(Girth ~ Height)
```

Various types of plots.

```{r}
par.old <- par(no.readonly = TRUE)
par(mfrow=c(2,3))
plot(Girth, type='h', main="type='h'") 
plot(Girth, type='o', main="type='o'") 
plot(Girth, type='l', main="type='l'")
plot(Girth, type='s', main="type='s'")
plot(Girth, type='b', main="type='b'")
plot(Girth, type='p', main="type='p'")
par(par.old)
```

Things to note:

- The `par` command controls the plotting parameters. `mfrow=c(2,3)` is used to produce a matrix of plots with 2 rows and 3 columns.
- The `par.old` object saves the original plotting setting. It is restored after plotting using `par(par.old)`.
- The `type` argument controls the type of plot. 
- The `main` argument controls the title.
- See `?plot` and `?par` for more options.

Control the plotting characters with the `pch` argument. 
```{r}
plot(Girth, pch='+', cex=3)
```

Control the line's type with `lty` argument, and width with `lwd`.

```{r}
par(mfrow=c(2,3))
plot(Girth, type='l', lty=1, lwd=2)
plot(Girth, type='l', lty=2, lwd=2)
plot(Girth, type='l', lty=3, lwd=2)
plot(Girth, type='l', lty=4, lwd=2)
plot(Girth, type='l', lty=5, lwd=2)
plot(Girth, type='l', lty=6, lwd=2)
```

Add line by slope and intercept with `abline`.

```{r}
plot(Girth)
abline(v=14, col='red') # vertical line at 14.
abline(h=9, lty=4,lwd=4, col='pink') # horizontal line at 9.
abline(a = 0, b=1) # linear line with intercept a=0, and slope b=1.
```

```{r}
plot(Girth)
points(x=1:30, y=rep(12,30), cex=0.5, col='darkblue')
lines(x=rep(c(5,10), 7), y=7:20, lty=2 )
lines(x=rep(c(5,10), 7)+2, y=7:20, lty=2 )
lines(x=rep(c(5,10), 7)+4, y=7:20, lty=2 , col='darkgreen')
lines(x=rep(c(5,10), 7)+6, y=7:20, lty=4 , col='brown', lwd=4)
```

Things to note:

- `points` adds points on an existing plot.
- `lines` adds lines on an existing plot.
- `col` controls the color of the element. It takes names or numbers as argument. 
- `cex` controls the scale of the element. Defaults to `cex=1`.


Add other elements.

```{r}
plot(Girth)
segments(x0=rep(c(5,10), 7), y0=7:20, x1=rep(c(5,10), 7)+2, y1=(7:20)+2 )
arrows(x0=13,y0=16,x1=16,y1=17, )
rect(xleft=10, ybottom=12,  xright=12, ytop=16)
polygon(x=c(10,11,12,11.5,10.5), y=c(9,9.5,10,10.5,9.8), col='grey')
title(main='This plot makes no sense', sub='Or does it?')
mtext('Printing in the margins', side=2)
mtext(expression(alpha==log(f[i])), side=4)
```

Things to note:

- The following functions add the elements they are named after: `segments`, `arrows`, `rect`, `polygon`, `title`. 
- `mtext` adds mathematical text. For more information for mathematical annotation see `?plotmath`.

Add a legend.
```{r}
plot(Girth, pch='G',ylim=c(8,77), xlab='Tree number', ylab='', type='b', col='blue')
points(Volume, pch='V', type='b', col='red')
legend(x=2, y=70, legend=c('Girth', 'Volume'), pch=c('G','V'), col=c('blue','red'), bg='grey')
```


Adjusting Axes with `xlim` and `ylim`.

```{r}
plot(Girth, xlim=c(0,15), ylim=c(8,12))
```

Use `layout` for complicated plot layouts.

```{r}
A<-matrix(c(1,1,2,3,4,4,5,6), byrow=TRUE, ncol=2)
layout(A,heights=c(1/14,6/14,1/14,6/14))

oma.saved <- par("oma")
par(oma = rep.int(0, 4))
par(oma = oma.saved)
o.par <- par(mar = rep.int(0, 4))
for (i in seq_len(6)) {
	plot.new()
	box()
	text(0.5, 0.5, paste('Box no.',i), cex=3)
}
```


Always detach.

```{r}
detach(trees)
```






### Fancy graphics Examples

Building a line graph from scratch.

```{r}
x = 1995:2005
y = c(81.1, 83.1, 84.3, 85.2, 85.4, 86.5, 88.3, 88.6, 90.8, 91.1, 91.3)
plot.new()
plot.window(xlim = range(x), ylim = range(y))
abline(h = -4:4, v = -4:4, col = "lightgrey")
lines(x, y, lwd = 2)
title(main = "A Line Graph Example",
		xlab = "Time",
		ylab = "Quality of R Graphics")
axis(1)
axis(2)
box()
```

Things to note:

- `plot.new` creates a new, empty, plotting device.
- `plot.window` determines the limits of the plotting region.
- `axis` adds the axes, and `box` the framing box. 
- The rest of the elements, you already know.

Rosette.

```{r}
n = 17
theta = seq(0, 2 * pi, length = n + 1)[1:n]
x = sin(theta)
y = cos(theta)
v1 = rep(1:n, n)
v2 = rep(1:n, rep(n, n))
plot.new()
plot.window(xlim = c(-1, 1), ylim = c(-1, 1), asp = 1)
segments(x[v1], y[v1], x[v2], y[v2])
box()
```



Arrows.

```{r}
plot.new()
plot.window(xlim = c(0, 1), ylim = c(0, 1))
arrows(.05, .075, .45, .9, code = 1)
arrows(.55, .9, .95, .075, code = 2)
arrows(.1, 0, .9, 0, code = 3)
text(.5, 1, "A", cex = 1.5)
text(0, 0, "B", cex = 1.5)
text(1, 0, "C", cex = 1.5)
```

Arrows as error bars.

```{r}
x = 1:10
y = runif(10) + rep(c(5, 6.5), c(5, 5))
yl = y - 0.25 - runif(10)/3
yu = y + 0.25 + runif(10)/3
plot.new()
plot.window(xlim = c(0.5, 10.5), ylim = range(yl, yu))
arrows(x, yl, x, yu, code = 3, angle = 90, length = .125)
points(x, y, pch = 19, cex = 1.5)
axis(1, at = 1:10, labels = LETTERS[1:10])
axis(2, las = 1)
box()
```


A histogram is nothing but a bunch of rectangle elements.

```{r}
plot.new()
plot.window(xlim = c(0, 5), ylim = c(0, 10))
rect(0:4, 0, 1:5, c(7, 8, 4, 3), col = "lightblue")
axis(1)
axis(2, las = 1)
```


Spiral Squares.

```{r}
plot.new()
plot.window(xlim = c(-1, 1), ylim = c(-1, 1), asp = 1)
x = c(-1, 1, 1, -1)
y = c( 1, 1, -1, -1)
polygon(x, y, col = "cornsilk")
vertex1 = c(1, 2, 3, 4)
vertex2 = c(2, 3, 4, 1)
for(i in 1:50) {
	x = 0.9 * x[vertex1] + 0.1 * x[vertex2]
	y = 0.9 * y[vertex1] + 0.1 * y[vertex2]
	polygon(x, y, col = "cornsilk")
}
```

Circles are just dense polygons.

```{r}
R = 1
xc = 0
yc = 0
n = 72
t = seq(0, 2 * pi, length = n)[1:(n-1)]
x = xc + R * cos(t)
y = yc + R * sin(t)
plot.new()
plot.window(xlim = range(x), ylim = range(y), asp = 1)
polygon(x, y, col = "lightblue", border = "navyblue")
```



Spiral- just a bunch of lines. 

```{r}
k = 5
n = k * 72
theta = seq(0, k * 2 * pi, length = n)
R = .98^(1:n - 1)
x = R * cos(theta)
y = R * sin(theta)
plot.new()
plot.window(xlim = range(x), ylim = range(y), asp = 1)
lines(x, y)
```



### Exporting a Plot

The pipeline for exporting graphics is similar to the export of data. 
Instead of the `write.table` or `save` functions, we will use the `pdf`, `tiff`, `png`, functions. Depending on the type of desired output.


Check and set the working directory.

```{r, eval=FALSE}
getwd()
setwd("/tmp/")
```

Export tiff.

```{r, eval=FALSE}
tiff(filename='graphicExample.tiff')
plot(rnorm(100))
dev.off()
```

Things to note:

- The `tiff` function tells R to open a .tiff file, and write the output of a plot.
- Only a single (the last) plot is saved.
- `dev.off` to close the tiff device, and return the plotting to the R console (or RStudio).

If you want to produce several plots, you can use a counter in the file's name. The counter uses the [printf](https://en.wikipedia.org/wiki/Printf_format_string) format string.

```{r, evel=FALSE}
tiff(filename='graphicExample%d.tiff') #Creates a sequence of files
plot(rnorm(100))
boxplot(rnorm(100))
hist(rnorm(100))
dev.off()
```

To see the list of all open devices use `dev.list()`. 
To close __all__ device, (not only the last one), use `graphics.off()`.


See `?pdf` and `?jpeg` for more info.




## The ggplot2 System


The philosophy of __ggplot2__ is very different from the __graphics__ device. 
Recall, in __ggplot2__, a plot is a object.
It can be queried, it can be changed, and among other things, it can be plotted. 

__ggplot2__ provides a convenience function for many plots: `qplot`. 
We take a non-typical approach by ignoring `qplot`, and presenting the fundamental building blocks. 
Once the building blocks have been understood, mastering `qplot` will be easy.

The following is taken from [UCLA's idre](http://www.ats.ucla.edu/stat/r/seminars/ggplot2_intro/ggplot2_intro.htm).


A __ggplot2__ object will have the following elements:

- __Data__ are the variables mapped to aesthetic features of the graph.
- __Aes__ is the mapping between objects to their visualization.
- __Geoms__ are the objects/shapes you see on the graph.
- __Stats__ are statistical transformations that summarize data, such as the mean or confidence intervals.
- __Scales__ define which aesthetic values are mapped to data values. Legends and axes display these mappings.
- __Coordiante systems__ define the plane on which data are mapped on the graphic.
- __Faceting__ splits the data into subsets to create multiple variations of the same graph (paneling).

The `nlme::Milk` dataset has the protein level of various cows, at various times, with various diets.
```{r}
library(nlme)
data(Milk)
head(Milk)
```

```{r}
library(ggplot2)
ggplot(data = Milk, aes(x=Time, y=protein)) +
  geom_point()
```

Things to note:

- The `ggplot` function is the constructor of the __ggplot2__ object. If the object is not assigned, it is plotted.
- The `aes` argument tells R that the `Time` variable in the `Milk` data is the x axis, and protein is y.
- The `geom_point` defines the __Geom__, i.e., it tells R to plot the points as they are (and not lines, histograms, etc.).
- The __ggplot2__ object is build by compounding its various elements separated by the `+` operator.
- All the variables that we will need are assumed to be in the `Milk` data frame. This means that (a) the data needs to be a data frame (not a matrix for instance), and (b) we will not be able to use variables that are not in the `Milk` data frame.


Let's add some color. 

```{r}
ggplot(data = Milk, aes(x=Time, y=protein)) +
  geom_point(aes(color=Diet))
```

The `color` argument tells R to use the variable `Diet` as the coloring. 
A legend is added by default. 
If we wanted a fixed color, and not a variable dependent color, `color` would have been put outside the `aes` function.

```{r}
ggplot(data = Milk, aes(x=Time, y=protein)) +
  geom_point(color="green")
```

Let's save the __ggplot2__ object so we can reuse it. 
Notice it is not plotted.

```{r}
p <- ggplot(data = Milk, aes(x=Time, y=protein)) +
  geom_point()
```

We can add _layers_ of new _geoms_ using the `+` operator. 
Here, we add a smoothing line.

```{r}
p + geom_smooth(method = 'gam')
```

Things to note:

- The smoothing line is a layer added with the `geom_smooth()` function.
- Lacking any arguments, the new layer will inherit the `aes` of the original object, x and y variables in particular. 


To split the plot along some variable, we use faceting, done with the `facet_wrap` function.

```{r}
p + facet_wrap(~Diet)
```

Instead of faceting, we can add a layer of the mean of each `Diet` subgroup, connected by lines.

```{r}
p + stat_summary(aes(color=Diet), fun.y="mean", geom="line")
```

Things to note:

- `stat_summary` adds a statistical summary.
- The summary is applied along `Diet` subgroups, because of the `color=Diet` aesthetic.
- The summary to be applied is the mean, because of `fun.y="mean"`.
- The group means are connected by lines, because of the `geom="line"` argument.

What layers can be added using the __geoms__ family of functions?

- `geom_bar`: bars with bases on the x-axis.
- `geom_boxplot`: boxes-and-whiskers.
- `geom_errorbar`: T-shaped error bars.
- `geom_histogram`: histogram.
- `geom_line`: lines.
- `geom_point`: points (scatterplot).
- `geom_ribbon`: bands spanning y-values across a range of x-values.
- `geom_smooth`: smoothed conditional means (e.g. loess smooth).


To demonstrate the layers added with the `geoms_*` functions, we start with a histogram.

```{r}
pro <- ggplot(Milk, aes(x=protein))
pro + geom_histogram(bins=30)
```

A bar plot.

```{r}
ggplot(Milk, aes(x=Diet)) +
  geom_bar()
```

A scatter plot.

```{r}
tp <- ggplot(Milk, aes(x=Time, y=protein))
tp + geom_point()
```

A smooth regression plot, reusing the `tp` object.

```{r}
tp + geom_smooth(method='gam')
```

And now, a simple line plot, reusing the `tp` object, and connecting lines along `Cow`.

```{r}
tp + geom_line(aes(group=Cow))
```

The line plot is completely incomprehensible. 
Better look at boxplots along time (even if omitting the `Cow` information).
```{r}
tp + geom_boxplot(aes(group=Time))
```

We can do some statistics for each subgroup.
The following will compute the mean and standard errors of `protein` at each time point.

```{r}
ggplot(Milk, aes(x=Time, y=protein)) +
  stat_summary(fun.data = 'mean_se')
```

Some popular statistical summaries, have gained their own functions:

- `mean_cl_boot`: mean and bootstrapped confidence interval (default 95%).
- `mean_cl_normal`: mean and Gaussian (t-distribution based) confidence interval (default 95%).
- `mean_dsl`: mean plus or minus standard deviation times some constant (default constant=2).
- `median_hilow`: median and outer quantiles (default outer quantiles = 0.025 and 0.975).

For less popular statistical summaries, we may specify the statistical function in `stat_summary`. The median is a first example.

```{r}
ggplot(Milk, aes(x=Time, y=protein)) +
  stat_summary(fun.y="median", geom="point")
```

We can also define our own statistical summaries.
```{r}
medianlog <- function(y) {median(log(y))}
ggplot(Milk, aes(x=Time, y=protein)) +
  stat_summary(fun.y="medianlog", geom="line")

```



__Faceting__ allows to split the plotting along some variable.
`face_wrap` tells R to compute the number of columns and rows of plots automatically. 

```{r}
ggplot(Milk, aes(x=protein, color=Diet)) +
  geom_density() +
  facet_wrap(~Time)
```

`facet_grid` forces the plot to appear allow rows or columns, using the `~` syntax.

```{r}
ggplot(Milk, aes(x=Time, y=protein)) +
  geom_point() +
  facet_grid(Diet~.)
```

To control the looks of the plot, __ggplot2__ uses __themes__.

```{r}
ggplot(Milk, aes(x=Time, y=protein)) +
  geom_point() +
  theme(panel.background=element_rect(fill="lightblue"))
```

```{r}
ggplot(Milk, aes(x=Time, y=protein)) +
  geom_point() +
  theme(panel.background=element_blank(),
        axis.title.x=element_blank())
```

Saving plots can be done using the `pdf` function, but possibly easier with the `ggsave` function.

Finally, what every user of __ggplot2__ constantly uses, is the online documentation at http://docs.ggplot2.org.




## Interactive Graphics
[TODO: improve intro]

As already mentioned, the recent and dramatic advancement in interactive visualization was made possible by the advances in web technologies, and the [D3.JS](https://d3js.org/) JavaScript library in particular. 
This is because it allows developers to rely on existing libraries designed for web browsing instead of re-implementing interactive visualizations.
These libraries are more visually pleasing, and computationally efficient, than anything they could have developed themselves.  

Some noteworthy interactive plotting systems are the following:


- __plotly__:
The __plotly__ package [@plotly] uses the (brilliant!) visualization framework of the [Plotly]() company to provide local, or web-publishable, interactive graphics. 

- __dygraphs__:
The [dygraphs](http://dygraphs.com/) JavaScript library is intended for interactive visualization of time series (`xts` class objects). 
The __dygraphs__ R package is an interface allowing the plotting of R objects with this library.
For more information see [here](https://rstudio.github.io/dygraphs/).

- __rCharts__:
If you like the __lattice__ plotting system, the __rCharts__ package will allow you to produce interactive plots from R using the __lattice__ syntax. 
For more information see [here](https://ramnathv.github.io/rCharts/).

- __clickme__:
Very similar to __rCharts__.

- __googleVis__:
TODO


- Highcharter:
TODO

- Rbokeh:
TODO

- __HTML Widgets__:
The __htmlwidgets__ package does not provide visualization, but rather, it facilitates the creation of new interactive visualizations. 
This is because it handles all the technical details that are required to use R output within JavaScript visualization libraries. 
It is available [here](http://www.htmlwidgets.org/), with a demo gallery [here](http://gallery.htmlwidgets.org/).



### Plotly
TODO: remove messages

```{r}
library(plotly)
set.seed(100)
d <- diamonds[sample(nrow(diamonds), 1000), ]
plot_ly(d, x = ~carat, y = ~price, color = ~carat,
        size = ~carat, text = ~paste("Clarity: ", clarity))
```

If you are comfortable with __ggplot2__, you may use the __ggplot2__ syntax, and export the final result to __plotly__.

```{r}
p <- ggplot(data = d, aes(x = carat, y = price)) +
  geom_smooth(aes(colour = cut, fill = cut), method = 'loess') + 
  facet_wrap(~ cut)

ggplotly(p)
```

For more on __plotly__ see https://plot.ly/r/.


### HTML Widgets
TODO






## Bibliographic Notes
For the __graphics__ package, see @Rlanguage.
For __ggplot2__ see @ggplot2.
A [video](https://www.youtube.com/watch?v=9Objw9Tvhb4&feature=youtu.be) by one of my heroes,  [Brian Caffo](http://www.bcaffo.com/), discussing __graphics__ vs. __ggplot2__.



## Practice Yourself

<!--chapter:end:10-plotting.Rmd-->

# Reports {#report}

```{r verbatim text, cache=FALSE, echo=FALSE}
knitr::opts_chunk$set(tidy = F)

knitr::knit_hooks$set(source = function(x, options){
  if (!is.null(options$verbatim) && options$verbatim){
    opts = gsub(",\\s*verbatim\\s*=\\s*TRUE\\s*", "", options$params.src)
    bef = sprintf('\n\n    ```{r %s}\n', opts, "\n")
    stringr::str_c(
      bef, 
      knitr:::indent_block(paste(x, collapse = '\n'), "    "), 
      "\n    ```\n"
    )
  } else {
    stringr::str_c("\n\n```", tolower(options$engine), "\n", 
      paste(x, collapse = '\n'), "\n```\n\n"
    )
  }
})
```


If you have ever written a report, you are probably familiar with the process of preparing your figures in some software, say R, and then copy-pasting into your text editor, say MS Word. 
While very popular, this process is both tedious, and plain painful if your data has changed and you need to update the report. 
Wouldn't it be nice if you could produce figures and numbers from within the text of the report, and everything else would be automated?
It turns out it is possible. 
There are actually several systems in R that allow this. 
We start with a brief review.

1. __Sweave__:
_LaTeX_ is a markup language that compiles to _Tex_ programs that compile, in turn, to documents (typically PS or PDFs).
If you never heard of it, it may be because you were born the the MS Windows+MS Word era. 
You should know, however, that _LaTeX_ was there much earlier, when computers were mainframes with text-only graphic devices. 
You should also know that _LaTeX_ is still very popular (in some communities) due to its very rich markup syntax, and beautiful output. 
_Sweave_ [@leisch2002sweave] is a compiler for _LaTeX_ that allows you do insert R commands in the _LaTeX_ source file, and get the result as part of the outputted PDF. 
It's name suggests just that: it allows to weave S^[Recall, S was the original software from which R evolved.] output into the document, thus, Sweave. 

1. __knitr__:
_Markdown_ is a text editing syntax that, unlike _LaTeX_, is aimed to be human-readable, but also compilable by a machine. 
If you ever tried to read HTML or Latex source files, you may understand why human-readability is a desirable property.
There are many _markdown_ compilers. One of the most popular is Pandoc, written by the Berkeley philosopher(!) Jon MacFarlane. 
The availability of Pandoc gave [Yihui Xie](https://yihui.name/), a name to remember, the idea that it is time for Sweave to evolve. 
Yihui thus wrote __knitr__ [@xie2015dynamic], which allows to write human readable text in _Rmarkdown_, a superset of _markdown_, compile it with R and the compile it with Pandoc. 
Because Pandoc can compile to PDF, but also to HTML, and DOCX, among others, this means that you can write in Rmarkdown, and get output in almost all text formats out there. 

1. __bookdown__:
__Bookdown__ [@xie2016bookdown] is an evolution of __knitr__, also written by Yihui Xie, now working for RStudio. 
The text you are now reading was actually written in __bookdown__.
It deals with the particular needs of writing large documents, and cross referencing in particular (which is very challenging if you want the text to be human readable).

1. __Shiny__:
Shiny is essentially a framework for quick web-development. 
It includes (i) an abstraction layer that specifies the layout of a web-site which is our report, (ii) the command to start a web server to deliver the site.
For more on Shiny see @shiny.






## knitr

### Installation
To run __knitr__ you will need to install the package.
```{r, eval=FALSE}
install.packages('knitr')
```

It is also recommended that you use it within RStudio (version>0.96), where you can easily create a new `.Rmd` file. 

### Pandoc Markdown

Because __knitr__ builds upon _Pandoc markdown_, here is a simple example of markdown text, to be used in a `.Rmd` file, which can be created using the _File-> New File -> R Markdown_ menu of RStudio.


Underscores or asterisks for `_italics1_` and `*italics2*` return _italics1_ and *italics2*.
Double underscores or asterisks for `__bold1__` and `**bold2**` return __bold1__ and **bold2**.
Subscripts are enclosed in tildes, `like~this~` (like~this~), and superscripts are enclosed in carets `like^this^` (like^this^).

For links use `[text](link)`, like `[my site](www.john-ros.com)`.
An image is the same as a link, starting with an exclamation, like this `![image caption](image path)`.

An itemized list simply starts with hyphens preceeded by a blank line (don't forget that!):
```

- bullet
- bullet
    - second level bullet
    - second level bullet
```
Compiles into:

- bullet
- bullet
    - second level bullet
    - second level bullet

An enumerated list starts with an arbitrary number:
```
1. number
1. number
    1. second level number
    1. second level number
```
Compiles into:

1. number
1. number
    1. second level number
    1. second level number


For more on markdown see https://bookdown.org/yihui/bookdown/markdown-syntax.html.




### Rmarkdown
_Rmarkdown_, is an extension of _markdown_ due to RStudio, that allows to incorporate R expressions in the text, that will be evaluated at the time of compilation, and the output automatically inserted in the outputted text. 
The output can be a `.PDF`, `.DOCX`, `.HTML` or others, thanks to the power of _Pandoc_.

The start of a code chunk is indicated by three backticks and the end of a code chunk is indicated by three backticks.
Here is an example.


```{r, eval=FALSE, verbatim = TRUE}
rnorm(10)
```


This chunk will compile to the following output (after setting `eval=FALSE` to `eval=TRUE`):
```{r, eval=TRUE}
rnorm(10)
```


Things to note:

- The evaluated expression is added in a chunk of highlighted text, before the R output.
- The output is prefixed with `##`.
- The `eval=` argument is not required, since it is set to `eval=TRUE` by default. It does demonstrate how to set the options of the code chunk.


In the same way, we may add a plot:
```{r, eval=FALSE, verbatim = TRUE}
plot(rnorm(10))
```

which compiles into
```{r}
plot(rnorm(10))
```

TODO: more code chunk options.


You can also call r expressions inline. 
This is done with a single tick and the `r` argument.
For instance:

> `` `r '\x60r rnorm(1)\x60'` `` is a random Gaussian

will output 

> `r rnorm(1)` is a random Gaussian. 





### Compiling
Once you have your `.Rmd` file written in RMarkdown, __knitr__ will take care of the compilation for you. 
You can call the `knitr::knitr` function directly from some `.R` file, or more conveniently, use the RStudio (0.96) Knit button above the text editing window.
The location of the output file will be presented in the console.


## bookdown
As previously stated, __bookdown__ is an extension of __knitr__ intended for documents more complicated than simple reports-- such as books.
Just like __knitr__, the writing is done in __RMarkdown__.
Being an extension of __knitr__, __bookdown__ does allow some markdowns that are not supported by other compilers. 
In particular, it has a more powerful cross referencing system. 





## Shiny

__Shiny__ [@shiny] is different than the previous systems, because it sets up an interactive web-site, and not a static file.
The power of Shiny is that the layout of the web-site, and the settings of the web-server, is made with several simple R commands, with no need for web-programming.
Once you have your app up and running, you can setup your own Shiny server on the web, or publish it via [Shinyapps.io](https://www.shinyapps.io/).
The freemium versions of the service can deal with a small amount of traffic. If you expect a lot of traffic, you will probably need the paid versions.


### Installation
To setup your first Shiny app, you will need the __shiny__ package. 
You will probably want RStudio, which facilitates the process.

```{r, eval=FALSE}
install.packages('shiny')
```

Once installed, you can run an example app to get the feel of it.

```{r, eval=FALSE}
library(shiny)
runExample("01_hello")
```

Remember to press the __Stop__ button in RStudio to stop the web-server, and get back to RStudio.


### The Basics of Shiny
Every Shiny app has two main building blocks.

1. A user interface, specified via the `ui.R` file in the app's directory.
1. A server side, specified via the `server.R` file, in the app's directory.

You can run the app via the __RunApp__ button in the RStudio interface, of by calling the app's directory with the `shinyApp` or `runApp` functions-- the former designed for single-app projects, and the latter, for multiple app projects. 

```{r, eval=FALSE}
shiny::runApp("my_app")
```

The site's layout, is specified via _layout functions_ in the `iu.R` file. 
For instance, the function `sidebarLayout`, as the name suggest, will create a sidebar. 
More layouts are detailed in the [layout guide](http://shiny.rstudio.com/articles/layout-guide.html).

The active elements in the UI, that control your report, are known as _widgets_.
Each widget will have a unique `inputId` so that it's values can be sent from the UI to the server. 
More about widgets, in the [widget gallery](http://shiny.rstudio.com/gallery/widget-gallery.html).


The `inputId` on the UI are mapped to `input` arguments on the server side. 
The value of the `mytext` `inputId` can be queried by the server using `input$mytext`.
These are called _reactive values_.
The way the server "listens" to the UI, is governed by a set of functions that must wrap the `input` object. 
These are the `observe`, `reactive`, and `reactive*` class of functions.

With `observe` the server will get triggered when any of the reactive values change. 
With `observeEvent` the server will only be triggered by specified reactive values. 
Using `observe` is easier, and `observeEvent` is more prudent programming.

A `reactive` function is a function that gets triggered when a reactive element changes.
It is defined on the server side, and reside within an `observe` function.



We now analyze the `1_Hello` app using these ideas.
Here is the `io.R` file.
```{r, eval=FALSE}
library(shiny)

shinyUI(fluidPage(

  titlePanel("Hello Shiny!"),

  sidebarLayout(
    sidebarPanel(
      sliderInput(inputId = "bins",
                  label = "Number of bins:", 
                  min = 1,
                  max = 50,
                  value = 30)
    ),

    mainPanel(
      plotOutput(outputId = "distPlot")
    )
  )
))

```

Here is the `server.R` file:
```{r, eval=FALSE}
library(shiny)

shinyServer(function(input, output) {

  output$distPlot <- renderPlot({
    x    <- faithful[, 2]  # Old Faithful Geyser data
    bins <- seq(min(x), max(x), length.out = input$bins + 1)

    hist(x, breaks = bins, col = 'darkgray', border = 'white')
  })
})
```

Things to note:

- `ShinyUI` is a (deprecated) wrapper for the UI.
- `fluidPage` ensures that the proportions of the elements adapt to the window side, thus, are fluid. 
- The building blocks of the layout are a title, and the body. The title is governed by `titlePanel`, and the body is governed by `sidebarLayout`. The `sidebarLayout` includes  the `sidebarPanel` to control the sidebar, and the `mainPanel` for the main panel.
- `sliderInput` calls a widget with a slider. Its `inputId` is `bins`, which is later used by the server within the `renderPlot` reactive function.
- `plotOutput` specifies that the content of the `mainPanel` is a plot (`textOutput` for text). This expectation is satisfied on the server side with the `renderPlot` function (`renderText`).
- `shinyServer` is a (deprecated) wrapper function for the server. 
- The server runs a function with an `input` and an `output`. The elements of `input` are the `inputId`s from the UI. The elements of the `output` will be called by the UI using their `outputId`. 

This is the output.
```{r}
knitr::include_url('http://shiny.rstudio.com/gallery/example-01-hello.html')
```




Here is another example, taken from the RStudio [Shiny examples](https://github.com/rstudio/shiny-examples/tree/master/006-tabsets).

`ui.R`:
```{r, eval=FALSE}
library(shiny)

fluidPage(
    
  titlePanel("Tabsets"),
  
  sidebarLayout(
    sidebarPanel(
      radioButtons(inputId = "dist", 
                   label = "Distribution type:",
                   c("Normal" = "norm",
                     "Uniform" = "unif",
                     "Log-normal" = "lnorm",
                     "Exponential" = "exp")),
      br(),
      
      sliderInput(inputId = "n", 
                  label = "Number of observations:", 
                   value = 500,
                   min = 1, 
                   max = 1000)
    ),
    
    mainPanel(
      tabsetPanel(type = "tabs", 
        tabPanel(title = "Plot", plotOutput(outputId = "plot")), 
        tabPanel(title = "Summary", verbatimTextOutput(outputId = "summary")), 
        tabPanel(title = "Table", tableOutput(outputId = "table"))
      )
    )
  )
)
```

`server.R`:
```{r, eval=FALSE}
library(shiny)

# Define server logic for random distribution application
function(input, output) {
  
  data <- reactive({
    dist <- switch(input$dist,
                   norm = rnorm,
                   unif = runif,
                   lnorm = rlnorm,
                   exp = rexp,
                   rnorm)
    
    dist(input$n)
  })
  
  output$plot <- renderPlot({
    dist <- input$dist
    n <- input$n
    
    hist(data(), main=paste('r', dist, '(', n, ')', sep=''))
  })
  
  output$summary <- renderPrint({
    summary(data())
  })
  
  output$table <- renderTable({
    data.frame(x=data())
  })
  
}
```

Things to note:

- We reused the `sidebarLayout`. 
- As the name suggests, `radioButtons` is a widget that produces radio buttons, above the `sliderInput` widget. Note the different `inputId`s.
- Different widgets are separated in `sidebarPanel` by commas. 
- `br()` produces extra vertical spacing. 
- `tabsetPanel` produces tabs in the main output panel. `tabPanel` governs the content of each panel. Notice the use of various output functions (`plotOutput`,`verbatimTextOutput`, `tableOutput`)  with corresponding `outputId`s. 
- In `server.R` we see the usual `function(input,output)`.
- The `reactive` function tells the server the trigger the function whenever `input` changes.
- The `output` object is constructed outside the `reactive` function. See how the elements of `output` correspond to the `outputId`s in the UI.


This is the output:
```{r, cache=TRUE}
knitr::include_url('https://shiny.rstudio.com/gallery/tabsets.html')
```


### Beyond the Basics

Now that we have seen the basics, we may consider extensions to the basic report. 

#### Widgets

- `actionButton`	Action Button.
- `checkboxGroupInput`	A group of check boxes.
- `checkboxInput`	A single check box.
- `dateInput`	A calendar to aid date selection.
- `dateRangeInput`	A pair of calendars for selecting a date range.
- `fileInput`	A file upload control wizard.
- `helpText`	Help text that can be added to an input form.
- `numericInput`	A field to enter numbers.
- `radioButtons`	A set of radio buttons.
- `selectInput`	A box with choices to select from.
- `sliderInput`	A slider bar.
- `submitButton`	A submit button.
- `textInput`	A field to enter text.

See examples [here](https://shiny.rstudio.com/gallery/widget-gallery.html).

```{r, cache=TRUE}
knitr::include_url('https://shiny.rstudio.com/gallery/widget-gallery.html')
```


#### Output Elements

The `ui.R` output types. 

- `htmlOutput`	raw HTML.
- `imageOutput`	image.
- `plotOutput`	plot.
- `tableOutput`	table.
- `textOutput`	text.
- `uiOutput`	raw HTML.
- `verbatimTextOutput`	text.

The corresponding `server.R` renderers.

- `renderImage`	images (saved as a link to a source file)
- `renderPlot`	plots
- `renderPrint`	any printed output
- `renderTable`	data frame, matrix, other table like structures
- `renderText`	character strings
- `renderUI`	a Shiny tag object or HTML


Your Shiny app can use any R object. 
The things to remember:

- The working directory of the app is the location of `server.R`.
- The code before `shinyServer` is run only once. 
- The code inside ``shinyServer` is run whenever a reactive is triggered, and may thus slow things.


To keep learning, see the RStudio's [tutorial](http://shiny.rstudio.com/tutorial/), and the Biblipgraphic notes herein.



## Flexdashboard
http://rmarkdown.rstudio.com/flexdashboard/

TODO: write section

## Bibliographic Notes

For RMarkdown see [here](http://rmarkdown.rstudio.com/).
For everything on __knitr__ see [Yihui's blog](https://yihui.name/knitr/), or the book @xie2015dynamic.
For a __bookdown__ manual, see @xie2016bookdown.
For a Shiny manual, see @shiny, the [RStudio tutorial](http://shiny.rstudio.com/tutorial/), or [Zev Ross's](http://zevross.com/blog/2016/04/19/r-powered-web-applications-with-shiny-a-tutorial-and-cheat-sheet-with-40-example-apps/) excellent guide.
Video tutorials are available [here](https://www.rstudio.com/resources/webinars/shiny-developer-conference/).

## Practice Yourself

<!--chapter:end:11-reporting.Rmd-->

# The Hadleyverse {#hadley}

The _Hadleyverse_, short for "Hadley Wickham's universe", is a set of packages that make it easier to handle data. 
If you are developing packages, you should be careful since using these packages may create many dependencies and compatibility issues. 
If you are analyzing data, and the portability of your functions to other users, machines, and operating systems is not of a concern, you will LOVE these packages. 
The term Hadleyverse refers to __all__ of Hadley's packages, but here, we mention only a useful subset, which can be collectively installed via the __tidyverse__ package:

- __ggplot2__ for data visualization. See the Plotting Chapter \@ref(plotting).
- __dplyr__ for data manipulation.
- __tidyr__ for data tidying.
- __readr__ for data import.
- __stringr__ for character strings. 
- __anytime__ for time data. 



## readr

The __readr__ package [@readr] replaces base functions for importing and exporting data such as `read.table`. 
It is faster, with a cleaner syntax. 

We will not go into the details and refer the reader to the official documentation [here](http://readr.tidyverse.org/articles/readr.html) and the [R for data sciecne](http://r4ds.had.co.nz/data-import.html) book.



## dplyr  

When you think of data frame operations, think __dplyr__ [@dplyr].
Notable utilities in the package include:

- `select()`	Select columns from a data frame.
- `filter()`	Filter rows according to some condition(s).
- `arrange()`	Sort / Re-order rows in a data frame.
- `mutate()`	Create new columns or transform existing ones.
- `group_by()`	Group a data frame by some factor(s) usually in conjunction to summary.
- `summarize()`	Summarize some values from the data frame or across groups.
- `inner_join(x,y,by="col")`return all rows from x where there are matching values in x, and all columns from x and y. If there are multiple matches between x and y, all combination of the matches are returned.
- `left_join(x,y,by="col")`	return all rows from x, and all columns from x and y. Rows in x with no match in y will have NA values in the new columns. If there are multiple matches between x and y, all combinations of the matches are returned.
- `right_join(x,y,by="col")` return all rows from y, and all columns from x and y. Rows in y with no match in x will have NA values in the new columns. If there are multiple matches between x and y, all combinations of the matches are returned.
- `anti_join(x,y,by="col")`	return all rows from x where there are not matching values in y, keeping just columns from x.

The following example involve `data.frame` objects, but __dplyr__ can handle other classes. 
In particular `data.table`s from the __data.table__ package [@datatable], which is designed for very large data sets. 

__dplyr__ can work with data stored in a database. 
In which case, it will convert your command to the appropriate SQL syntax, and issue it to the database. 
This has the advantage that (a) you do not need to know the specific SQL implementation of your database, and (b), you can enjoy the optimized algorithms provided by the database supplier. 
For more on this, see the [databses vignette](https://cran.r-project.org/web/packages/dplyr/vignettes/databases.html).



The following examples are taken from [Kevin Markham](https://github.com/justmarkham/dplyr-tutorial/blob/master/dplyr-tutorial.Rmd).
The `nycflights13::flights` has delay data for US flights. 
```{r}
library(nycflights13)
flights
```

The data is of class `tbl_df` which is an extension of the `data.frame` class, designed for large data sets. 
Notice that the printing of `flights` is short, even without calling the `head` function. This is a feature of the `tbl_df` class ( `print(data.frame)` would try to load all the data, thus take a long time).

```{r}
class(flights) # a tbl_df is an extension of the data.frame class
```


Let's filter the observations from the first day of the first month.
Notice how much better (i.e. readable) is the __dplyr__ syntax, with piping, compared to the basic syntax.

```{r, eval=FALSE}
flights[flights$month == 1 & flights$day == 1, ] # old style

library(dplyr) 
filter(flights, month == 1, day == 1) #dplyr style
flights %>% filter(month == 1, day == 1) # dplyr with piping.
```

More filtering.

```{r, eval=FALSE}
filter(flights, month == 1 | month == 2) # First OR second month.
slice(flights, 1:10) # selects first ten rows.

arrange(flights, year, month, day) # sort
arrange(flights, desc(arr_delay)) # sort descending

select(flights, year, month, day) # select columns year, month, and day
select(flights, year:day) # select column range
select(flights, -(year:day)) # drop columns
rename(flights, tail_num = tailnum) # rename column

# add a new computed colume
mutate(flights,
  gain = arr_delay - dep_delay,
  speed = distance / air_time * 60) 

# you can refer to columns you just created! (gain)
mutate(flights,
  gain = arr_delay - dep_delay,
  gain_per_hour = gain / (air_time / 60)
)

# keep only new variables, not all data frame.
transmute(flights,
  gain = arr_delay - dep_delay,
  gain_per_hour = gain / (air_time / 60)
)

# simple statistics
summarise(flights,
  delay = mean(dep_delay, na.rm = TRUE)
  )
# random subsample 
sample_n(flights, 10) 
sample_frac(flights, 0.01) 

```

We now perform operations on subgroups.
we group observations along the plane's tail number (`tailnum`), and compute the count, average distance traveled, and average delay.
We group with `group_by`, and compute subgroup statistics with `summarise`.

```{r}
by_tailnum <- group_by(flights, tailnum)

delay <- summarise(by_tailnum,
  count = n(),
  avg.dist = mean(distance, na.rm = TRUE),
  avg.delay = mean(arr_delay, na.rm = TRUE))

delay
```


We can group along several variables, with a hierarchy.
We then collapse the hierarchy one by one. 

```{r}
daily <- group_by(flights, year, month, day)
per_day   <- summarise(daily, flights = n())
per_month <- summarise(per_day, flights = sum(flights))
per_year  <- summarise(per_month, flights = sum(flights))
```

Things to note:

- Every call to `summarise` collapses one level in the hierarchy of grouping. The output of `group_by` recalls the hierarchy of aggregation, and collapses along this hierarchy.

We can use __dplyr__ for two table operations, i.e., _joins_.
For this, we join the flight data, with the airplane data in `airplanes`.

```{r}
library(dplyr) 
airlines  

# select the subset of interesting flight data. 
flights2 <- flights %>% select(year:day, hour, origin, dest, tailnum, carrier) 

# join on left table with automatic matching.
flights2 %>% left_join(airlines) 

flights2 %>% left_join(weather) 

# join with named matching
flights2 %>% left_join(planes, by = "tailnum") 

# join with explicit column matching
flights2 %>% left_join(airports, by= c("dest" = "faa")) 
```

Types of join with SQL equivalent. 

```{r}
# Create simple data
(df1 <- data_frame(x = c(1, 2), y = 2:1))
(df2 <- data_frame(x = c(1, 3), a = 10, b = "a"))

# Return only matched rows
df1 %>% inner_join(df2) # SELECT * FROM x JOIN y ON x.a = y.a

# Return all rows in df1.
df1 %>% left_join(df2) # SELECT * FROM x LEFT JOIN y ON x.a = y.a

# Return all rows in df2.
df1 %>% right_join(df2) # SELECT * FROM x RIGHT JOIN y ON x.a = y.a

# Return all rows. 
df1 %>% full_join(df2) # SELECT * FROM x FULL JOIN y ON x.a = y.a

# Like left_join, but returning only columns in df1
df1 %>% semi_join(df2, by = "x")  # SELECT * FROM x WHERE EXISTS (SELECT 1 FROM y WHERE x.a = y.a)
```






## tidyr







## reshape2



## stringr


## anytime


## Biblipgraphic Notes

## Practice Yourself

<!--chapter:end:12-hadleyverse.Rmd-->

# Sparse Representations {#sparse}

Analyzing "bigdata" in R is a challenge because the workspace is memory resident, i.e., all your objects are stored in RAM.
As a rule of thumb, fitting models requires about 5 times the size of the data. 
This means that if you have 1 GB of data, you might need about 5 GB to fit a linear models.
We will discuss how to compute _out of RAM_  in the Memory Efficiency Chapter \@ref(memory).
In this chapter, we discuss efficient representations of your data, so that it takes less memory.
The fundamental idea, is that if your data is _sparse_, i.e., there are many zero entries in your data, then a naive `data.frame` or `matrix` will consume memory for all these zeroes. 
If, however, you have many recurring zeroes, it is more efficient to save only the non-zero entries.

When we say _data_, we actually mean the `model.matrix`.
The `model.matrix` is a matrix that R grows, converting all your factors to numeric variables that can be computed with.
_Dummy coding_ of your factors, for instance, is something that is done in your `model.matrix`.
If you have a factor with many levels, you can imagine that after dummy coding it, many zeroes will be present.

The __Matrix__ package replaces the `matrix` class, with several sparse representations of matrix objects. 

When using sparse representation, and the __Matrix__ package, you will need an implementation of your favorite model fitting algorithm (e.g. `lm`) that is adapted to these sparse representations; otherwise, R will cast the sparse matrix into a regular (non-sparse) matrix, and you will have saved nothing in RAM.

```{remark}
If you are familiar with MATLAB you should know that one of the great capabilities of MATLAB, is the excellent treatment of sparse matrices with the `sparse` function.
```




Before we go into details, here is a simple example.
We will create a factor of letters with the `letters` function.
Clearly, this factor can take only $26$ values. 
This means that $25/26$ of the `model.matrix` will be zeroes after dummy coding.
We will compare the memory footprint of the naive `model.matrix` with the sparse representation of the same matrix. 

```{r make x, cache=TRUE}
library(magrittr)
reps <- 1e6 # number of samples
y<-rnorm(reps)
x<- letters %>% 
  sample(reps, replace=TRUE) %>% 
  factor
```

The object `x` is a factor of letters:
```{r}
head(x)
```

We dummy code `x` with the `model.matrix` function.

```{r make X1, cache=TRUE}
X.1 <- model.matrix(~x-1)
head(X.1)
```

We call __MatrixModels__ for an implementation of `model.matrix` that supports sparse representations.

```{r make X2, cache=TRUE}
suppressPackageStartupMessages(library(MatrixModels))
X.2<- as(x,"sparseMatrix") %>% t # Makes sparse dummy model.matrix
head(X.2)
```

Notice that the matrices have the same dimensions:

```{r}
dim(X.1)
dim(X.2)
```

The memory footprint of the matrices, given by the `pryr::object_size` function, are very very different.

```{r}
pryr::object_size(X.1)
pryr::object_size(X.2)
```

Things to note:

- The sparse representation takes a whole lot less memory than the non sparse. 
- The `as(,"sparseMatrix")` function grows the dummy variable representation of the factor `x`.
- The __pryr__ package provides many facilities for inspecting the memory footprint of your objects and code. 


With a sparse representation, we not only saved on RAM, but also on the computing time of fitting a model.
Here is the timing of a non sparse representation:

```{r nonSparse lm, cache=TRUE, dependson='make X1'}
system.time(lm.1 <- lm(y ~ X.1)) 
```

Well actually, `lm` is a wrapper for the `lm.fit` function. 
If we override all the overhead of `lm`, and call `lm.fit` directly, we gain some time:

```{r nonSparse lmfit, cache=TRUE, dependson='make X1'}
system.time(lm.1 <- lm.fit(y=y, x=X.1))
```

We now do the same with the sparse representation:

```{r sparselm, cache=TRUE, dependson='make X2'}
system.time(lm.2 <- MatrixModels:::lm.fit.sparse(X.2,y))
```

It is only left to verify that the returned coefficients are the same:

```{r}
all.equal(lm.2, unname(lm.1$coefficients), tolerance = 1e-12)
```

You can also visualize the non zero entries, i.e., the sparsity structure.

```{r}
image(X.2[1:26,1:26])
```



## Sparse Matrix Representations

We first distinguish between the two main goals of the efficient representation:
(i) efficient writing, i.e., modification; 
(ii) efficient reading, i.e., access. 
For our purposes, we will typically want efficient reading, since the `model.matrix` will not change while a model is being fitted.

Representations designed for writing include the _dictionary of keys_, _list of lists_, and a _coordinate list_. 
Representations designed for efficient reading include the _compressed sparse row_ and _compressed sparse column_.


### Coordinate List Representation {#coo}
A _coordinate list representation_, also known as _COO_, or _triplet represantation_ is simply a list of the non zero entries. 
Each element in the list is a triplet of the column, row, and value, of each non-zero entry in the matrix.


### Compressed Column Oriented Representation 
A _compressed column oriented representation_, also known as _compressed sparse column_, or _CSC_, where the __column__ index is similar to COO, but instead of saving the row indexes, we save the locations in the column index vectors where the row index has to increase. 
The following figure may clarify this simple idea.

![The CSC data structure. From @shah2004sparse. Remember that MATLAB is written in C, where the indexing starts at $0$, and not $1$.](art/crc.png)

The nature of statistical applications is such, that CSC representation is typically the most economical, justifying its popularity.


### Compressed Row Oriented Representation 
A _compressed row oriented representation_, also known as _compressed sparse row_, or _CSR_, is very similar to CSC, after switching the role of rows and columns.
CSR is much less popular than CSC.





### Sparse Algorithms

We will go into the details of some algorithms in the Numerical Linear Algebra Chapter \@ref(algebra).
For our current purposes two things need to be emphasized:

1. A mathematician may write $Ax=b \Rightarrow x=A^{-1}b$. A computer, however, would __never__ compute $A^{-1}$ in order to find $x$, but rather use one of many endlessly many numerical algorithms. 

1. Working with sparse representations requires using a function that is aware of the representation you are using. 




## Sparse Matrices and Sparse Models in R

### The Matrix Package

The __Matrix__ package provides facilities to deal with real (stored as double precision), logical and so-called "pattern" (binary) dense and sparse matrices. 
There are provisions to provide integer and complex (stored as double precision complex) matrices.

The sparse matrix classes include:

- `TsparseMatrix`: a virtual class of the various sparse matrices in triplet representation.
- `CsparseMatrix`: a virtual class of the various sparse matrices in CSC representation.
- `RsparseMatrix`: a virtual class of the various sparse matrices in CSR representation.

For matrices of real numbers, stored in _double precision_, the __Matrix__ package provides the following (non virtual) classes:

- `dgTMatrix`: a __general__ sparse matrix of __doubles__, in __triplet__ representation.
- `dgCMatrix`: a __general__ sparse matrix of __doubles__, in __CSC__ representation.
- `dsCMatrix`: a __symmetric__ sparse matrix of __doubles__, in __CSC__ representation.
- `dtCMatrix`: a __triangular__ sparse matrix of __doubles__, in __CSC__ representation.

Why bother with distinguishing between the different shapes of the matrix?
Because the more structure is assumed on a matrix, the more our (statistical) algorithms can be optimized. 
For our purposes `dgCMatrix` will be the most useful.



### The glmnet Package

As previously stated, an efficient storage of the `model.matrix` is half of the story.
We now need implementations of our favorite statistical algorithms that make use of this representation. 
At the time of writing, a very useful package that does that is the __glmnet__ package, which allows to fit linear models, generalized linear models, with ridge, lasso, and elastic net regularization. 
The __glmnet__ package allows all of this, using the sparse matrices of the __Matrix__ package. 

The following example is taken from [John Myles White's blog](http://www.johnmyleswhite.com/notebook/2011/10/31/using-sparse-matrices-in-r/), and compares the runtime of fitting an OLS model, using `glmnet` with both sparse and dense matrix representations. 
```{r sparse glmnet simulation, cache=TRUE}
suppressPackageStartupMessages(library('glmnet'))

set.seed(1)
performance <- data.frame()
 
for (sim in 1:10){
  n <- 10000
  p <- 500
 
  nzc <- trunc(p / 10)
 
  x <- matrix(rnorm(n * p), n, p) #make a dense matrix
  iz <- sample(1:(n * p),
               size = n * p * 0.85,
               replace = FALSE)
  x[iz] <- 0 # sparsify by injecting zeroes
  sx <- Matrix(x, sparse = TRUE) # save as a sparse object
 
  beta <- rnorm(nzc)
  fx <- x[, seq(nzc)] %*% beta
 
  eps <- rnorm(n)
  y <- fx + eps # make data
 
  # Now to the actual model fitting:
  sparse.times <- system.time(fit1 <- glmnet(sx, y)) # sparse glmnet
  full.times <- system.time(fit2 <- glmnet(x, y)) # dense glmnet
  
  sparse.size <- as.numeric(object.size(sx))
  full.size <- as.numeric(object.size(x))
 
  performance <- rbind(performance, data.frame(Format = 'Sparse',
                                                UserTime = sparse.times[1],
                                               SystemTime = sparse.times[2],
                                               ElapsedTime = sparse.times[3],
                                               Size = sparse.size))
  performance <- rbind(performance, data.frame(Format = 'Full',
                                               UserTime = full.times[1],
                                               SystemTime = full.times[2],
                                               ElapsedTime = full.times[3],
                                               Size = full.size))
}

```

Things to note:

- The simulation calls `glmnet` twice. Once with the non-sparse object `x`, and once with its sparse version `sx`.
- The degree of sparsity of `sx` is $85\%$. We know this because we "injected" zeroes in $0.85$ of the locations of `x`. 
- Because `y` is continuous `glmnet` will fit a simple OLS model. We will see later how to use it to fit GLMs and use lasso, ridge, and elastic-net regularization. 

We now inspect the computing time, and the memory footprint, only to discover that sparse representations make a BIG difference. 

```{r}
suppressPackageStartupMessages(library('ggplot2'))
ggplot(performance, aes(x = Format, y = ElapsedTime, fill = Format)) +
  stat_summary(fun.data = 'mean_cl_boot', geom = 'bar') +
  stat_summary(fun.data = 'mean_cl_boot', geom = 'errorbar') +
  ylab('Elapsed Time in Seconds') 
```

```{r}
ggplot(performance, aes(x = Format, y = Size / 1000000, fill = Format)) +
  stat_summary(fun.data = 'mean_cl_boot', geom = 'bar') +
  stat_summary(fun.data = 'mean_cl_boot', geom = 'errorbar') +
  ylab('Matrix Size in MB') 
```


How do we perform other types of regression with the __glmnet__?
We just need to use the `family` and `alpha` arguments of `glmnet::glmnet`.
The `family` argument governs the type of GLM to fit: logistic, Poisson, probit, or other types of GLM.
The `alpha` argument controls the type of regularization. Set to `alpha=0` for ridge, `alpha=1` for lasso, and any value in between for elastic-net regularization.




## Bibliographic Notes

The best place to start reading on sparse representations and algorithms is the [vignettes](http://svitsrv25.epfl.ch/R-doc/library/Matrix/doc/) of the __Matrix__ package. 
@gilbert1992sparse is also a great read for some general background.
For the theory on solving sparse linear systems see @davis2006direct.
For general numerical linear algebra see @gentle2012numerical.


## Practice Yourself

<!--chapter:end:13-sparsity.Rmd-->

# Memory Efficiency {#memory}

As put by @kane2013scalable, it was quite  puzzling when very few of the competitors, for the Million dollars prize in the [Netflix challenge](https://en.wikipedia.org/wiki/Netflix_Prize), were statisticians.
This is perhaps because the statistical community historically uses SAS, SPSS, and R. 
The first two tools are very well equipped to deal with big data, but are very unfriendly when trying to implement a new method. 
R, on the other hand, is very friendly for innovation, but was not equipped to deal with the large data sets of the Netflix challenge. 
A lot has changed in R since 2006. This is the topic of this chapter. 



As we have seen in the Sparsity Chapter \@ref(sparse), an efficient representation of your data in RAM will reduce computing time, and will allow you to fit models that would otherwise require tremendous amounts of RAM.
Not all problems are sparse however. 
It is also possible that your data does not fit in RAM, even if sparse.
There are several scenarios to consider:

1. Your data fits in RAM, but is too big to compute with.
1. Your data does not fit in RAM, but fits in your local storage (HD, SSD, etc.)
1. Your data does not fit in your local storage.

If your data fits in RAM, but is too large to compute with, a solution is to replace the algorithm you are using.
Instead of computing with the whole data, your algorithm will compute with parts of the data, also called _chunks_, or _batches_.
These algorithms are known as _external memory algorithms_ (EMA).

If your data does not fit in RAM, but fits in your local storage, you have two options.
The first is to save your data in a _database management system_ (DBMS).
This will allow you to use the algorithms provided by your DBMS, or let R use an EMA while "chunking" from your DBMS. 
Alternatively, and preferably, you may avoid using a DBMS, and work with the data directly form your local storage by saving your data in some efficient manner. 

Finally, if your data does not fit on you local storage, you will need some external storage solution such as a distributed DBMS, or distributed file system.


```{remark}
If you use Linux, you may be better of than Windows users. 
Linux will allow you to compute with larger datasets using its _swap file_ that extends RAM using your HD or SSD.
On the other hand, relying on the swap file is a BAD practice since it is much slower than RAM, and you can typically do much better using the tricks of this chapter. 
Also, while I LOVE Linux, I would never dare to recommend switching to Linux just to deal with memory contraints. 
```


## Efficient Computing from RAM

If our data can fit in RAM, but is still too large to compute with it (recall that fitting a model requires roughly 5-10 times more memory than saving it), there are several facilities to be used.
The first, is the sparse representation discussed in Chapter \@ref(sparse), which is relevant when you have factors, which will typically map to sparse model matrices. 
Another way is to use _external memory algorithms_ (EMA). 

The `biglm::biglm` function provides an EMA for linear regression.
The following if taken from the function's example.

```{r biglm}
data(trees)
ff<-log(Volume)~log(Girth)+log(Height)

chunk1<-trees[1:10,]
chunk2<-trees[11:20,]
chunk3<-trees[21:31,]

library(biglm)
a <- biglm(ff,chunk1)
a <- update(a,chunk2)
a <- update(a,chunk3)

coef(a)
```

Things to note:

- The data has been chunked along rows. 
- The initial fit is done with the `biglm` function.
- The model is updated with further chunks using the `update` function.

We now compare it to the in-memory version of `lm` to verify the results are the same.

```{r}
b <- lm(ff, data=trees)
rbind(coef(a),coef(b))
```

Other packages that follow these lines, particularly with classification using SVMs, are  __LiblineaR__, and __RSofia__. 


### Summary Statistics from RAM

If you are not going to do any model fitting, and all you want is efficient filtering, selection and summary statistics, then a lot of my warnings above are irrelevant.
For these purposes, the facilities provided by __base__, __stats__, and __dplyr__ are probably enough.
If the data is large, however, these facilities may be too slow.
If your data fits into RAM, but speed bothers you, take a look at the __data.table__ package. 
The syntax is less friendly than __dplyr__, but __data.table__ is BLAZING FAST compared to competitors. 
Here is a little benchmark^[The code was contributed by Liad Shekel.].

First, we setup the data. 
```{r dplys vs datatable}
library(data.table)

n <- 1e6 # number of rows
k <- c(200,500) # number of distinct values for each 'group_by' variable
p <- 3 # number of variables to summarize

L1 <- sapply(k, function(x) as.character(sample(1:x, n, replace = TRUE) ))
L2 <- sapply(1:p, function(x) rnorm(n) )

tbl <- data.table(L1,L2) %>% 
  setnames(c(paste("v",1:length(k),sep=""), paste("x",1:p,sep="") ))

tbl_dt <- tbl
tbl_df <- tbl %>% as.data.frame
```

We compare the aggregation speeds.
Here is the timing for __dplyr__.

```{r datatable aggregation}
system.time( tbl_df %>% 
               group_by(v1,v2) %>% 
               summarize(
                 x1 = sum(abs(x1)), 
                 x2 = sum(abs(x2)), 
                 x3 = sum(abs(x3)) 
                 )
             )

```

And now the timing for __data.table__.

```{r}
system.time( 
  tbl_dt[ ,  .( x1 = sum(abs(x1)), x2 = sum(abs(x2)), x3 = sum(abs(x3)) ), .(v1,v2)]
  )
```

The winner is obvious. 
Let's compare filtering (i.e. row subsets, i.e. SQL's SELECT).

```{r}
system.time( 
  tbl_df %>% filter(v1 == "1") 
  )
```

```{r}
system.time( 
  tbl_dt[v1 == "1"] 
  )
```






## Computing from a Database

The early solutions to oversized data relied on storing your data in some DBMS such as _MySQL_, _PostgresSQL_, _SQLite_, _H2_, _Oracle_, etc.
Several R packages provide interfaces to these DBMSs, such as __sqldf__, __RDBI__, __RSQite__.
Some will even include the DBMS as part of the package itself.

Storing your data in a DBMS has the advantage that you can typically rely on DBMS providers to include very efficient algorithms for the queries they support. 
On the downside, SQL queries may include a lot of summary statistics, but will rarely include model fitting^[This is slowly changing. Indeed, Microsoft's SQL Server 2016 is already providing [in-database-analytics](https://blogs.technet.microsoft.com/dataplatforminsider/2016/03/29/in-database-advanced-analytics-with-r-in-sql-server-2016/), and other will surely follow.]. 
This means that even for simple things like linear models, you will have to revert to R's facilities-- typically some sort of EMA with chunking from the DBMS. 
For this reason, and others, we prefer to compute from efficient file structures, as described in Section \@ref(file-structure).

If, however, you have a powerful DBMS around, or you only need summary statistics, or you are an SQL master, keep reading.

The package __RSQLite__ includes an SQLite server, which we now setup for demonstration.
The package __dplyr__, discussed in the Hadleyverse Chapter \@ref(hadley), will take care of translating the __dplyr__ syntax, to the SQL syntax of the DBMS. 
The following example is taken from the __dplyr__ [Databases vignette](https://cran.r-project.org/web/packages/dplyr/vignettes/databases.html).

```{r}
library(RSQLite)
library(dplyr)

file.remove('my_db.sqlite3')
my_db <- src_sqlite(path = "my_db.sqlite3", create = TRUE)

library(nycflights13)
flights_sqlite <- copy_to(
  dest= my_db, 
  df= flights, 
  temporary = FALSE, 
  indexes = list(c("year", "month", "day"), "carrier", "tailnum"))
```

Things to note:

- `src_sqlite` to start an empty table, managed by SQLite, at the desired path.
- `copy_to` copies data from R to the database. 
- Typically, setting up a DBMS like this makes no sense, since it requires loading the data into RAM, which is precisely what we want to avoid.

We can now start querying the DBMS.

```{r}
select(flights_sqlite, year:day, dep_delay, arr_delay)
```

```{r}
filter(flights_sqlite, dep_delay > 240)
```

```{r}
summarise(flights_sqlite, delay = mean(dep_time))
```








## Computing From Efficient File Structrures {#file-structure}

It is possible to save your data on your storage device, without the DBMS layer to manage it.
This has several advantages:

- You don't need to manage a DBMS.
- You don't have the computational overhead of the DBMS. 
- You may optimize the file structure for statistical modelling, and not for join and summary operations, as in relational DBMSs. 

There are several facilities that allow you to save and compute directly from your storage:

1. __Memory Mapping__:
Where RAM addresses are mapped to a file on your storage. 
This extends the RAM to the capacity of your storage (HD, SSD,...).
Performance slightly deteriorates, but the access is typically very fast.
This approach is implemented in the __bigmemory__ package.

1. __Efficient Binaries__:
Where the data is stored as a file on the storage device. 
The file is binary, with a well designed structure, so that chunking is easy.
This approach is implemented in the __ff__ package, and the commercial __RevoScaleR__ package. 

Your algorithms need to be aware of the facility you are using. 
For this reason each facility ( __bigmemory__, __ff__, __RevoScaleR__,...) has an eco-system of packages that implement various statistical methods using that facility.
As a general rule, you can see which package builds on a package using the _Reverse Depends_ entry in the package description.
For the __bigmemory__ package, for instance, [we can see](https://cran.r-project.org/web/packages/bigmemory/index.html) that the packages __bigalgebra__, __biganalytics__, __bigFastlm__, __biglasso__, __bigpca__, __bigtabulate__, __GHap__, and __oem__, build upon it. 
We can expect this list to expand.


Here is a benchmark result, from @wang2015statistical. 
It can be seen that __ff__ and __bigmemory__ have similar performance, while __RevoScaleR__ (RRE in the figure) outperforms them.
This has to do both with the efficiency of the binary representation, but also because __RevoScaleR__ is inherently parallel. 
More on this in the Parallelization Chapter \@ref(parallel).
![__bigmemory__ vs. __ff__ vs. __RevoScaleR__ when reading, trasnforming, or fitting a model to 12GB of data, on a standrard laptop (in 2015 standards).](art/benchmark.png)






### bigmemory

We now demonstrate the workflow of the __bigmemory__ package.
We will see that __bigmemory__, with it's `big.matrix` object is a very powerful mechanism.
If you deal with big numeric matrices, you will find it very useful.
If you deal with big data frames, or any other non-numeric matrix, __bigmemory__ may not be the appropriate tool, and you should try __ff__, or the commercial __RevoScaleR__.


```{r}
# download.file("http://www.cms.gov/Research-Statistics-Data-and-Systems/Statistics-Trends-and-Reports/BSAPUFS/Downloads/2010_Carrier_PUF.zip", "2010_Carrier_PUF.zip")
# unzip(zipfile="2010_Carrier_PUF.zip")

library("bigmemory")
x <- read.big.matrix("2010_BSA_Carrier_PUF.csv", header = TRUE, 
                     backingfile = "airline.bin", 
                     descriptorfile = "airline.desc", 
                     type = "integer")
dim(x)
pryr::object_size(x)
class(x)
```

Things to note:

- The basic building block of the __bigmemory__ ecosystem, is the `big.matrix` class, we constructed with `read.big.matrix`. 
- `read.big.matrix` handles the import to R, and the saving to a memory mapped file. The implementation is such that at no point does R hold the data in RAM.
- The memory mapped file will be there after the session is over. It can thus be called by other R sessions using ` attach.big.matrix("airline.desc")`. This will be useful when parallelizing. 
- `pryr::object_size` return the size of the object. Since `x` holds only the memory mappings, it is much smaller than the 100MB of data that it holds. 



We can now start computing with the data.
Many statistical procedures for the `big.matrix` object are provided by the __biganalytics__ package. 
In particular, the `biglm.big.matrix` and `bigglm.big.matrix` functions, provide an interface from `big.matrix` objects, to the EMA linear models in `biglm::biglm` and `biglm::bigglm`.

```{r, cache=TRUE}
library(biganalytics)
biglm.2 <- bigglm.big.matrix(BENE_SEX_IDENT_CD~CAR_LINE_HCPCS_CD, data=x)
coef(biglm.2)
```

Other notable packages that operate with `big.matrix` objects include:

- __bigtabulate__: Extend the bigmemory package with 'table', 'tapply', and 'split' support for 'big.matrix' objects.
- __bigalgebra__: For matrix operation.
- __bigpca__: principle components analysis (PCA), or singular value decomposition (SVD).
- __bigFastlm__: for (fast) linear models.
- __biglasso__: extends lasso and elastic nets.
- __GHap__: Haplotype calling from phased SNP data.



## ff

The __ff__ packages replaces R's in-RAM storage mechanism with on-disk (efficient) storage.
Unlike __bigmemory__, __ff__ supports all of R vector types such as factors, and not only numeric.
Unlike `big.matrix`, which deals with (numeric) matrices, the `ffdf` class can deal with data frames. 

Here is an example.
First open a connection to the file, without actually importing it using the `LaF::laf_open_csv` function.

```{r}
.dat <- LaF::laf_open_csv(filename = "2010_BSA_Carrier_PUF.csv",
                    column_types = c("integer", "integer", "categorical", "categorical", "categorical", "integer", "integer", "categorical", "integer", "integer", "integer"), 
                    column_names = c("sex", "age", "diagnose", "healthcare.procedure", "typeofservice", "service.count", "provider.type", "servicesprocessed", "place.served", "payment", "carrierline.count"), 
                    skip = 1)
```

Now write the data to local storage as an ff data frame, using `laf_to_ffdf`.

```{r}
data.ffdf <- ffbase::laf_to_ffdf(laf = .dat)
head(data.ffdf)
```

We can verify that the `ffdf` data frame has a small RAM footprint.

```{r}
pryr::object_size(data.ffdf)
```

The __ffbase__ package provides several statistical tools to compute with `ff` class objects.
Here is simple table.

```{r ffdf table, cache=TRUE}
ffbase:::table.ff(data.ffdf$age) 
```

The EMA implementation of `biglm::biglm` and `biglm::bigglm` have their __ff__ versions.

```{r ffdf bigglm}
library(biglm)
mymodel.ffdf <- biglm(payment ~ factor(sex) + factor(age) + place.served, 
                              data = data.ffdf)
summary(mymodel.ffdf)
```

Things to note:

- `biglm::biglm` notices the input of of class `ffdf` and calls the appropriate implementation.
- The model formula, `payment ~ factor(sex) + factor(age) + place.served`, includes factors which cause no difficulty.
- You cannot inspect the factor coding (dummy? effect?) using `model.matrix`. 
This is because EMAs never really construct the whole matrix, let alone, save it in memory.






## Computing from a Distributed File System

If your data is SOOO big that it cannot fit on your local storage, you will need a distributed file system or DBMS.
We do not cover this topic here, and refer the reader to the __RHipe__, __RHadoop__, and __RSpark__ packages and references therein.



## Bibliographic Notes

An absolute SUPERB review on computing with big data is @wang2015statistical, and references therein (@kane2013scalable in particular). 
For an up-to-date list of the packages that deal with memory constraints, see the __Large memory and out-of-memory data__ section in the High Performance Computing [R task view](https://cran.r-project.org/web/views/HighPerformanceComputing.html).

## Practice Yourself

<!--chapter:end:14-memory.Rmd-->

# Parallel Computing {#parallel}

You would think that because you have an expensive multicore computer your computations will speed up. 
Well, no.
At least not if you don't make sure they do.
By default, no matter how many cores you have, the operating system will allocate each R session to a single core. 

For starters, we need to distinguish between two types of parallelism:

1. __Explicit parallelism__: where the user handles the parallelisation. 
1. __Implicit parallelism__: where the parallelisation is abstracted away from the user.

Clearly, implicit parallelism is more desirable, but the state of mathematical computing is such that no sufficiently general implicit parallelism framework exists. 
The [R Consortium](https://www.r-consortium.org/projects/awarded-projects) is currently financing a major project for a A "Unified Framework For Distributed Computing in R" so we can expect things to change soon. 
In the meanwhile, most of the parallel implementations are explicit. 



## Explicit Parallelism

R provides many frameworks for explicit parallelism.
Because the parallelism is initiated by the user, we first need to decide __when to parallelize?__
As a rule of thumb, you want to parallelise when you encounter a CPU bottleneck, and not a memory bottleneck.
Memory bottlenecks are released with sparsity (Chapter \@ref(sparse)), or efficient memory usage (Chapter \@ref(memory)).

Several ways to diagnose your bottleneck include:

- Keep your Windows Task Manager, or Linux `top` open, and look for the CPU load, and RAM loads. 
- The computation takes a long time, and when you stop it pressing ESC, R is immediately responsive. If it is not immediately responsive, you have a memory bottleneck.
- Profile your code. See Hadley's [guide](http://adv-r.had.co.nz/Profiling.html).

For reasons detailed in @kane2013scalable, we will present the __foreach__ parallelisation package [@foreach]. 
It will allow us to: 

1. Decouple between our parallel algorithm and the parallelisation mechanism: we write parallelisable code once, and can then switch the underlying parallelisation mechanism. 

1. Combine with the `big.matrix` object from Chapter \@ref(memory) for _shared memory parallisation_: all the machines may see the same data, so that we don't need to export objects from machine to machine. 


What do we mean by "switch the underlying parallesation mechanism"? 
It means there are several packages that will handle communication between machines. 
Some are very general and will work on any cluster. 
Some are more specific and will work only on a single multicore machine (not a cluster) with a particular operating system.
These mechanisms include __multicore__, __snow__, __parallel__, and __Rmpi__.
The compatibility between these mechanisms and __foreach__ is provided by another set of packages:
__doMC__ , __doMPI__, __doRedis__, __doParallel__, and __doSNOW__.


```{remark}
I personally prefer the __multicore__ mechanism, with the __doMC__ adapter for __foreach__.
I will not use this combo, however, because __multicore__ will not work on Windows machines.
I will thus use the more general __snow__ and __doParallel__ combo. 
If you do happen to run on Linux, or Unix, you will want to replace all __doParallel__ functionality with __doMC__.
```


Let's start with a simple example, taken from ["Getting Started with doParallel and foreach"](http://debian.mc.vanderbilt.edu/R/CRAN/web/packages/doParallel/vignettes/gettingstartedParallel.pdf).

```{r}
library(doParallel)
cl <- makeCluster(2)
registerDoParallel(cl)
result <- foreach(i=1:3) %dopar% sqrt(i)
class(result)
result
```

Things to note:

- `makeCluster` creates an object with the information our cluster. 
On a single machine it is very simple. On a cluster of machines, you will need to specify the i.p. addresses or other identifiers of the machines. 
- `registerDoParallel` is used to inform the __foreach__ package of the presence of our cluster. 
- The `foreach` function handles the looping. In particular note the `%dopar` operator that ensures that looping is in parallel. `%dopar%` can be replaced by `%do%` if you want serial looping (like the `for` loop), for instance, for debugging. 
- The output of the various machines is collected by `foreach` to a list object. 
- In this simple example, no data is shared between machines so we are not putting the shared memory capabilities to the test. 
- We can check how many workers were involved using the `getDoParWorkers()` function.
- We can check the parallelisation mechanism used with the `getDoParName()` function.


Here is a more involved example.
We now try to make [Bootstrap](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)) inference on the coefficients of a logistic regression.
Bootstrapping means that in each iteration, we resample the data, and refit the model. 

```{r}
x <- iris[which(iris[,5] != "setosa"), c(1,5)]
trials <- 1e4
ptime <- system.time({
 r <- foreach(icount(trials), .combine=cbind) %dopar% {
 ind <- sample(100, 100, replace=TRUE)
 result1 <- glm(x[ind,2]~x[ind,1], family=binomial(logit))
 coefficients(result1)
 }
 })[3]
ptime
```

Things to note:

- As usual, we use the `foreach` function with the `%dopar%` operator to loop in parallel.
- The `icounts` function generates a counter. 
- The `.combine=cbind` argument tells the `foreach` function how to combine the output of different machines, so that the returned object is not the default list. 


How long would that have taken in a simple (serial) loop? 
We only need to replace `%dopar%` with `%do%` to test.

```{r}
stime <- system.time({
 r <- foreach(icount(trials), .combine=cbind) %do% {
 ind <- sample(100, 100, replace=TRUE)
 result1 <- glm(x[ind,2]~x[ind,1], family=binomial(logit))
 coefficients(result1)
 }
 })[3]
stime
```

Yes. Parallelising is clearly faster.


Let's see how we can combine the power of __bigmemory__ and __foreach__ by creating a file mapped `big.matrix` object, which is shared by all machines.
The following example is taken from @kane2013scalable, and uses the `big.matrix` object we created in Chapter \@ref(memory).

```{r}
library(bigmemory)
x <- attach.big.matrix("airline.desc")

library(foreach)
library(doSNOW)
cl <- makeSOCKcluster(rep("localhost", 4)) # make a cluster of 4 machines
registerDoSNOW(cl) # register machines for foreach()
```

Get a "description" of the `big.matrix` object that will be used to call it from each machine.
```{r}
xdesc <- describe(x) 
```

Split the data along values of `BENE_AGE_CAT_CD`.
```{r}
G <- split(1:nrow(x), x[, "BENE_AGE_CAT_CD"]) 
```

Define a function that computes quantiles of `CAR_LINE_ICD9_DGNS_CD`.
```{r}
GetDepQuantiles <- function(rows, data) {
 quantile(data[rows, "CAR_LINE_ICD9_DGNS_CD"], probs = c(0.5, 0.9, 0.99),
 na.rm = TRUE)
}
```

We are all set up to loop, in parallel, and compute quantiles of `CAR_LINE_ICD9_DGNS_CD` for each value of `BENE_AGE_CAT_CD`.

```{r}
qs <- foreach(g = G, .combine = rbind) %dopar% {
 require("bigmemory")
 x <- attach.big.matrix(xdesc)
 GetDepQuantiles(rows = g, data = x)
}
qs
```







## Implicit Parallelism

We will not elaborate on implicit parallelism except mentioning the following:

- You can enjoy parallel linear algebra by replacing the linear algebra libraries with BLAS and LAPACK as described [here](https://www.r-bloggers.com/faster-r-through-better-blas/).
- You should read the "Parallel computing: Implicit parallelism" section in the excellent [High Performance Computing](https://cran.r-project.org/web/views/HighPerformanceComputing.html) task view, for the latest developments in implicit parallelism.






## Bibliographic Notes
For a brief and excellent explanation on parallel computing in R see @schmidberger2009state.
For a full review see @chapple2016mastering.
For an up-to-date list of packages supporting parallel programming see the High Performance Computing [R task view](https://cran.r-project.org/web/views/HighPerformanceComputing.html).

## Practice Yourself

<!--chapter:end:15-parallel.Rmd-->

# Numerical Linear Algebra {#algebra}

In your algebra courses you would write $Ax=b$ and solve $x=A^{-1}b$. 
This is useful to understand the algebraic properties of $x$, but a computer would never recover $x$ that way. 
Even the computation of the sample variance, $S^2(x)=(n-1)^{-1}\sum (x_i-\bar x)^2$ is not solved that way in a computer, because of numerical and speed considerations.

In this chapter, we discuss several ways a computer solves systems of linear equations, with their application to statistics, namely, to OLS problems.  


## LU Factorization

```{definition, label="lu", name="LU Factorization"}
For some matrix $A$, the LU factorization is defined as 
\begin{align}
 A = L U 
\end{align}
where L is unit lower triangular and U is upper triangular. 
```

The LU factorization is essentially the matrix notation for the [Gaussian elimination](https://en.wikipedia.org/wiki/Gaussian_elimination) you did in your introductory algebra courses. 

For a square $n \times n$ matrix, the LU factorization requires $n^3/3$ operations, and stores $n^2+n$ elements in memory. 



## Cholesky Factorization

```{definition, label="nonnegative", name="Non Negative Matrix"}
A matrix $A$ is said to be _non-negative_ if $x'Ax \geq 0$ for all $x$.
```

Seeing the matrix $A$ as a function, non-negative matrices can be thought of as functions that generalize the _squaring_ operation.



```{definition, label="cholesky", name="Cholesky Factorization"}
For some non-negative matrix $A$, the Cholesky factorization is defined as 
\begin{align}
 A = T'T 
\end{align}
where T is upper triangular with positive diagonal elements.
```

For obvious reasons, the Cholesky factorization is known as the _square root_ of a matrix. 

Because Cholesky is less general than LU, it is also more efficient.
It can be computed in $n^3/6$ operations, and requires storing $n(n+1)/2$ elements.


## QR Factorization

```{definition, label="qr", name="QR Factorization"}
For some matrix $A$, the QR factorization is defined as 
\begin{align}
 A = QR 
\end{align}
where Q is orthogonal and R is upper triangular. 
```

The QR factorization is very useful to solve the OLS problem as we will see in \@ref(solving-ols).
The QR factorization takes $2n^3/3$ operations to compute. 
Three major methods for computing the QR factorization exist. These rely on _Householder transformations_, _Givens transformations_, and a (modified) _Gram-Schmidt procedure_ [@gentle2012numerical].





## Singular Value Factorization

```{definition, label="svd", name="SVD"}
For an arbitrary $n\times m$ matrix  $A$, the _singular valued decomposition_ (SVD), is defined as \begin{align}
 A = U \Sigma V' 
\end{align}
where U is an orthonormal $n \times n$ matrix, V is an $m \times m$ orthonormal matrix, and $\Sigma$ is diagonal.
```

The SVD factorization is very useful for algebraic analysis, but less so for computations.
This is because it is (typically) solved via the QR factorization.



## Iterative Methods

The various matrix factorizations above may be used to solve a system of linear equations, and in particular, the OLS problem. 
There is, however, a very different approach to solving systems of linear equations. 
This approach relies on the fact that solutions of linear systems of equations, can be cast as optimization problems: simply find $x$ by minimizing  $\Vert Ax-b \Vert$.

Some methods for solving (convex) optimization problems are reviewed in the Convex Optimization Chapter \ref@(convex).
For our purposes we will just mention that historically (this means in the `lm` function, and in the LAPACK numerical libraries) the factorization approach was preferred, and now optimization approaches are preferred. 
This is because the optimization approach is more numerically stable, and easier to parallelize. 



## Solving the OLS Problem {#solving-ols}

Recalling the OLS problem in Eq.\@ref(eq:ols-matrix): we wish to find $\beta$ such that  
\begin{align}
  \hat \beta:= argmin_\beta \{ \Vert y-X\beta \Vert^2_2 \}.
\end{align}

The solution, $\hat \beta$ that solves this problem has to satisfy 
\begin{align}
  X'X \beta = X'y.
  (\#eq:normal-equations)
\end{align}
Eq.\@ref(eq:normal-equations) are known as the _normal equations_.
The normal equations are the link between the OLS problem, and the matrix factorization discussed above. 


Using the QR decomposition in the normal equations we have that 
\begin{align*}
  \hat \beta = R_{(1:p,1:p)}^{-1} y,
\end{align*}
where $(R_{n\times p})=(R_{(1:p,1:p)},0_{(p+1:n,1:p)})$ is the 









## Bibliographic Notes
For an excellent introduction to numerical algorithms in statistics, see @weihs2013foundations.
For an emphasis on numerical linear algebra, see @gentle2012numerical, and @golub2012matrix.

## Practice Yourself

<!--chapter:end:16-algebra.Rmd-->

# Convex Optimization {#convex}



## Bibliographic Notes

## Practice Yourself

<!--chapter:end:17-convex-optimization.Rmd-->

# RCpp {#rcpp}

## Bibliographic Notes

## Practice Yourself

<!--chapter:end:18-rcpp.Rmd-->

# Debugging Tools {#debugging}



## Bibliographic Notes

## Practice Yourself

<!--chapter:end:19-debugging.Rmd-->

# Bibliography {#bib}

<!--chapter:end:20-bibliography.Rmd-->

