<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title></title>
  <meta name="description" content="">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="lme.html">
<link rel="next" href="supervised.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.3/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.8.0/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-1.39.2/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-1.39.2/plotly-latest.min.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">R Course</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preface</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#notation-conventions"><i class="fa fa-check"></i><b>1.1</b> Notation Conventions</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i><b>1.2</b> Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a><ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#what-r"><i class="fa fa-check"></i><b>2.1</b> What is R?</a></li>
<li class="chapter" data-level="2.2" data-path="intro.html"><a href="intro.html#ecosystem"><i class="fa fa-check"></i><b>2.2</b> The R Ecosystem</a></li>
<li class="chapter" data-level="2.3" data-path="intro.html"><a href="intro.html#bibliographic-notes"><i class="fa fa-check"></i><b>2.3</b> Bibliographic Notes</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="basics.html"><a href="basics.html"><i class="fa fa-check"></i><b>3</b> R Basics</a><ul>
<li class="chapter" data-level="3.0.1" data-path="basics.html"><a href="basics.html#other-ides"><i class="fa fa-check"></i><b>3.0.1</b> Other IDEs</a></li>
<li class="chapter" data-level="3.1" data-path="basics.html"><a href="basics.html#file-types"><i class="fa fa-check"></i><b>3.1</b> File types</a></li>
<li class="chapter" data-level="3.2" data-path="basics.html"><a href="basics.html#simple-calculator"><i class="fa fa-check"></i><b>3.2</b> Simple calculator</a></li>
<li class="chapter" data-level="3.3" data-path="basics.html"><a href="basics.html#probability-calculator"><i class="fa fa-check"></i><b>3.3</b> Probability calculator</a></li>
<li class="chapter" data-level="3.4" data-path="basics.html"><a href="basics.html#getting-help"><i class="fa fa-check"></i><b>3.4</b> Getting Help</a></li>
<li class="chapter" data-level="3.5" data-path="basics.html"><a href="basics.html#variable-asignment"><i class="fa fa-check"></i><b>3.5</b> Variable Asignment</a></li>
<li class="chapter" data-level="3.6" data-path="basics.html"><a href="basics.html#missing"><i class="fa fa-check"></i><b>3.6</b> Missing</a></li>
<li class="chapter" data-level="3.7" data-path="basics.html"><a href="basics.html#piping"><i class="fa fa-check"></i><b>3.7</b> Piping</a></li>
<li class="chapter" data-level="3.8" data-path="basics.html"><a href="basics.html#vector-creation-and-manipulation"><i class="fa fa-check"></i><b>3.8</b> Vector Creation and Manipulation</a></li>
<li class="chapter" data-level="3.9" data-path="basics.html"><a href="basics.html#search-paths-and-packages"><i class="fa fa-check"></i><b>3.9</b> Search Paths and Packages</a></li>
<li class="chapter" data-level="3.10" data-path="basics.html"><a href="basics.html#simple-plotting"><i class="fa fa-check"></i><b>3.10</b> Simple Plotting</a></li>
<li class="chapter" data-level="3.11" data-path="basics.html"><a href="basics.html#object-types"><i class="fa fa-check"></i><b>3.11</b> Object Types</a></li>
<li class="chapter" data-level="3.12" data-path="basics.html"><a href="basics.html#data-frames"><i class="fa fa-check"></i><b>3.12</b> Data Frames</a></li>
<li class="chapter" data-level="3.13" data-path="basics.html"><a href="basics.html#exctraction"><i class="fa fa-check"></i><b>3.13</b> Exctraction</a></li>
<li class="chapter" data-level="3.14" data-path="basics.html"><a href="basics.html#augmentations-of-the-data.frame-class"><i class="fa fa-check"></i><b>3.14</b> Augmentations of the data.frame class</a></li>
<li class="chapter" data-level="3.15" data-path="basics.html"><a href="basics.html#data-import-and-export"><i class="fa fa-check"></i><b>3.15</b> Data Import and Export</a><ul>
<li class="chapter" data-level="3.15.1" data-path="basics.html"><a href="basics.html#import-from-web"><i class="fa fa-check"></i><b>3.15.1</b> Import from WEB</a></li>
<li class="chapter" data-level="3.15.2" data-path="basics.html"><a href="basics.html#export-as-csv"><i class="fa fa-check"></i><b>3.15.2</b> Export as CSV</a></li>
<li class="chapter" data-level="3.15.3" data-path="basics.html"><a href="basics.html#export-non-csv-files"><i class="fa fa-check"></i><b>3.15.3</b> Export non-CSV files</a></li>
<li class="chapter" data-level="3.15.4" data-path="basics.html"><a href="basics.html#reading-from-text-files"><i class="fa fa-check"></i><b>3.15.4</b> Reading From Text Files</a></li>
<li class="chapter" data-level="3.15.5" data-path="basics.html"><a href="basics.html#writing-data-to-text-files"><i class="fa fa-check"></i><b>3.15.5</b> Writing Data to Text Files</a></li>
<li class="chapter" data-level="3.15.6" data-path="basics.html"><a href="basics.html#xlsx-files"><i class="fa fa-check"></i><b>3.15.6</b> .XLS(X) files</a></li>
<li class="chapter" data-level="3.15.7" data-path="basics.html"><a href="basics.html#massive-files"><i class="fa fa-check"></i><b>3.15.7</b> Massive files</a></li>
<li class="chapter" data-level="3.15.8" data-path="basics.html"><a href="basics.html#databases"><i class="fa fa-check"></i><b>3.15.8</b> Databases</a></li>
</ul></li>
<li class="chapter" data-level="3.16" data-path="basics.html"><a href="basics.html#functions"><i class="fa fa-check"></i><b>3.16</b> Functions</a></li>
<li class="chapter" data-level="3.17" data-path="basics.html"><a href="basics.html#looping"><i class="fa fa-check"></i><b>3.17</b> Looping</a></li>
<li class="chapter" data-level="3.18" data-path="basics.html"><a href="basics.html#apply"><i class="fa fa-check"></i><b>3.18</b> Apply</a></li>
<li class="chapter" data-level="3.19" data-path="basics.html"><a href="basics.html#recursion"><i class="fa fa-check"></i><b>3.19</b> Recursion</a></li>
<li class="chapter" data-level="3.20" data-path="basics.html"><a href="basics.html#dates-and-times"><i class="fa fa-check"></i><b>3.20</b> Dates and Times</a></li>
<li class="chapter" data-level="3.21" data-path="basics.html"><a href="basics.html#bibliographic-notes-1"><i class="fa fa-check"></i><b>3.21</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="3.22" data-path="basics.html"><a href="basics.html#practice-yourself"><i class="fa fa-check"></i><b>3.22</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="datatable.html"><a href="datatable.html"><i class="fa fa-check"></i><b>4</b> data.table</a><ul>
<li class="chapter" data-level="4.1" data-path="datatable.html"><a href="datatable.html#make-your-own-variables"><i class="fa fa-check"></i><b>4.1</b> Make your own variables</a></li>
<li class="chapter" data-level="4.2" data-path="datatable.html"><a href="datatable.html#join"><i class="fa fa-check"></i><b>4.2</b> Join</a></li>
<li class="chapter" data-level="4.3" data-path="datatable.html"><a href="datatable.html#reshaping-data"><i class="fa fa-check"></i><b>4.3</b> Reshaping data</a><ul>
<li class="chapter" data-level="4.3.1" data-path="datatable.html"><a href="datatable.html#wide-to-long"><i class="fa fa-check"></i><b>4.3.1</b> Wide to long</a></li>
<li class="chapter" data-level="4.3.2" data-path="datatable.html"><a href="datatable.html#long-to-wide"><i class="fa fa-check"></i><b>4.3.2</b> Long to wide</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="datatable.html"><a href="datatable.html#bibliographic-notes-2"><i class="fa fa-check"></i><b>4.4</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="4.5" data-path="datatable.html"><a href="datatable.html#practice-yourself-1"><i class="fa fa-check"></i><b>4.5</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="eda.html"><a href="eda.html"><i class="fa fa-check"></i><b>5</b> Exploratory Data Analysis</a><ul>
<li class="chapter" data-level="5.1" data-path="eda.html"><a href="eda.html#summary-statistics"><i class="fa fa-check"></i><b>5.1</b> Summary Statistics</a><ul>
<li class="chapter" data-level="5.1.1" data-path="eda.html"><a href="eda.html#categorical-data"><i class="fa fa-check"></i><b>5.1.1</b> Categorical Data</a></li>
<li class="chapter" data-level="5.1.2" data-path="eda.html"><a href="eda.html#continous-data"><i class="fa fa-check"></i><b>5.1.2</b> Continous Data</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="eda.html"><a href="eda.html#visualization"><i class="fa fa-check"></i><b>5.2</b> Visualization</a><ul>
<li class="chapter" data-level="5.2.1" data-path="eda.html"><a href="eda.html#categorical-data-1"><i class="fa fa-check"></i><b>5.2.1</b> Categorical Data</a></li>
<li class="chapter" data-level="5.2.2" data-path="eda.html"><a href="eda.html#continuous-data"><i class="fa fa-check"></i><b>5.2.2</b> Continuous Data</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="eda.html"><a href="eda.html#mixed-type-data"><i class="fa fa-check"></i><b>5.3</b> Mixed Type Data</a><ul>
<li class="chapter" data-level="5.3.1" data-path="eda.html"><a href="eda.html#alluvian-diagram"><i class="fa fa-check"></i><b>5.3.1</b> Alluvian Diagram</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="eda.html"><a href="eda.html#bibliographic-notes-3"><i class="fa fa-check"></i><b>5.4</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="5.5" data-path="eda.html"><a href="eda.html#practice-yourself-2"><i class="fa fa-check"></i><b>5.5</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="lm.html"><a href="lm.html"><i class="fa fa-check"></i><b>6</b> Linear Models</a><ul>
<li class="chapter" data-level="6.1" data-path="lm.html"><a href="lm.html#problem-setup"><i class="fa fa-check"></i><b>6.1</b> Problem Setup</a></li>
<li class="chapter" data-level="6.2" data-path="lm.html"><a href="lm.html#ols-estimation-in-r"><i class="fa fa-check"></i><b>6.2</b> OLS Estimation in R</a></li>
<li class="chapter" data-level="6.3" data-path="lm.html"><a href="lm.html#inference"><i class="fa fa-check"></i><b>6.3</b> Inference</a><ul>
<li class="chapter" data-level="6.3.1" data-path="lm.html"><a href="lm.html#testing-a-hypothesis-on-a-single-coefficient"><i class="fa fa-check"></i><b>6.3.1</b> Testing a Hypothesis on a Single Coefficient</a></li>
<li class="chapter" data-level="6.3.2" data-path="lm.html"><a href="lm.html#constructing-a-confidence-interval-on-a-single-coefficient"><i class="fa fa-check"></i><b>6.3.2</b> Constructing a Confidence Interval on a Single Coefficient</a></li>
<li class="chapter" data-level="6.3.3" data-path="lm.html"><a href="lm.html#multiple-regression"><i class="fa fa-check"></i><b>6.3.3</b> Multiple Regression</a></li>
<li class="chapter" data-level="6.3.4" data-path="lm.html"><a href="lm.html#anova"><i class="fa fa-check"></i><b>6.3.4</b> ANOVA (*)</a></li>
<li class="chapter" data-level="6.3.5" data-path="lm.html"><a href="lm.html#testing-a-hypothesis-on-a-single-contrast"><i class="fa fa-check"></i><b>6.3.5</b> Testing a Hypothesis on a Single Contrast (*)</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="lm.html"><a href="lm.html#bibliographic-notes-4"><i class="fa fa-check"></i><b>6.4</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="6.5" data-path="lm.html"><a href="lm.html#practice-yourself-3"><i class="fa fa-check"></i><b>6.5</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="glm.html"><a href="glm.html"><i class="fa fa-check"></i><b>7</b> Generalized Linear Models</a><ul>
<li class="chapter" data-level="7.1" data-path="glm.html"><a href="glm.html#problem-setup-1"><i class="fa fa-check"></i><b>7.1</b> Problem Setup</a></li>
<li class="chapter" data-level="7.2" data-path="glm.html"><a href="glm.html#logistic-regression"><i class="fa fa-check"></i><b>7.2</b> Logistic Regression</a><ul>
<li class="chapter" data-level="7.2.1" data-path="glm.html"><a href="glm.html#logistic-regression-with-r"><i class="fa fa-check"></i><b>7.2.1</b> Logistic Regression with R</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="glm.html"><a href="glm.html#poisson-regression"><i class="fa fa-check"></i><b>7.3</b> Poisson Regression</a></li>
<li class="chapter" data-level="7.4" data-path="glm.html"><a href="glm.html#extensions"><i class="fa fa-check"></i><b>7.4</b> Extensions</a></li>
<li class="chapter" data-level="7.5" data-path="glm.html"><a href="glm.html#bibliographic-notes-5"><i class="fa fa-check"></i><b>7.5</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="7.6" data-path="glm.html"><a href="glm.html#practice-glm"><i class="fa fa-check"></i><b>7.6</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="lme.html"><a href="lme.html"><i class="fa fa-check"></i><b>8</b> Linear Mixed Models</a><ul>
<li class="chapter" data-level="8.1" data-path="lme.html"><a href="lme.html#problem-setup-2"><i class="fa fa-check"></i><b>8.1</b> Problem Setup</a><ul>
<li class="chapter" data-level="8.1.1" data-path="lme.html"><a href="lme.html#non-linear-mixed-models"><i class="fa fa-check"></i><b>8.1.1</b> Non-Linear Mixed Models</a></li>
<li class="chapter" data-level="8.1.2" data-path="lme.html"><a href="lme.html#generalized-linear-mixed-models-glmm"><i class="fa fa-check"></i><b>8.1.2</b> Generalized Linear Mixed Models (GLMM)</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="lme.html"><a href="lme.html#mixed-models-with-r"><i class="fa fa-check"></i><b>8.2</b> Mixed Models with R</a><ul>
<li class="chapter" data-level="8.2.1" data-path="lme.html"><a href="lme.html#a-single-random-effect"><i class="fa fa-check"></i><b>8.2.1</b> A Single Random Effect</a></li>
<li class="chapter" data-level="8.2.2" data-path="lme.html"><a href="lme.html#multiple-random-effects"><i class="fa fa-check"></i><b>8.2.2</b> Multiple Random Effects</a></li>
<li class="chapter" data-level="8.2.3" data-path="lme.html"><a href="lme.html#a-full-mixed-model"><i class="fa fa-check"></i><b>8.2.3</b> A Full Mixed-Model</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="lme.html"><a href="lme.html#serial"><i class="fa fa-check"></i><b>8.3</b> Serial Correlations</a></li>
<li class="chapter" data-level="8.4" data-path="lme.html"><a href="lme.html#extensions-1"><i class="fa fa-check"></i><b>8.4</b> Extensions</a><ul>
<li class="chapter" data-level="8.4.1" data-path="lme.html"><a href="lme.html#cluster-robust-standard-errors"><i class="fa fa-check"></i><b>8.4.1</b> Cluster Robust Standard Errors</a></li>
<li class="chapter" data-level="8.4.2" data-path="lme.html"><a href="lme.html#linear-models-for-panel-data"><i class="fa fa-check"></i><b>8.4.2</b> Linear Models for Panel Data</a></li>
<li class="chapter" data-level="8.4.3" data-path="lme.html"><a href="lme.html#testing-hypotheses-on-correlations"><i class="fa fa-check"></i><b>8.4.3</b> Testing Hypotheses on Correlations</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="lme.html"><a href="lme.html#relation-to-other-estimators"><i class="fa fa-check"></i><b>8.5</b> Relation to Other Estimators</a><ul>
<li class="chapter" data-level="8.5.1" data-path="lme.html"><a href="lme.html#fixed-effects-in-the-econometric-literature"><i class="fa fa-check"></i><b>8.5.1</b> Fixed Effects in the Econometric Literature</a></li>
<li class="chapter" data-level="8.5.2" data-path="lme.html"><a href="lme.html#relation-to-generalized-least-squares-gls"><i class="fa fa-check"></i><b>8.5.2</b> Relation to Generalized Least Squares (GLS)</a></li>
<li class="chapter" data-level="8.5.3" data-path="lme.html"><a href="lme.html#relation-to-conditional-gaussian-fields"><i class="fa fa-check"></i><b>8.5.3</b> Relation to Conditional Gaussian Fields</a></li>
<li class="chapter" data-level="8.5.4" data-path="lme.html"><a href="lme.html#relation-to-empirical-risk-minimization-erm"><i class="fa fa-check"></i><b>8.5.4</b> Relation to Empirical Risk Minimization (ERM)</a></li>
<li class="chapter" data-level="8.5.5" data-path="lme.html"><a href="lme.html#relation-to-m-estimation"><i class="fa fa-check"></i><b>8.5.5</b> Relation to M-Estimation</a></li>
<li class="chapter" data-level="8.5.6" data-path="lme.html"><a href="lme.html#relation-to-generalize-estimating-equations-gee"><i class="fa fa-check"></i><b>8.5.6</b> Relation to Generalize Estimating Equations (GEE)</a></li>
<li class="chapter" data-level="8.5.7" data-path="lme.html"><a href="lme.html#manova"><i class="fa fa-check"></i><b>8.5.7</b> Relation to MANOVA</a></li>
<li class="chapter" data-level="8.5.8" data-path="lme.html"><a href="lme.html#relation-to-seemingly-unrelated-equations-sur"><i class="fa fa-check"></i><b>8.5.8</b> Relation to Seemingly Unrelated Equations (SUR)</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="lme.html"><a href="lme.html#the-variance-components-view"><i class="fa fa-check"></i><b>8.6</b> The Variance-Components View</a></li>
<li class="chapter" data-level="8.7" data-path="lme.html"><a href="lme.html#bibliographic-notes-6"><i class="fa fa-check"></i><b>8.7</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="8.8" data-path="lme.html"><a href="lme.html#practice-yourself-4"><i class="fa fa-check"></i><b>8.8</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="multivariate.html"><a href="multivariate.html"><i class="fa fa-check"></i><b>9</b> Multivariate Data Analysis</a><ul>
<li class="chapter" data-level="9.1" data-path="multivariate.html"><a href="multivariate.html#signal-detection"><i class="fa fa-check"></i><b>9.1</b> Signal Detection</a><ul>
<li class="chapter" data-level="9.1.1" data-path="multivariate.html"><a href="multivariate.html#hotellings-t2-test"><i class="fa fa-check"></i><b>9.1.1</b> Hotelling’s T2 Test</a></li>
<li class="chapter" data-level="9.1.2" data-path="multivariate.html"><a href="multivariate.html#various-types-of-signal-to-detect"><i class="fa fa-check"></i><b>9.1.2</b> Various Types of Signal to Detect</a></li>
<li class="chapter" data-level="9.1.3" data-path="multivariate.html"><a href="multivariate.html#simes-test"><i class="fa fa-check"></i><b>9.1.3</b> Simes’ Test</a></li>
<li class="chapter" data-level="9.1.4" data-path="multivariate.html"><a href="multivariate.html#signal-detection-with-r"><i class="fa fa-check"></i><b>9.1.4</b> Signal Detection with R</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="multivariate.html"><a href="multivariate.html#signal-counting"><i class="fa fa-check"></i><b>9.2</b> Signal Counting</a></li>
<li class="chapter" data-level="9.3" data-path="multivariate.html"><a href="multivariate.html#identification"><i class="fa fa-check"></i><b>9.3</b> Signal Identification</a><ul>
<li class="chapter" data-level="9.3.1" data-path="multivariate.html"><a href="multivariate.html#signal-identification-in-r"><i class="fa fa-check"></i><b>9.3.1</b> Signal Identification in R</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="multivariate.html"><a href="multivariate.html#signal-estimation"><i class="fa fa-check"></i><b>9.4</b> Signal Estimation (*)</a></li>
<li class="chapter" data-level="9.5" data-path="multivariate.html"><a href="multivariate.html#multivariate-regression"><i class="fa fa-check"></i><b>9.5</b> Multivariate Regression (*)</a><ul>
<li class="chapter" data-level="9.5.1" data-path="multivariate.html"><a href="multivariate.html#multivariate-regression-with-r"><i class="fa fa-check"></i><b>9.5.1</b> Multivariate Regression with R</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="multivariate.html"><a href="multivariate.html#graphical-models"><i class="fa fa-check"></i><b>9.6</b> Graphical Models (*)</a><ul>
<li class="chapter" data-level="9.6.1" data-path="multivariate.html"><a href="multivariate.html#graphical-models-in-r"><i class="fa fa-check"></i><b>9.6.1</b> Graphical Models in R</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="multivariate.html"><a href="multivariate.html#biblipgraphic-notes"><i class="fa fa-check"></i><b>9.7</b> Biblipgraphic Notes</a></li>
<li class="chapter" data-level="9.8" data-path="multivariate.html"><a href="multivariate.html#practice-yourself-5"><i class="fa fa-check"></i><b>9.8</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="supervised.html"><a href="supervised.html"><i class="fa fa-check"></i><b>10</b> Supervised Learning</a><ul>
<li class="chapter" data-level="10.1" data-path="supervised.html"><a href="supervised.html#problem-setup-3"><i class="fa fa-check"></i><b>10.1</b> Problem Setup</a><ul>
<li class="chapter" data-level="10.1.1" data-path="supervised.html"><a href="supervised.html#common-hypothesis-classes"><i class="fa fa-check"></i><b>10.1.1</b> Common Hypothesis Classes</a></li>
<li class="chapter" data-level="10.1.2" data-path="supervised.html"><a href="supervised.html#common-complexity-penalties"><i class="fa fa-check"></i><b>10.1.2</b> Common Complexity Penalties</a></li>
<li class="chapter" data-level="10.1.3" data-path="supervised.html"><a href="supervised.html#unbiased-risk-estimation"><i class="fa fa-check"></i><b>10.1.3</b> Unbiased Risk Estimation</a></li>
<li class="chapter" data-level="10.1.4" data-path="supervised.html"><a href="supervised.html#collecting-the-pieces"><i class="fa fa-check"></i><b>10.1.4</b> Collecting the Pieces</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="supervised.html"><a href="supervised.html#supervised-learning-in-r"><i class="fa fa-check"></i><b>10.2</b> Supervised Learning in R</a><ul>
<li class="chapter" data-level="10.2.1" data-path="supervised.html"><a href="supervised.html#least-squares"><i class="fa fa-check"></i><b>10.2.1</b> Linear Models with Least Squares Loss</a></li>
<li class="chapter" data-level="10.2.2" data-path="supervised.html"><a href="supervised.html#svm"><i class="fa fa-check"></i><b>10.2.2</b> SVM</a></li>
<li class="chapter" data-level="10.2.3" data-path="supervised.html"><a href="supervised.html#neural-nets"><i class="fa fa-check"></i><b>10.2.3</b> Neural Nets</a></li>
<li class="chapter" data-level="10.2.4" data-path="supervised.html"><a href="supervised.html#classification-and-regression-trees-cart"><i class="fa fa-check"></i><b>10.2.4</b> Classification and Regression Trees (CART)</a></li>
<li class="chapter" data-level="10.2.5" data-path="supervised.html"><a href="supervised.html#k-nearest-neighbour-knn"><i class="fa fa-check"></i><b>10.2.5</b> K-nearest neighbour (KNN)</a></li>
<li class="chapter" data-level="10.2.6" data-path="supervised.html"><a href="supervised.html#linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>10.2.6</b> Linear Discriminant Analysis (LDA)</a></li>
<li class="chapter" data-level="10.2.7" data-path="supervised.html"><a href="supervised.html#naive-bayes"><i class="fa fa-check"></i><b>10.2.7</b> Naive Bayes</a></li>
<li class="chapter" data-level="10.2.8" data-path="supervised.html"><a href="supervised.html#random-forrest"><i class="fa fa-check"></i><b>10.2.8</b> Random Forrest</a></li>
<li class="chapter" data-level="10.2.9" data-path="supervised.html"><a href="supervised.html#gradient-boosting"><i class="fa fa-check"></i><b>10.2.9</b> Gradient Boosting</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="supervised.html"><a href="supervised.html#bibliographic-notes-7"><i class="fa fa-check"></i><b>10.3</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="10.4" data-path="supervised.html"><a href="supervised.html#practice-yourself-6"><i class="fa fa-check"></i><b>10.4</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="unsupervised.html"><a href="unsupervised.html"><i class="fa fa-check"></i><b>11</b> Unsupervised Learning</a><ul>
<li class="chapter" data-level="11.1" data-path="unsupervised.html"><a href="unsupervised.html#dim-reduce"><i class="fa fa-check"></i><b>11.1</b> Dimensionality Reduction</a><ul>
<li class="chapter" data-level="11.1.1" data-path="unsupervised.html"><a href="unsupervised.html#pca"><i class="fa fa-check"></i><b>11.1.1</b> Principal Component Analysis</a></li>
<li class="chapter" data-level="11.1.2" data-path="unsupervised.html"><a href="unsupervised.html#dimensionality-reduction-preliminaries"><i class="fa fa-check"></i><b>11.1.2</b> Dimensionality Reduction Preliminaries</a></li>
<li class="chapter" data-level="11.1.3" data-path="unsupervised.html"><a href="unsupervised.html#latent-variable-generative-approaches"><i class="fa fa-check"></i><b>11.1.3</b> Latent Variable Generative Approaches</a></li>
<li class="chapter" data-level="11.1.4" data-path="unsupervised.html"><a href="unsupervised.html#purely-algorithmic-approaches"><i class="fa fa-check"></i><b>11.1.4</b> Purely Algorithmic Approaches</a></li>
<li class="chapter" data-level="11.1.5" data-path="unsupervised.html"><a href="unsupervised.html#dimensionality-reduction-in-r"><i class="fa fa-check"></i><b>11.1.5</b> Dimensionality Reduction in R</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="unsupervised.html"><a href="unsupervised.html#cluster"><i class="fa fa-check"></i><b>11.2</b> Clustering</a><ul>
<li class="chapter" data-level="11.2.1" data-path="unsupervised.html"><a href="unsupervised.html#latent-variable-generative-approaches-1"><i class="fa fa-check"></i><b>11.2.1</b> Latent Variable Generative Approaches</a></li>
<li class="chapter" data-level="11.2.2" data-path="unsupervised.html"><a href="unsupervised.html#purely-algorithmic-approaches-1"><i class="fa fa-check"></i><b>11.2.2</b> Purely Algorithmic Approaches</a></li>
<li class="chapter" data-level="11.2.3" data-path="unsupervised.html"><a href="unsupervised.html#clustering-in-r"><i class="fa fa-check"></i><b>11.2.3</b> Clustering in R</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="unsupervised.html"><a href="unsupervised.html#bibliographic-notes-8"><i class="fa fa-check"></i><b>11.3</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="11.4" data-path="unsupervised.html"><a href="unsupervised.html#practice-yourself-7"><i class="fa fa-check"></i><b>11.4</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="plotting.html"><a href="plotting.html"><i class="fa fa-check"></i><b>12</b> Plotting</a><ul>
<li class="chapter" data-level="12.1" data-path="plotting.html"><a href="plotting.html#the-graphics-system"><i class="fa fa-check"></i><b>12.1</b> The graphics System</a><ul>
<li class="chapter" data-level="12.1.1" data-path="plotting.html"><a href="plotting.html#using-existing-plotting-functions"><i class="fa fa-check"></i><b>12.1.1</b> Using Existing Plotting Functions</a></li>
<li class="chapter" data-level="12.1.2" data-path="plotting.html"><a href="plotting.html#exporting-a-plot"><i class="fa fa-check"></i><b>12.1.2</b> Exporting a Plot</a></li>
<li class="chapter" data-level="12.1.3" data-path="plotting.html"><a href="plotting.html#fancy"><i class="fa fa-check"></i><b>12.1.3</b> Fancy graphics Examples</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="plotting.html"><a href="plotting.html#the-ggplot2-system"><i class="fa fa-check"></i><b>12.2</b> The ggplot2 System</a><ul>
<li class="chapter" data-level="12.2.1" data-path="plotting.html"><a href="plotting.html#extensions-of-the-ggplot2-system"><i class="fa fa-check"></i><b>12.2.1</b> Extensions of the ggplot2 System</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="plotting.html"><a href="plotting.html#interactive-graphics"><i class="fa fa-check"></i><b>12.3</b> Interactive Graphics</a><ul>
<li class="chapter" data-level="12.3.1" data-path="plotting.html"><a href="plotting.html#plotly"><i class="fa fa-check"></i><b>12.3.1</b> Plotly</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="plotting.html"><a href="plotting.html#r2d3"><i class="fa fa-check"></i><b>12.4</b> r2d3</a></li>
<li class="chapter" data-level="12.5" data-path="plotting.html"><a href="plotting.html#bibliographic-notes-9"><i class="fa fa-check"></i><b>12.5</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="12.6" data-path="plotting.html"><a href="plotting.html#practice-yourself-8"><i class="fa fa-check"></i><b>12.6</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="report.html"><a href="report.html"><i class="fa fa-check"></i><b>13</b> Reports</a><ul>
<li class="chapter" data-level="13.1" data-path="report.html"><a href="report.html#knitr"><i class="fa fa-check"></i><b>13.1</b> knitr</a><ul>
<li class="chapter" data-level="13.1.1" data-path="report.html"><a href="report.html#installation"><i class="fa fa-check"></i><b>13.1.1</b> Installation</a></li>
<li class="chapter" data-level="13.1.2" data-path="report.html"><a href="report.html#pandoc-markdown"><i class="fa fa-check"></i><b>13.1.2</b> Pandoc Markdown</a></li>
<li class="chapter" data-level="13.1.3" data-path="report.html"><a href="report.html#rmarkdown"><i class="fa fa-check"></i><b>13.1.3</b> Rmarkdown</a></li>
<li class="chapter" data-level="13.1.4" data-path="report.html"><a href="report.html#bibtex"><i class="fa fa-check"></i><b>13.1.4</b> BibTex</a></li>
<li class="chapter" data-level="13.1.5" data-path="report.html"><a href="report.html#compiling"><i class="fa fa-check"></i><b>13.1.5</b> Compiling</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="report.html"><a href="report.html#bookdown"><i class="fa fa-check"></i><b>13.2</b> bookdown</a></li>
<li class="chapter" data-level="13.3" data-path="report.html"><a href="report.html#shiny"><i class="fa fa-check"></i><b>13.3</b> Shiny</a><ul>
<li class="chapter" data-level="13.3.1" data-path="report.html"><a href="report.html#installation-1"><i class="fa fa-check"></i><b>13.3.1</b> Installation</a></li>
<li class="chapter" data-level="13.3.2" data-path="report.html"><a href="report.html#the-basics-of-shiny"><i class="fa fa-check"></i><b>13.3.2</b> The Basics of Shiny</a></li>
<li class="chapter" data-level="13.3.3" data-path="report.html"><a href="report.html#beyond-the-basics"><i class="fa fa-check"></i><b>13.3.3</b> Beyond the Basics</a></li>
<li class="chapter" data-level="13.3.4" data-path="report.html"><a href="report.html#shinydashboard"><i class="fa fa-check"></i><b>13.3.4</b> shinydashboard</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="report.html"><a href="report.html#flexdashboard"><i class="fa fa-check"></i><b>13.4</b> flexdashboard</a></li>
<li class="chapter" data-level="13.5" data-path="report.html"><a href="report.html#bibliographic-notes-10"><i class="fa fa-check"></i><b>13.5</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="13.6" data-path="report.html"><a href="report.html#practice-yourself-9"><i class="fa fa-check"></i><b>13.6</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="sparse.html"><a href="sparse.html"><i class="fa fa-check"></i><b>14</b> Sparse Representations</a><ul>
<li class="chapter" data-level="14.1" data-path="sparse.html"><a href="sparse.html#sparse-matrix-representations"><i class="fa fa-check"></i><b>14.1</b> Sparse Matrix Representations</a><ul>
<li class="chapter" data-level="14.1.1" data-path="sparse.html"><a href="sparse.html#coo"><i class="fa fa-check"></i><b>14.1.1</b> Coordinate List Representation</a></li>
<li class="chapter" data-level="14.1.2" data-path="sparse.html"><a href="sparse.html#compressed-column-oriented-representation"><i class="fa fa-check"></i><b>14.1.2</b> Compressed Column Oriented Representation</a></li>
<li class="chapter" data-level="14.1.3" data-path="sparse.html"><a href="sparse.html#compressed-row-oriented-representation"><i class="fa fa-check"></i><b>14.1.3</b> Compressed Row Oriented Representation</a></li>
<li class="chapter" data-level="14.1.4" data-path="sparse.html"><a href="sparse.html#sparse-algorithms"><i class="fa fa-check"></i><b>14.1.4</b> Sparse Algorithms</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="sparse.html"><a href="sparse.html#sparse-matrices-and-sparse-models-in-r"><i class="fa fa-check"></i><b>14.2</b> Sparse Matrices and Sparse Models in R</a><ul>
<li class="chapter" data-level="14.2.1" data-path="sparse.html"><a href="sparse.html#the-matrix-package"><i class="fa fa-check"></i><b>14.2.1</b> The Matrix Package</a></li>
<li class="chapter" data-level="14.2.2" data-path="sparse.html"><a href="sparse.html#the-matrixmodels-package"><i class="fa fa-check"></i><b>14.2.2</b> The MatrixModels Package</a></li>
<li class="chapter" data-level="14.2.3" data-path="sparse.html"><a href="sparse.html#the-glmnet-package"><i class="fa fa-check"></i><b>14.2.3</b> The glmnet Package</a></li>
<li class="chapter" data-level="14.2.4" data-path="sparse.html"><a href="sparse.html#the-sparsem-package"><i class="fa fa-check"></i><b>14.2.4</b> The SparseM Package</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="sparse.html"><a href="sparse.html#bibliographic-notes-11"><i class="fa fa-check"></i><b>14.3</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="14.4" data-path="sparse.html"><a href="sparse.html#practice-yourself-10"><i class="fa fa-check"></i><b>14.4</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="memory.html"><a href="memory.html"><i class="fa fa-check"></i><b>15</b> Memory Efficiency</a><ul>
<li class="chapter" data-level="15.1" data-path="memory.html"><a href="memory.html#efficient-computing-from-ram"><i class="fa fa-check"></i><b>15.1</b> Efficient Computing from RAM</a><ul>
<li class="chapter" data-level="15.1.1" data-path="memory.html"><a href="memory.html#summary-statistics-from-ram"><i class="fa fa-check"></i><b>15.1.1</b> Summary Statistics from RAM</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="memory.html"><a href="memory.html#computing-from-a-database"><i class="fa fa-check"></i><b>15.2</b> Computing from a Database</a></li>
<li class="chapter" data-level="15.3" data-path="memory.html"><a href="memory.html#file-structure"><i class="fa fa-check"></i><b>15.3</b> Computing From Efficient File Structrures</a><ul>
<li class="chapter" data-level="15.3.1" data-path="memory.html"><a href="memory.html#bigmemory"><i class="fa fa-check"></i><b>15.3.1</b> bigmemory</a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="memory.html"><a href="memory.html#ff"><i class="fa fa-check"></i><b>15.4</b> ff</a></li>
<li class="chapter" data-level="15.5" data-path="memory.html"><a href="memory.html#matter"><i class="fa fa-check"></i><b>15.5</b> matter</a></li>
<li class="chapter" data-level="15.6" data-path="memory.html"><a href="memory.html#iotools"><i class="fa fa-check"></i><b>15.6</b> iotools</a></li>
<li class="chapter" data-level="15.7" data-path="memory.html"><a href="memory.html#hdf5"><i class="fa fa-check"></i><b>15.7</b> HDF5</a></li>
<li class="chapter" data-level="15.8" data-path="memory.html"><a href="memory.html#delayedarray"><i class="fa fa-check"></i><b>15.8</b> DelayedArray</a><ul>
<li class="chapter" data-level="15.8.1" data-path="memory.html"><a href="memory.html#delayedmatrixstats"><i class="fa fa-check"></i><b>15.8.1</b> DelayedMatrixStats</a></li>
<li class="chapter" data-level="15.8.2" data-path="memory.html"><a href="memory.html#beachmat"><i class="fa fa-check"></i><b>15.8.2</b> beachmat</a></li>
<li class="chapter" data-level="15.8.3" data-path="memory.html"><a href="memory.html#restfulse"><i class="fa fa-check"></i><b>15.8.3</b> restfulSE</a></li>
</ul></li>
<li class="chapter" data-level="15.9" data-path="memory.html"><a href="memory.html#computing-from-a-distributed-file-system"><i class="fa fa-check"></i><b>15.9</b> Computing from a Distributed File System</a></li>
<li class="chapter" data-level="15.10" data-path="memory.html"><a href="memory.html#bibliographic-notes-12"><i class="fa fa-check"></i><b>15.10</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="15.11" data-path="memory.html"><a href="memory.html#practice-yourself-11"><i class="fa fa-check"></i><b>15.11</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="parallel.html"><a href="parallel.html"><i class="fa fa-check"></i><b>16</b> Parallel Computing</a><ul>
<li class="chapter" data-level="16.1" data-path="parallel.html"><a href="parallel.html#implicit-parallelism"><i class="fa fa-check"></i><b>16.1</b> Implicit Parallelism</a></li>
<li class="chapter" data-level="16.2" data-path="parallel.html"><a href="parallel.html#explicit-parallelism"><i class="fa fa-check"></i><b>16.2</b> Explicit Parallelism</a><ul>
<li class="chapter" data-level="16.2.1" data-path="parallel.html"><a href="parallel.html#caution-implicit-with-explicit-parallelism"><i class="fa fa-check"></i><b>16.2.1</b> Caution: Implicit with Explicit Parallelism</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="parallel.html"><a href="parallel.html#bibliographic-notes-13"><i class="fa fa-check"></i><b>16.3</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="16.4" data-path="parallel.html"><a href="parallel.html#practice-yourself-12"><i class="fa fa-check"></i><b>16.4</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="algebra.html"><a href="algebra.html"><i class="fa fa-check"></i><b>17</b> Numerical Linear Algebra</a><ul>
<li class="chapter" data-level="17.1" data-path="algebra.html"><a href="algebra.html#lu-factorization"><i class="fa fa-check"></i><b>17.1</b> LU Factorization</a></li>
<li class="chapter" data-level="17.2" data-path="algebra.html"><a href="algebra.html#cholesky-factorization"><i class="fa fa-check"></i><b>17.2</b> Cholesky Factorization</a></li>
<li class="chapter" data-level="17.3" data-path="algebra.html"><a href="algebra.html#qr-factorization"><i class="fa fa-check"></i><b>17.3</b> QR Factorization</a></li>
<li class="chapter" data-level="17.4" data-path="algebra.html"><a href="algebra.html#singular-value-factorization"><i class="fa fa-check"></i><b>17.4</b> Singular Value Factorization</a></li>
<li class="chapter" data-level="17.5" data-path="algebra.html"><a href="algebra.html#iterative-methods"><i class="fa fa-check"></i><b>17.5</b> Iterative Methods</a></li>
<li class="chapter" data-level="17.6" data-path="algebra.html"><a href="algebra.html#solving-ols"><i class="fa fa-check"></i><b>17.6</b> Solving the OLS Problem</a></li>
<li class="chapter" data-level="17.7" data-path="algebra.html"><a href="algebra.html#bibliographic-notes-14"><i class="fa fa-check"></i><b>17.7</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="17.8" data-path="algebra.html"><a href="algebra.html#practice-yourself-13"><i class="fa fa-check"></i><b>17.8</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="convex.html"><a href="convex.html"><i class="fa fa-check"></i><b>18</b> Convex Optimization</a><ul>
<li class="chapter" data-level="18.1" data-path="convex.html"><a href="convex.html#theoretical-backround"><i class="fa fa-check"></i><b>18.1</b> Theoretical Backround</a></li>
<li class="chapter" data-level="18.2" data-path="convex.html"><a href="convex.html#optimizing-with-r"><i class="fa fa-check"></i><b>18.2</b> Optimizing with R</a><ul>
<li class="chapter" data-level="18.2.1" data-path="convex.html"><a href="convex.html#the-optim-function"><i class="fa fa-check"></i><b>18.2.1</b> The optim Function</a></li>
<li class="chapter" data-level="18.2.2" data-path="convex.html"><a href="convex.html#the-nloptr-package"><i class="fa fa-check"></i><b>18.2.2</b> The nloptr Package</a></li>
<li class="chapter" data-level="18.2.3" data-path="convex.html"><a href="convex.html#minqa-package"><i class="fa fa-check"></i><b>18.2.3</b> minqa Package</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="convex.html"><a href="convex.html#bibliographic-notes-15"><i class="fa fa-check"></i><b>18.3</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="18.4" data-path="convex.html"><a href="convex.html#practice-yourself-14"><i class="fa fa-check"></i><b>18.4</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="rcpp.html"><a href="rcpp.html"><i class="fa fa-check"></i><b>19</b> RCpp</a><ul>
<li class="chapter" data-level="19.1" data-path="rcpp.html"><a href="rcpp.html#bibliographic-notes-16"><i class="fa fa-check"></i><b>19.1</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="19.2" data-path="rcpp.html"><a href="rcpp.html#practice-yourself-15"><i class="fa fa-check"></i><b>19.2</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="debugging.html"><a href="debugging.html"><i class="fa fa-check"></i><b>20</b> Debugging Tools</a><ul>
<li class="chapter" data-level="20.1" data-path="debugging.html"><a href="debugging.html#bibliographic-notes-17"><i class="fa fa-check"></i><b>20.1</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="20.2" data-path="debugging.html"><a href="debugging.html#practice-yourself-16"><i class="fa fa-check"></i><b>20.2</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="econometrics.html"><a href="econometrics.html"><i class="fa fa-check"></i><b>21</b> Econometrics</a><ul>
<li class="chapter" data-level="21.1" data-path="econometrics.html"><a href="econometrics.html#bibliographic-notes-18"><i class="fa fa-check"></i><b>21.1</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="21.2" data-path="econometrics.html"><a href="econometrics.html#practice-yourself-17"><i class="fa fa-check"></i><b>21.2</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="psychometrics.html"><a href="psychometrics.html"><i class="fa fa-check"></i><b>22</b> Psychometrics</a><ul>
<li class="chapter" data-level="22.1" data-path="psychometrics.html"><a href="psychometrics.html#bibliographic-notes-19"><i class="fa fa-check"></i><b>22.1</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="22.2" data-path="psychometrics.html"><a href="psychometrics.html#practice-yourself-18"><i class="fa fa-check"></i><b>22.2</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="hadley.html"><a href="hadley.html"><i class="fa fa-check"></i><b>23</b> The Hadleyverse</a><ul>
<li class="chapter" data-level="23.1" data-path="hadley.html"><a href="hadley.html#readr"><i class="fa fa-check"></i><b>23.1</b> readr</a></li>
<li class="chapter" data-level="23.2" data-path="hadley.html"><a href="hadley.html#dplyr"><i class="fa fa-check"></i><b>23.2</b> dplyr</a></li>
<li class="chapter" data-level="23.3" data-path="hadley.html"><a href="hadley.html#tidyr"><i class="fa fa-check"></i><b>23.3</b> tidyr</a></li>
<li class="chapter" data-level="23.4" data-path="hadley.html"><a href="hadley.html#reshape2"><i class="fa fa-check"></i><b>23.4</b> reshape2</a></li>
<li class="chapter" data-level="23.5" data-path="hadley.html"><a href="hadley.html#stringr"><i class="fa fa-check"></i><b>23.5</b> stringr</a></li>
<li class="chapter" data-level="23.6" data-path="hadley.html"><a href="hadley.html#anytime"><i class="fa fa-check"></i><b>23.6</b> anytime</a></li>
<li class="chapter" data-level="23.7" data-path="hadley.html"><a href="hadley.html#biblipgraphic-notes-1"><i class="fa fa-check"></i><b>23.7</b> Biblipgraphic Notes</a></li>
<li class="chapter" data-level="23.8" data-path="hadley.html"><a href="hadley.html#practice-yourself-19"><i class="fa fa-check"></i><b>23.8</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="causality.html"><a href="causality.html"><i class="fa fa-check"></i><b>24</b> Causal Inferense</a><ul>
<li class="chapter" data-level="24.1" data-path="causality.html"><a href="causality.html#causal-inference-from-designed-experiments"><i class="fa fa-check"></i><b>24.1</b> Causal Inference From Designed Experiments</a><ul>
<li class="chapter" data-level="24.1.1" data-path="causality.html"><a href="causality.html#design-of-experiments"><i class="fa fa-check"></i><b>24.1.1</b> Design of Experiments</a></li>
<li class="chapter" data-level="24.1.2" data-path="causality.html"><a href="causality.html#randomized-inference"><i class="fa fa-check"></i><b>24.1.2</b> Randomized Inference</a></li>
</ul></li>
<li class="chapter" data-level="24.2" data-path="causality.html"><a href="causality.html#causal-inference-from-observational-data"><i class="fa fa-check"></i><b>24.2</b> Causal Inference from Observational Data</a><ul>
<li class="chapter" data-level="24.2.1" data-path="causality.html"><a href="causality.html#principal-stratification"><i class="fa fa-check"></i><b>24.2.1</b> Principal Stratification</a></li>
<li class="chapter" data-level="24.2.2" data-path="causality.html"><a href="causality.html#instrumental-variables"><i class="fa fa-check"></i><b>24.2.2</b> Instrumental Variables</a></li>
<li class="chapter" data-level="24.2.3" data-path="causality.html"><a href="causality.html#propensity-scores"><i class="fa fa-check"></i><b>24.2.3</b> Propensity Scores</a></li>
<li class="chapter" data-level="24.2.4" data-path="causality.html"><a href="causality.html#direct-lieklihood"><i class="fa fa-check"></i><b>24.2.4</b> Direct Lieklihood</a></li>
<li class="chapter" data-level="24.2.5" data-path="causality.html"><a href="causality.html#regression-discontinuity"><i class="fa fa-check"></i><b>24.2.5</b> Regression Discontinuity</a></li>
</ul></li>
<li class="chapter" data-level="24.3" data-path="causality.html"><a href="causality.html#bibliographic-notes-20"><i class="fa fa-check"></i><b>24.3</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="24.4" data-path="causality.html"><a href="causality.html#practice-yourself-20"><i class="fa fa-check"></i><b>24.4</b> Practice Yourself</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="multivariate" class="section level1">
<h1><span class="header-section-number">Chapter 9</span> Multivariate Data Analysis</h1>
<p>The term “multivariate data analysis” is so broad and so overloaded, that we start by clarifying what is discussed and what is not discussed in this chapter. Broadly speaking, we will discuss statistical <em>inference</em>, and leave more “exploratory flavored” matters like clustering, and visualization, to the Unsupervised Learning Chapter <a href="unsupervised.html#unsupervised">11</a>.</p>
<p>We start with an example.</p>

<div class="example">
<span id="exm:icu" class="example"><strong>Example 9.1  </strong></span>Consider the problem of a patient monitored in the intensive care unit. At every minute the monitor takes <span class="math inline">\(p\)</span> physiological measurements: blood pressure, body temperature, etc. The total number of minutes in our data is <span class="math inline">\(n\)</span>, so that in total, we have <span class="math inline">\(n \times p\)</span> measurements, arranged in a matrix. We also know the typical measurements for this patient when healthy: <span class="math inline">\(\mu_0\)</span>.
</div>

<p>Formally, let <span class="math inline">\(y\)</span> be single (random) measurement of a <span class="math inline">\(p\)</span>-variate random vector. Denote <span class="math inline">\(\mu:=E[y]\)</span>. Here is the set of problems we will discuss, in order of their statistical difficulty.</p>
<ul>
<li><p><strong>Signal detection</strong>: a.k.a. <em>multivariate hypothesis testing</em>, i.e., testing if <span class="math inline">\(\mu\)</span> equals <span class="math inline">\(\mu_0\)</span> and for <span class="math inline">\(\mu_0=0\)</span> in particular. In our example: “are the current measurement different than a typical one?”</p></li>
<li><p><strong>Signal counting</strong>: Counting the number of elements in <span class="math inline">\(\mu\)</span> that differ from <span class="math inline">\(\mu_0\)</span>, and for <span class="math inline">\(\mu_0=0\)</span> in particular. In our example: “how many measurements differ than their typical values?”</p></li>
<li><p><strong>Signal identification</strong>: a.k.a. <em>multiple testing</em>, i.e., testing which of the elements in <span class="math inline">\(\mu\)</span> differ from <span class="math inline">\(\mu_0\)</span> and for <span class="math inline">\(\mu_0=0\)</span> in particular. In the ANOVA literature, this is known as a <strong>post-hoc</strong> analysis. In our example: “which measurements differ than their typical values?”</p></li>
<li><p><strong>Signal estimation</strong>: Estimating the magnitudes of the departure of <span class="math inline">\(\mu\)</span> from <span class="math inline">\(\mu_0\)</span>, and for <span class="math inline">\(\mu_0=0\)</span> in particular. If estimation follows a <em>signal detection</em> or <em>signal identification</em> stage, this is known as a <em>selective estimation</em> problem. In our example: “what is the value of the measurements that differ than their typical values?”</p></li>
<li><p><strong>Multivariate Regression</strong>: a.k.a. <em>MANOVA</em> in statistical literature, and <em>structured learning</em> in the machine learning literature. In our example: “what factors affect the physiological measurements?”</p></li>
</ul>

<div class="example">
<span id="exm:brain-imaging" class="example"><strong>Example 9.2  </strong></span>Consider the problem of detecting regions of cognitive function in the brain using fMRI. Each measurement is the activation level at each location in a brain’s region. If the region has a cognitive function, the mean activation differs than <span class="math inline">\(\mu_0=0\)</span> when the region is evoked.
</div>


<div class="example">
<span id="exm:genetics" class="example"><strong>Example 9.3  </strong></span>Consider the problem of detecting cancer encoding regions in the genome. Each measurement is the vector of the genetic configuration of an individual. A cancer encoding region will have a different (multivariate) distribution between sick and healthy. In particular, <span class="math inline">\(\mu\)</span> of sick will differ from <span class="math inline">\(\mu\)</span> of healthy.
</div>


<div class="example">
<span id="exm:regression" class="example"><strong>Example 9.4  </strong></span>Consider the problem of the simplest multiple regression. The estimated coefficient, <span class="math inline">\(\hat \beta\)</span> are a random vector. Regression theory tells us that its covariance is <span class="math inline">\((X&#39;X)^{-1}\sigma^2\)</span>, and null mean of <span class="math inline">\(\beta\)</span>. We thus see that inference on the vector of regression coefficients, is nothing more than a multivaraite inference problem.
</div>


<div class="remark">
 <span class="remark"><em>Remark. </em></span> In the above, “signal” is defined in terms of <span class="math inline">\(\mu\)</span>. It is possible that the signal is not in the location, <span class="math inline">\(\mu\)</span>, but rather in the covariance, <span class="math inline">\(\Sigma\)</span>. We do not discuss these problems here, and refer the reader to <span class="citation">@nadler2008finite</span>.
</div>

<p>Another possible question is: does a multivariate analysis gives us something we cannot get from a mass-univariate analysis (i.e., a multivariate analysis on each variable separately). In Example <a href="multivariate.html#exm:icu">9.1</a> we could have just performed multiple univariate tests, and sign an alarm when any of the univariate detectors was triggered. The reason we want a multivariate detector, and not multiple univariate detectors is that it is possible that each measurement alone is borderline, but together, the signal accumulates. In our ICU example is may mean that the pulse is borderline, the body temperature is borderline, etc. Analyzed simultaneously, it is clear that the patient is in distress.</p>
<p>The next figure<a href="#fn16" class="footnoteRef" id="fnref16"><sup>16</sup></a> illustrates the idea that some bi-variate measurements may seem ordinary univariately, while very anomalous when examined bi-variately.</p>

<div class="remark">
 <span class="remark"><em>Remark. </em></span> The following figure may also be used to demonstrate the difference between Euclidean Distance and Mahalanobis Distance.
</div>

<pre><code>## 
## Attaching package: &#39;ellipse&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:graphics&#39;:
## 
##     pairs</code></pre>
<p><img src="Rcourse_files/figure-html/unnamed-chunk-199-1.png" width="50%" /></p>
<div id="signal-detection" class="section level2">
<h2><span class="header-section-number">9.1</span> Signal Detection</h2>
<p>Signal detection deals with the detection of the departure of <span class="math inline">\(\mu\)</span> from some <span class="math inline">\(\mu_0\)</span>, and especially, <span class="math inline">\(\mu_0=0\)</span>. This problem can be thought of as the multivariate counterpart of the univariate hypothesis t-test.</p>
<div id="hotellings-t2-test" class="section level3">
<h3><span class="header-section-number">9.1.1</span> Hotelling’s T2 Test</h3>
<p>The most fundamental approach to signal detection is a mere generalization of the t-test, known as <em>Hotelling’s <span class="math inline">\(T^2\)</span> test</em>.</p>
Recall the univariate t-statistic of a data vector <span class="math inline">\(x\)</span> of length <span class="math inline">\(n\)</span>:
<span class="math display" id="eq:t-test">\[\begin{align}
  t^2(x):= \frac{(\bar{x}-\mu_0)^2}{Var[\bar{x}]}= (\bar{x}-\mu_0)Var[\bar{x}]^{-1}(\bar{x}-\mu_0),
  \tag{9.1}
\end{align}\]</span>
<p>where <span class="math inline">\(Var[\bar{x}]=S^2(x)/n\)</span>, and <span class="math inline">\(S^2(x)\)</span> is the unbiased variance estimator <span class="math inline">\(S^2(x):=(n-1)^{-1}\sum (x_i-\bar x)^2\)</span>.</p>
Generalizing Eq<a href="multivariate.html#eq:t-test">(9.1)</a> to the multivariate case: <span class="math inline">\(\mu_0\)</span> is a <span class="math inline">\(p\)</span>-vector, <span class="math inline">\(\bar x\)</span> is a <span class="math inline">\(p\)</span>-vector, and <span class="math inline">\(Var[\bar x]\)</span> is a <span class="math inline">\(p \times p\)</span> matrix of the covariance between the <span class="math inline">\(p\)</span> coordinated of <span class="math inline">\(\bar x\)</span>. When operating with vectors, the squaring becomes a quadratic form, and the division becomes a matrix inverse. We thus have
<span class="math display" id="eq:hotelling-test">\[\begin{align}
  T^2(x):= (\bar{x}-\mu_0)&#39; Var[\bar{x}]^{-1} (\bar{x}-\mu_0),
  \tag{9.2}
\end{align}\]</span>
which is the definition of Hotelling’s <span class="math inline">\(T^2\)</span> test statistic. We typically denote the covariance between coordinates in <span class="math inline">\(x\)</span> with <span class="math inline">\(\hat \Sigma(x)\)</span>, so that <span class="math inline">\(\widehat \Sigma_{k,l}:=\widehat {Cov}[x_k,x_l]=(n-1)^{-1} \sum (x_{k,i}-\bar x_k)(x_{l,i}-\bar x_l)\)</span>. Using the <span class="math inline">\(\Sigma\)</span> notation, Eq.<a href="multivariate.html#eq:hotelling-test">(9.2)</a> becomes
<span class="math display">\[\begin{align}
  T^2(x):= n (\bar{x}-\mu_0)&#39; \hat \Sigma(x)^{-1} (\bar{x}-\mu_0),
\end{align}\]</span>
<p>which is the standard notation of Hotelling’s test statistic.</p>
<p>For inference, we need the null distribution of Hotelling’s test statistic. For this we introduce some vocabulary<a href="#fn17" class="footnoteRef" id="fnref17"><sup>17</sup></a>:</p>
<ol style="list-style-type: decimal">
<li><strong>Low Dimension</strong>: We call a problem <em>low dimensional</em> if <span class="math inline">\(n \gg p\)</span>, i.e. <span class="math inline">\(p/n \approx 0\)</span>. This means there are many observations per estimated parameter.</li>
<li><strong>High Dimension</strong>: We call a problem <em>high dimensional</em> if <span class="math inline">\(p/n \to c\)</span>, where <span class="math inline">\(c\in (0,1)\)</span>. This means there are more observations than parameters, but not many.</li>
<li><strong>Very High Dimension</strong>: We call a problem <em>very high dimensional</em> if <span class="math inline">\(p/n \to c\)</span>, where <span class="math inline">\(1&lt;c&lt;\infty\)</span>. This means there are less observations than parameter.</li>
</ol>
<p>Hotelling’s <span class="math inline">\(T^2\)</span> test can only be used in the low dimensional regime. For some intuition on this statement, think of taking <span class="math inline">\(n=20\)</span> measurements of <span class="math inline">\(p=100\)</span> physiological variables. We seemingly have <span class="math inline">\(20\)</span> observations, but there are <span class="math inline">\(100\)</span> unknown quantities in <span class="math inline">\(\mu\)</span>. Would you trust your conclusion that <span class="math inline">\(\bar x\)</span> is different than <span class="math inline">\(\mu_0\)</span> based on merely <span class="math inline">\(20\)</span> observations.</p>
<p>The above criticism is formalized in <span class="citation">@bai1996effect</span>. For modern applications, Hotelling’s <span class="math inline">\(T^2\)</span> is not recommended, since many modern alternatives have been made available. See <span class="citation">@rosenblatt2016better</span> and references for a review.</p>
</div>
<div id="various-types-of-signal-to-detect" class="section level3">
<h3><span class="header-section-number">9.1.2</span> Various Types of Signal to Detect</h3>
<p>In the previous, we assumed that the signal is a departure of <span class="math inline">\(\mu\)</span> from some <span class="math inline">\(\mu_0\)</span>. For vactor-valued data <span class="math inline">\(y\)</span>, that is distributed <span class="math inline">\(F\)</span>, we may define “signal” as any departure from some <span class="math inline">\(F_0\)</span>. This is the multivaraite counterpart of goodness-of-fit (GOF) tests.</p>
<p>Even when restricting “signal” to departures of <span class="math inline">\(\mu\)</span> from <span class="math inline">\(\mu_0\)</span>, we may try to detect various types of signal:</p>
<ol style="list-style-type: decimal">
<li><strong>Dense Signal</strong>: when the departure is in all coordinates of <span class="math inline">\(\mu\)</span>.</li>
<li><strong>Sparse Signal</strong>: when the departure is in a subset of coordinates of <span class="math inline">\(\mu\)</span>.</li>
</ol>
<p>A manufactoring motivation is consistent with a dense signal: if a manufacturing process has failed, we expect a change in many measurements (i.e. coordinates of <span class="math inline">\(\mu\)</span>). A brain-imaging motivation is consistent with a dense signal: if a region encodes cognitive function, we expect a change in many brain locations (i.e. coordinates of <span class="math inline">\(\mu\)</span>.) A genetic motivation is consistent with a sparse signal: if susceptibility of disease is genetic, only a small subset of locations in the genome will encode it.</p>
<p>Hotelling’s <span class="math inline">\(T^2\)</span> statistic is designed for dense signal. The following is a simple statistic designed for sparse signal.</p>
</div>
<div id="simes-test" class="section level3">
<h3><span class="header-section-number">9.1.3</span> Simes’ Test</h3>
<p>Hotelling’s <span class="math inline">\(T^2\)</span> statistic has currently two limitations: It is designed for dense signals, and it requires estimating the covariance, which is a very difficult problem.</p>
<p>An algorithm, that is sensitive to sparse signal and allows statistically valid detection under a wide range of covariances (even if we don’t know the covariance) is known as <em>Simes’ Test</em>. The statistic is defined vie the following algorithm:</p>
<ol style="list-style-type: decimal">
<li>Compute <span class="math inline">\(p\)</span> variable-wise p-values: <span class="math inline">\(p_1,\dots,p_j\)</span>.</li>
<li>Denote <span class="math inline">\(p_{(1)},\dots,p_{(j)}\)</span> the sorted p-values.</li>
<li>Simes’ statistic is <span class="math inline">\(p_{Simes}:=min_j\{p_{(j)} \times p/j\}\)</span>.</li>
<li>Reject the “no signal” null hypothesis at significance <span class="math inline">\(\alpha\)</span> if <span class="math inline">\(p_{Simes}&lt;\alpha\)</span>.</li>
</ol>
</div>
<div id="signal-detection-with-r" class="section level3">
<h3><span class="header-section-number">9.1.4</span> Signal Detection with R</h3>
<p>Let’s generate some data with no signal.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(mvtnorm)
n &lt;-<span class="st"> </span><span class="dv">100</span> <span class="co"># observations</span>
p &lt;-<span class="st"> </span><span class="dv">18</span> <span class="co"># parameter dimension</span>
mu &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,p) <span class="co"># no signal</span>
x &lt;-<span class="st"> </span><span class="kw">rmvnorm</span>(<span class="dt">n =</span> n, <span class="dt">mean =</span> mu)
<span class="kw">dim</span>(x)</code></pre></div>
<pre><code>## [1] 100  18</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lattice<span class="op">::</span><span class="kw">levelplot</span>(x)</code></pre></div>
<p><img src="Rcourse_files/figure-html/unnamed-chunk-200-1.png" width="50%" /></p>
<p>Now make our own Hotelling function.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">hotellingOneSample &lt;-<span class="st"> </span><span class="cf">function</span>(x, <span class="dt">mu0=</span><span class="kw">rep</span>(<span class="dv">0</span>,<span class="kw">ncol</span>(x))){
  n &lt;-<span class="st"> </span><span class="kw">nrow</span>(x)
  p &lt;-<span class="st"> </span><span class="kw">ncol</span>(x)
  <span class="kw">stopifnot</span>(n <span class="op">&gt;</span><span class="st"> </span><span class="dv">5</span><span class="op">*</span><span class="st"> </span>p)
  bar.x &lt;-<span class="st"> </span><span class="kw">colMeans</span>(x)
  Sigma &lt;-<span class="st"> </span><span class="kw">var</span>(x)
  Sigma.inv &lt;-<span class="st"> </span><span class="kw">solve</span>(Sigma)
  T2 &lt;-<span class="st"> </span>n <span class="op">*</span><span class="st"> </span>(bar.x<span class="op">-</span>mu0) <span class="op">%*%</span><span class="st"> </span>Sigma.inv <span class="op">%*%</span><span class="st"> </span>(bar.x<span class="op">-</span>mu0)
  p.value &lt;-<span class="st"> </span><span class="kw">pchisq</span>(<span class="dt">q =</span> T2, <span class="dt">df =</span> p, <span class="dt">lower.tail =</span> <span class="ot">FALSE</span>)
  <span class="kw">return</span>(<span class="kw">list</span>(<span class="dt">statistic=</span>T2, <span class="dt">pvalue=</span>p.value))
}
<span class="kw">hotellingOneSample</span>(x)</code></pre></div>
<pre><code>## $statistic
##          [,1]
## [1,] 17.22438
## 
## $pvalue
##           [,1]
## [1,] 0.5077323</code></pre>
<p>Things to note:</p>
<ul>
<li><code>stopifnot(n &gt; 5 * p)</code> is a little verification to check that the problem is indeed low dimensional. Otherwise, the <span class="math inline">\(\chi^2\)</span> approximation cannot be trusted.</li>
<li><code>solve</code> returns a matrix inverse.</li>
<li><code>%*%</code> is the matrix product operator (see also <code>crossprod()</code>).</li>
<li>A function may return only a single object, so we wrap the statistic and its p-value in a <code>list</code> object.</li>
</ul>
<p>Just for verification, we compare our home made Hotelling’s test, to the implementation in the <strong>rrcov</strong> package. The statistic is clearly OK, but our <span class="math inline">\(\chi^2\)</span> approximation of the distribution leaves room to desire. Personally, I would never trust a Hotelling test if <span class="math inline">\(n\)</span> is not much greater than <span class="math inline">\(p\)</span>, in which case I would use a high-dimensional adaptation (see Bibliography).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">rrcov<span class="op">::</span><span class="kw">T2.test</span>(x)</code></pre></div>
<pre><code>## 
##  One-sample Hotelling test
## 
## data:  x
## T2 = 17.22400, F = 0.79259, df1 = 18, df2 = 82, p-value = 0.703
## alternative hypothesis: true mean vector is not equal to (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)&#39; 
## 
## sample estimates:
##                      [,1]       [,2]      [,3]       [,4]      [,5]
## mean x-vector -0.01746212 0.03776332 0.1006145 -0.2083005 0.1026982
##                      [,6]         [,7]       [,8]       [,9]       [,10]
## mean x-vector -0.05220043 -0.009497987 -0.1139856 0.02851701 -0.03089953
##                     [,11]      [,12]      [,13]      [,14]      [,15]
## mean x-vector -0.02457798 -0.1270753 0.04717076 0.01683591 0.03085023
##                   [,16]       [,17]     [,18]
## mean x-vector 0.1499485 -0.07630663 0.1004852</code></pre>
<p>Let’s do the same with Simes’:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Simes &lt;-<span class="st"> </span><span class="cf">function</span>(x){
  p.vals &lt;-<span class="st"> </span><span class="kw">apply</span>(x, <span class="dv">2</span>, <span class="cf">function</span>(z) <span class="kw">t.test</span>(z)<span class="op">$</span>p.value) <span class="co"># Compute variable-wise pvalues</span>
  p &lt;-<span class="st"> </span><span class="kw">ncol</span>(x)
  p.Simes &lt;-<span class="st"> </span>p <span class="op">*</span><span class="st"> </span><span class="kw">min</span>(<span class="kw">sort</span>(p.vals)<span class="op">/</span><span class="kw">seq_along</span>(p.vals)) <span class="co"># Compute the Simes statistic</span>
  <span class="kw">return</span>(<span class="kw">c</span>(<span class="dt">pvalue=</span>p.Simes))
}
<span class="kw">Simes</span>(x)</code></pre></div>
<pre><code>##    pvalue 
## 0.6398998</code></pre>
<p>And now we verify that both tests can indeed detect signal when present. Are p-values small enough to reject the “no signal” null hypothesis?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mu &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dt">x =</span> <span class="dv">10</span><span class="op">/</span>p,<span class="dt">times=</span>p) <span class="co"># inject signal</span>
x &lt;-<span class="st"> </span><span class="kw">rmvnorm</span>(<span class="dt">n =</span> n, <span class="dt">mean =</span> mu)
<span class="kw">hotellingOneSample</span>(x)</code></pre></div>
<pre><code>## $statistic
##          [,1]
## [1,] 686.8046
## 
## $pvalue
##               [,1]
## [1,] 3.575926e-134</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">Simes</span>(x)</code></pre></div>
<pre><code>##       pvalue 
## 2.765312e-10</code></pre>
<p>… yes. All p-values are very small, so that all statistics can detect the non-null distribution.</p>
</div>
</div>
<div id="signal-counting" class="section level2">
<h2><span class="header-section-number">9.2</span> Signal Counting</h2>
<p>There are many ways to approach the <em>signal counting</em> problem. For the purposes of this book, however, we will not discuss them directly, and solve the signal counting problem as a signal identification problem: if we know <strong>where</strong> <span class="math inline">\(\mu\)</span> departs from <span class="math inline">\(\mu_0\)</span>, we only need to count coordinates to solve the signal counting problem.</p>

<div class="remark">
 <span class="remark"><em>Remark. </em></span> In the sparsity or multiple-testing literature, what we call “signal counting” is known as “adapting to sparsit”, or “adaptivity”.
</div>

</div>
<div id="identification" class="section level2">
<h2><span class="header-section-number">9.3</span> Signal Identification</h2>
<p>The problem of <em>signal identification</em> is also known as <em>selective testing</em>, or more commonly as <em>multiple testing</em>.</p>
<p>In the ANOVA literature, an identification stage will typically follow a detection stage. These are known as the <em>omnibus F test</em>, and <em>post-hoc</em> tests, respectively. In the multiple testing literature there will typically be no preliminary detection stage. It is typically assumed that signal is present, and the only question is “where?”</p>
<p>The first question when approaching a multiple testing problem is “what is an error”? Is an error declaring a coordinate in <span class="math inline">\(\mu\)</span> to be different than <span class="math inline">\(\mu_0\)</span> when it is actually not? Is an error an overly high proportion of falsely identified coordinates? The former is known as the <em>family wise error rate</em> (FWER), and the latter as the <em>false discovery rate</em> (FDR).</p>

<div class="remark">
 <span class="remark"><em>Remark. </em></span> These types of errors have many names in many communities. See the Wikipedia entry on <a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic">ROC</a> for a table of the (endless) possible error measures.
</div>

<div id="signal-identification-in-r" class="section level3">
<h3><span class="header-section-number">9.3.1</span> Signal Identification in R</h3>
<p>One (of many) ways to do signal identification involves the <code>stats::p.adjust</code> function. The function takes as inputs a <span class="math inline">\(p\)</span>-vector of the variable-wise <strong>p-values</strong>. Why do we start with variable-wise p-values, and not the full data set?</p>
<ol style="list-style-type: lower-alpha">
<li>Because we want to make inference variable-wise, so it is natural to start with variable-wise statistics.</li>
<li>Because we want to avoid dealing with covariances if possible. Computing variable-wise p-values does not require estimating covariances.</li>
<li>So that the identification problem is decoupled from the variable-wise inference problem, and may be applied much more generally than in the setup we presented.</li>
</ol>
<p>We start be generating some high-dimensional multivariate data and computing the coordinate-wise (i.e. hypothesis-wise) p-value.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(mvtnorm)
n &lt;-<span class="st"> </span><span class="fl">1e1</span>
p &lt;-<span class="st"> </span><span class="fl">1e2</span>
mu &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,p)
x &lt;-<span class="st"> </span><span class="kw">rmvnorm</span>(<span class="dt">n =</span> n, <span class="dt">mean =</span> mu)
<span class="kw">dim</span>(x)</code></pre></div>
<pre><code>## [1]  10 100</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lattice<span class="op">::</span><span class="kw">levelplot</span>(x)</code></pre></div>
<p><img src="Rcourse_files/figure-html/unnamed-chunk-207-1.png" width="50%" /></p>
<p>We now compute the pvalues of each coordinate. We use a coordinate-wise t-test. Why a t-test? Because for the purpose of demonstration we want a simple test. In reality, you may use any test that returns valid p-values.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">t.pval &lt;-<span class="st"> </span><span class="cf">function</span>(y) <span class="kw">t.test</span>(y)<span class="op">$</span>p.value
p.values &lt;-<span class="st"> </span><span class="kw">apply</span>(<span class="dt">X =</span> x, <span class="dt">MARGIN =</span> <span class="dv">2</span>, <span class="dt">FUN =</span> t.pval) 
<span class="kw">plot</span>(p.values, <span class="dt">type=</span><span class="st">&#39;h&#39;</span>)</code></pre></div>
<p><img src="Rcourse_files/figure-html/unnamed-chunk-208-1.png" width="50%" /></p>
<p>Things to note:</p>
<ul>
<li><code>t.pval</code> is a function that merely returns the p-value of a t.test.</li>
<li>We used the <code>apply</code> function to apply the same function to each column of <code>x</code>.</li>
<li><code>MARGIN=2</code> tells <code>apply</code> to compute over columns and not rows.</li>
<li>The output, <code>p.values</code>, is a vector of 100 p-values.</li>
</ul>
<p>We are now ready to do the identification, i.e., find which coordinate of <span class="math inline">\(\mu\)</span> is different than <span class="math inline">\(\mu_0=0\)</span>. The workflow for identification has the same structure, regardless of the desired error guarantees:</p>
<ol style="list-style-type: decimal">
<li>Compute an <code>adjusted p-value</code>.</li>
<li>Compare the adjusted p-value to the desired error level.</li>
</ol>
<p>If we want <span class="math inline">\(FWER \leq 0.05\)</span>, meaning that we allow a <span class="math inline">\(5\%\)</span> probability of making any mistake, we will use the <code>method=&quot;holm&quot;</code> argument of <code>p.adjust</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">alpha &lt;-<span class="st"> </span><span class="fl">0.05</span>
p.values.holm &lt;-<span class="st"> </span><span class="kw">p.adjust</span>(p.values, <span class="dt">method =</span> <span class="st">&#39;holm&#39;</span> )
<span class="kw">which</span>(p.values.holm <span class="op">&lt;</span><span class="st"> </span>alpha)</code></pre></div>
<pre><code>## integer(0)</code></pre>
<p>If we want <span class="math inline">\(FDR \leq 0.05\)</span>, meaning that we allow the proportion of false discoveries to be no larger than <span class="math inline">\(5\%\)</span>, we use the <code>method=&quot;BH&quot;</code> argument of <code>p.adjust</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">alpha &lt;-<span class="st"> </span><span class="fl">0.05</span>
p.values.BH &lt;-<span class="st"> </span><span class="kw">p.adjust</span>(p.values, <span class="dt">method =</span> <span class="st">&#39;BH&#39;</span> )
<span class="kw">which</span>(p.values.BH <span class="op">&lt;</span><span class="st"> </span>alpha)</code></pre></div>
<pre><code>## integer(0)</code></pre>
<p>We now inject some strong signal in <span class="math inline">\(\mu\)</span> just to see that the process works. We will artificially inject signal in the first 10 coordinates.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mu[<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>] &lt;-<span class="st"> </span><span class="dv">2</span> <span class="co"># inject signal in first 10 variables</span>
x &lt;-<span class="st"> </span><span class="kw">rmvnorm</span>(<span class="dt">n =</span> n, <span class="dt">mean =</span> mu) <span class="co"># generate data</span>
p.values &lt;-<span class="st"> </span><span class="kw">apply</span>(<span class="dt">X =</span> x, <span class="dt">MARGIN =</span> <span class="dv">2</span>, <span class="dt">FUN =</span> t.pval) 
p.values.BH &lt;-<span class="st"> </span><span class="kw">p.adjust</span>(p.values, <span class="dt">method =</span> <span class="st">&#39;BH&#39;</span> )
<span class="kw">which</span>(p.values.BH <span class="op">&lt;</span><span class="st"> </span>alpha)</code></pre></div>
<pre><code>##  [1]  1  2  3  4  5  6  7  9 10 55</code></pre>
<p>Indeed- we are now able to detect that the first coordinates carry signal, because their respective coordinate-wise null hypotheses have been rejected.</p>
</div>
</div>
<div id="signal-estimation" class="section level2">
<h2><span class="header-section-number">9.4</span> Signal Estimation (*)</h2>
<p>The estimation of the elements of <span class="math inline">\(\mu\)</span> is a seemingly straightforward task. This is not the case, however, if we estimate only the elements that were selected because they were significant (or any other data-dependent criterion). Clearly, estimating only significant entries will introduce a bias in the estimation. In the statistical literature, this is known as <em>selection bias</em>. Selection bias also occurs when you perform inference on regression coefficients after some model selection, say, with a lasso, or a forward search<a href="#fn18" class="footnoteRef" id="fnref18"><sup>18</sup></a>.</p>
<p>Selective inference is a complicated and active research topic so we will not offer any off-the-shelf solution to the matter. The curious reader is invited to read <span class="citation">@rosenblatt2014selective</span>, <span class="citation">@javanmard2014confidence</span>, or <a href="http://www.stat.berkeley.edu/~wfithian/">Will Fithian’s</a> PhD thesis <span class="citation">[@fithian2015topics]</span> for more on the topic.</p>
</div>
<div id="multivariate-regression" class="section level2">
<h2><span class="header-section-number">9.5</span> Multivariate Regression (*)</h2>
<p><em>Multivaraite regression</em>, a.k.a. <em>MANOVA</em>, similar to <a href="https://en.wikipedia.org/wiki/Structured_prediction"><em>structured learning</em></a> in machine learning, is simply a regression problem where the outcome, <span class="math inline">\(y\)</span>, is not scalar values but vector valued. It is not to be confused with <em>multiple regression</em> where the predictor, <span class="math inline">\(x\)</span>, is vector valued, but the outcome is scalar.</p>
<p>If the linear models generalize the two-sample t-test from two, to multiple populations, then multivariate regression generalizes Hotelling’s test in the same way.</p>
<p>When the entries of <span class="math inline">\(y\)</span> are independent, MANOVA collapses to multiple univariate regressions. It is only when entries in <span class="math inline">\(y\)</span> are correlated that we can gain in accuracy and power by harnessing these correlations through the MANOVA framework.</p>
<div id="multivariate-regression-with-r" class="section level3">
<h3><span class="header-section-number">9.5.1</span> Multivariate Regression with R</h3>
<p>TODO</p>
</div>
</div>
<div id="graphical-models" class="section level2">
<h2><span class="header-section-number">9.6</span> Graphical Models (*)</h2>
<p>Fitting a multivariate distribution, i.e. learning a <em>graphical model</em>, is a very hard task. To see why, consider the problem of <span class="math inline">\(p\)</span> continuous variables. In the simplest case, where we can assume normality, fitting a distributions means estimating the <span class="math inline">\(p\)</span> parameters in the expectation, <span class="math inline">\(\mu\)</span>, and <span class="math inline">\(p(p+1)/2\)</span> parameters in the covariance, <span class="math inline">\(\Sigma\)</span>. The number of observations required for this task, <span class="math inline">\(n\)</span>, may be formidable.</p>
<p>A more humble task, is to identify <strong>independencies</strong>, known as <em>structure learning</em> in the machine learning literature. Under the multivariate normality assumption, this means identifying zero entries in <span class="math inline">\(\Sigma\)</span>, or more precisely, zero entries in <span class="math inline">\(\Sigma^{-1}\)</span>. This task can be approached as a <strong>signal identification</strong> problem (<a href="multivariate.html#identification">9.3</a>). The same solutions may be applied to identify non-zero entries in <span class="math inline">\(\Sigma\)</span>, instead of <span class="math inline">\(\mu\)</span> as discussed until now.</p>
<p>If multivariate normality cannot be assumed, then identifying independencies cannot be done via the covariance matrix <span class="math inline">\(\Sigma\)</span> and more elaborate algorithms are required.</p>
<div id="graphical-models-in-r" class="section level3">
<h3><span class="header-section-number">9.6.1</span> Graphical Models in R</h3>
<p>TODO</p>
</div>
</div>
<div id="biblipgraphic-notes" class="section level2">
<h2><span class="header-section-number">9.7</span> Biblipgraphic Notes</h2>
<p>For a general introduction to multivariate data analysis see <span class="citation">@anderson2004introduction</span>. For an R oriented introduction, see <span class="citation">@everitt2011introduction</span>. For more on the difficulties with high dimensional problems, see <span class="citation">@bai1996effect</span>. For some cutting edge solutions for testing in high-dimension, see <span class="citation">@rosenblatt2016better</span> and references therein. Simes’ test is not very well known. It is introduced in <span class="citation">@simes1986improved</span>, and proven to control the type I error of detection under a PRDS type of dependence in <span class="citation">@benjamini2001control</span>. For more on multiple testing, and signal identification, see <span class="citation">@efron2012large</span>. For more on the choice of your error rate see <span class="citation">@rosenblatt2013practitioner</span>. For an excellent review on graphical models see <span class="citation">@kalisch2014causal</span>. Everything you need on graphical models, Bayesian belief networks, and structure learning in R, is collected in the <a href="https://cran.r-project.org/web/views/gR.html">Task View</a>.</p>
</div>
<div id="practice-yourself-5" class="section level2">
<h2><span class="header-section-number">9.8</span> Practice Yourself</h2>
<ol style="list-style-type: decimal">
<li><p>Generate multivariate data with:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">3</span>)
mean&lt;-<span class="kw">rexp</span>(<span class="dv">50</span>,<span class="dv">6</span>)
multi&lt;-<span class="st">  </span><span class="kw">rmvnorm</span>(<span class="dt">n =</span> <span class="dv">100</span>, <span class="dt">mean =</span> mean) </code></pre></div>
<ol style="list-style-type: lower-alpha">
<li>Use Hotelling’s test to determine if <span class="math inline">\(\mu\)</span> equals <span class="math inline">\(\mu_0=0\)</span>. Can you detect the signal?</li>
<li>Perform t.test on each variable and extract the p-value. Try to identify visually the variables which depart from <span class="math inline">\(\mu_0\)</span>.</li>
<li>Use <code>p.adjust</code> to identify in which variables there are any departures from <span class="math inline">\(\mu_0=0\)</span>. Allow 5% probability of making any false identification.</li>
<li>Use <code>p.adjust</code> to identify in which variables there are any departures from <span class="math inline">\(\mu_0=0\)</span>. Allow a 5% proportion of errors within identifications.</li>
</ol></li>
<li>Generate multivariate data from two groups: <code>rmvnorm(n = 100, mean = rep(0,10))</code> for the first, and <code>rmvnorm(n = 100, mean = rep(0.1,10))</code> for the second.
<ol style="list-style-type: lower-alpha">
<li>Do we agree the groups differ?</li>
<li>Implement the two-group Hotelling test described in Wikipedia: (<a href="https://en.wikipedia.org/wiki/Hotelling%27s_T-squared_distribution#Two-sample_statistic">https://en.wikipedia.org/wiki/Hotelling%27s_T-squared_distribution#Two-sample_statistic</a>).</li>
<li>Verify that you are able to detect that the groups differ.</li>
<li>Perform a two-group t-test on each coordinate. On which coordinates can you detect signal while controlling the FWER? On which while controlling the FDR? Use <code>p.adjust</code>.</li>
</ol></li>
<li><p>Return to the previous problem, but set <code>n=9</code>. Verify that you cannot compute your Hotelling statistic.</p></li>
</ol>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="16">
<li id="fn16"><p>My thanks to Efrat Vilneski for the figure.<a href="multivariate.html#fnref16">↩</a></p></li>
<li id="fn17"><p>This vocabulary is not standard in the literature, so when you read a text, you need to verify yourself what the author means.<a href="multivariate.html#fnref17">↩</a></p></li>
<li id="fn18"><p>You might find this shocking, but it does mean that you cannot trust the <code>summary</code> table of a model that was selected from a multitude of models.<a href="multivariate.html#fnref18">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="lme.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="supervised.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/50-multivariate.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["Rcourse.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
